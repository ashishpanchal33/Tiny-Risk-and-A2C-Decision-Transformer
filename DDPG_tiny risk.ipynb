{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "358fdba8-2d74-46d3-8d0c-650038d08878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ddpg/#ddpg_continuous_actionpy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#import tyro\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a1da81-2341-40e2-b749-915e79723108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] =\"expandable_segments:True\"\n",
    "import gymnasium as gym\n",
    "\n",
    "#import gym\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import functools\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete, Box, Dict\n",
    "\n",
    "from pettingzoo import AECEnv\n",
    "\n",
    "\n",
    "\n",
    "import gymnasium\n",
    "\n",
    "\n",
    "from pettingzoo.utils import agent_selector, wrappers\n",
    "\n",
    "from gymnasium.utils import EzPickle\n",
    "\n",
    "\n",
    "\n",
    "from statistics import NormalDist\n",
    "\n",
    "import pygame\n",
    "\n",
    "from typing import Any , Generic, Iterable, Iterator, TypeVar\n",
    "ActionType = TypeVar(\"ActionType\")\n",
    "\n",
    "import collections\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bf526ce-0ee4-4087-91bf-994df54a3be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  Utilities.new_models import *\n",
    "from  Utilities.Transformer_risk_act_2 import *\n",
    "import utils_gym\n",
    "import env_model_class_2\n",
    "\n",
    "\n",
    "from board_env import *\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b2a79e0-f1da-41f8-aa8e-2c1f388c2083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "209de013-e362-45d3-88e4-f4a98e8375c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model arch def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "171f9c25-a240-49fa-be75-47dd78de3e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = 'Tiny_Risk'#os.path.basename(__file__)[: -len(\".py\")]\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"cleanRL\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = None\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "    save_model: bool = False\n",
    "    \"\"\"whether to save model into the `runs/{run_name}` folder\"\"\"\n",
    "    upload_model: bool = False\n",
    "    \"\"\"whether to upload the saved model to huggingface\"\"\"\n",
    "    hf_entity: str = \"\"\n",
    "    \"\"\"the user or org name of the model repository from the Hugging Face Hub\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"Tiny_Risk\" #\"Hopper-v4\"\n",
    "    \"\"\"the environment id of the Atari game\"\"\"\n",
    "    total_timesteps: int = 1000000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 3e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    buffer_size: int = int(1e6)\n",
    "    \"\"\"the replay memory buffer size\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    tau: float = 0.005\n",
    "    \"\"\"target smoothing coefficient (default: 0.005)\"\"\"\n",
    "    batch_size: int = 256\n",
    "    \"\"\"the batch size of sample from the reply memory\"\"\"\n",
    "    exploration_noise: float = 0.1\n",
    "    \"\"\"the scale of exploration noise\"\"\"\n",
    "    learning_starts: int = 25e3\n",
    "    \"\"\"timestep to start learning\"\"\"\n",
    "    policy_frequency: int = 2\n",
    "    \"\"\"the frequency of training policy (delayed)\"\"\"\n",
    "    noise_clip: float = 0.5\n",
    "    \"\"\"noise clip parameter of the Target Policy Smoothing Regularization\"\"\"\n",
    "\n",
    "\n",
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env.action_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs,action,h_dim=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs + h_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "        self.embed_action = layer_init(torch.nn.Linear(2, h_dim), std=0.01)\n",
    "\n",
    "    def forward(self, x, a1,a2):\n",
    "        #x = torch.cat([x, a], 1)\n",
    "        a = torch.concat((a1,a2),axis = -1)\n",
    "        a = self.embed_action(a)\n",
    "\n",
    "        x = torch.concat((x,a),axis = 1)\n",
    "\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def differentiable_randomized_argmax(softmax_output, temperature=1.0):\n",
    "\n",
    "    if temperature < 0:\n",
    "        raise ValueError(\"Temperature must be non-negative.\")\n",
    "    max_prob = torch.max(softmax_output, dim=-1, keepdim=True)[0]\n",
    "    top_k_mask = (softmax_output == max_prob).float()\n",
    "    # Generate Gumbel noise\n",
    "    gumbel_noise = -torch.log(-torch.log(torch.rand_like(softmax_output) + 1e-9) + 1e-9)  # Adding small epsilon for numerical stability\n",
    "\n",
    "    # Add Gumbel noise to logits (implicitly working with logits since softmax_output is probabilities)\n",
    "    # To make it more numerically stable if starting from probabilities, you might want to work with logits directly if possible.\n",
    "    # However, we can approximate logits from probabilities (though it might lose some precision)\n",
    "    # logits = torch.log(softmax_output + 1e-9) # Avoid log(0)\n",
    "    # noisy_logits = logits + gumbel_noise\n",
    "\n",
    "    # More directly with probabilities (and numerically safer)\n",
    "    noisy_logits = torch.log(softmax_output + 1e-9) + gumbel_noise * top_k_mask # Approximate logits then add noise\n",
    "    noisy_output = nn.functional.softmax(noisy_logits / temperature, dim=-1) # Apply softmax with temperature\n",
    "\n",
    "    a=torch.argmax(noisy_output,-1)\n",
    "    \n",
    "    return a\n",
    "\n",
    "\n",
    "\n",
    "class ActorDiscrete(nn.Module):\n",
    "    def __init__(self, obs, action_space): # Pass action_space instead of env\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs, 256) # Use obs.shape directly\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_pi = nn.Linear(256, action_space) # Output size is the number of discrete actions\n",
    "\n",
    "    def forward(self, x,action_mask=[],use_action_mask=True,give_prob=False):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc_pi(x) # Output logits for each discrete action\n",
    "        \n",
    "        if use_action_mask:\n",
    "            probs = (F.softmax(logits*action_mask + 1e-9, dim=-1)) # Apply softmax to get probabilities\n",
    "        else:\n",
    "            probs = (F.softmax(logits+ 1e-9, dim=-1))\n",
    "        if give_prob:\n",
    "            return probs # Return the probability distribution over actions\n",
    "        else:\n",
    "            dis = Categorical(probs)\n",
    "            acts = dis.sample()\n",
    "            return  acts#differentiable_randomized_argmax(probs)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs,action_space,h_dim = 6):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs+h_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mu = nn.Linear(256, 1)\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((1 - 0) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((1 - 0) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.embed_action_1 = torch.nn.Embedding(action_space+1, h_dim)\n",
    "\n",
    "    def forward(self, x,a):\n",
    "        \n",
    "    \n",
    "        a = self.embed_action_1(a.long())\n",
    "\n",
    "        x = torch.concat((x,a),axis = -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc_mu(x))\n",
    "        return x * self.action_scale + self.action_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c60d09-73bd-437f-b009-ada92c1897f4",
   "metadata": {},
   "source": [
    "if False:\n",
    "exp_12 = dict(\n",
    "exp_name = 'exp01_ddpg_4_agents_1_hero_',\n",
    "learning_rate = 5e-4,#0.003,#0.0003,\n",
    "batch_size = 2,\n",
    "gamma = 0.99,\n",
    "num_steps=1200000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 3000,#10000,\n",
    "hero_agent_count = 1,\n",
    "model_name={1:\"transformer_model\"}#,2:'transformer_model'}\n",
    ",entropy=True,\n",
    "return_prob=2,\n",
    "actor_wt = 0.5,\n",
    "CE_wt = 0.01,\n",
    "small = True,\n",
    "num_episodes = 4,\n",
    "context_len = 256,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=False,#False,\n",
    "drop_last=True,\n",
    "TB_log=True,\n",
    "learning_starts =100,\n",
    "update_epochs = 1,\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "pin_memory_device= (\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "    \n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 3#4\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.01\n",
    "                 )\n",
    ",model_config = dict(\n",
    "                    n_blocks      =   3,\n",
    "                    embed_dim     =   64,#128 ,\n",
    "                    context_len   =   256,#256  ,\n",
    "                    n_heads       =   1,\n",
    "                    dropout_p     =   0.1,\n",
    "                    wt_decay      =   0.0001,\n",
    "                    warmup_steps  =   100   ,\n",
    "                    #tau           =   0.95,\n",
    "                    chunk_size    =   64,#8,#32,#64\n",
    "                    chunk_overlap =   1,\n",
    "                    rb_len        =   20,\n",
    "                \n",
    "                    warmup_epoch  =   5,  \n",
    "                    total_epoch   =   20,\n",
    "                    initial_lr    =   5e-4,\n",
    "                    final_lr      =   1e-6,\n",
    "\n",
    "                    beta           = 0.5,#args.model_config['beta']         #0.2 #Q_mse\n",
    "                    alpha          = 1,#args.model_config['alpha']          #0.1  #actionloss\n",
    "                    entropy_coeff  = 0.2,#args.model_config['entropy_coeff']         #0.1#0.5   #entropy loss in action\n",
    "                    val_loss_coeff = 0.5#args.model_config['val_loss_coeff']        #0.5      #Q loss\n",
    "            \n",
    "                \n",
    "                    ,learning_rate = 3e-4 \"\"\"the learning rate of the optimizer\"\"\"\n",
    "                    ,buffer_size = int(1e6) \"\"\"the replay memory buffer size\"\"\"\n",
    "                    ,gamma = 0.99 \"\"\"the discount factor gamma\"\"\"\n",
    "                    ,tau = 0.005 \"\"\"target smoothing coefficient (default: 0.005)\"\"\"\n",
    "                    ,batch_size = 256 \"\"\"the batch size of sample from the reply memory\"\"\"\n",
    "                    ,exploration_noise = 0.1 \"\"\"the scale of exploration noise\"\"\"\n",
    "                    ,learning_starts = 25e3 \"\"\"timestep to start learning\"\"\"\n",
    "                    ,policy_frequency = 2 \"\"\"the frequency of training policy (delayed)\"\"\"\n",
    "                    ,noise_clip = 0.5 \"\"\"noise clip parameter of the Target Policy Smoothing Regularization\"\"\"\n",
    "\n",
    "    \n",
    "                    )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2864bb62-5660-4a08-b782-02d0732e4a2d",
   "metadata": {},
   "source": [
    "## env config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba604305-ee44-42b7-b916-91ddd5d05354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.buffers import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc82383d-97ec-40f0-b9b3-4b8f08eb42a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 3#4\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.01\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9580c235-e152-4d5a-bd79-703710908934",
   "metadata": {},
   "source": [
    "# Hero agent def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "b5ae1e0a-62a8-4bb5-8e95-c10ff2c47c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "        #self.model = \n",
    "\n",
    "\n",
    "\n",
    "class Hero_agent(int):\n",
    "    def init_properties(self,agent_count,phases,cp=[],df=[],direct_action=True):\n",
    "        #self.draw_count = 0\n",
    "        self.init_win_count_iter(agent_count)\n",
    "        self.init_move_count_epi(phases)\n",
    "        self.cp = cp\n",
    "        self.df = df\n",
    "        self.direct_action = direct_action\n",
    "        self.init_reward_concern(agent_count,cp=cp,df=df)\n",
    "        \n",
    "    def init_reward_concern(self,agent_count,cp=[],df=[]):\n",
    "        if len(cp)==0:\n",
    "            cp = [int(self)]\n",
    "        self.concern=torch.tensor([(1 if i in cp \n",
    "                             else \n",
    "                             (-1 if i in df \n",
    "                                  else 0)) for i in range(1,agent_count+1) ])\n",
    "        #self.concern_2 = self.concern\n",
    "        #self.concern_2[self-1] =0\n",
    "        \n",
    "        self.multi_dependency = (sum(self.concern !=0)>1)\n",
    "        \n",
    "        \n",
    "    def init_model(self,model_name=\"DDQN_module\",\n",
    "                   kwarg = dict({})):\n",
    "        self.model = model_selector(model_name=model_name, \n",
    "                                    kwarg = kwarg)\n",
    "\n",
    "        \n",
    "    def init_win_count_iter(self,agent_count):\n",
    "        self.count_dict = {i:0 for i in range(1,agent_count+1)}\n",
    "        self.count_draw_dict = {i:0 for i in range(1,agent_count+1)}\n",
    "        self.draw_territory_count = 0\n",
    "    def init_move_count_epi(self,phases):\n",
    "        self.bad_move_count = 0\n",
    "        self.bad_move_phase_count = {i:0 for i in phases}\n",
    "        self.move_count =  {i:0 for i in phases}        \n",
    "    \n",
    "    def model_def(self, model):\n",
    "        self.model =model\n",
    "\n",
    "    def action_predict(self,save_R=True,return_R = False,action_masks = [],return_log_prob_a2 = False):\n",
    "        return self.model.action_predict(save_R=save_R,return_R = return_R, action_masks = action_masks,return_log_prob_a2 = return_log_prob_a2)\n",
    "\n",
    "    def action_predict_direct(self,data,return_R = False,return_log_prob_a2 = False):\n",
    "        return self.model.action_predict_direct(data,return_R = return_R,return_log_prob_a2 = return_log_prob_a2)\n",
    "    def save_models(self):\n",
    "        self.model.save_models()\n",
    "\n",
    "    def process_reward(self,rewards,step,hero_steps):\n",
    "        if self.multi_dependency and self.direct_action:\n",
    "            return (rewards*self.concern.to(rewards.device)).sum(-1)[:step][hero_steps][:,None]\n",
    "        elif self.multi_dependency and not self.direct_action:\n",
    "            base_rew = torch.zeros( rewards[:step,self-1][hero_steps].shape,require_grad=False)\n",
    "            #print(base_rew)\n",
    "\n",
    "            \n",
    "            hero_step_list  = np.arange(0,step)[hero_steps]\n",
    "            for i,j in zip(hero_step_list[:-1],hero_step_list[1:]):\n",
    "                if j-i>1:\n",
    "                    #print(j,i,rewards[i:j],(rewards[i:j]*self.concern),(rewards[i:j]*self.concern).sum())\n",
    "                    base_rew[i]+= (rewards[i:j]*self.concern).sum()\n",
    "            #print(base_rew,rewards[hero_step_list[-1]:],(rewards[hero_step_list[-1]:]*self.concern))\n",
    "            base_rew[-1]+= (rewards[hero_step_list[-1]:]*self.concern).sum()\n",
    "            \n",
    "            return base_rew[:,None]\n",
    "            \n",
    "        else:\n",
    "            return rewards[:step][hero_steps][:,None]\n",
    "\n",
    "\n",
    "    def init_model_config(self,args):\n",
    "        self.args = args\n",
    "\n",
    "    def init_memory_config(self,args):\n",
    "        self.args = args\n",
    "        \n",
    "    def init_loss_config(self,args):\n",
    "        self.args = args\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #def model_forward_call(self,name,kwarg):\n",
    "    #    return self.model_dict[name](**kwarg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1dfa91-b15b-4980-88de-9ed8268141d4",
   "metadata": {},
   "source": [
    "## Memory agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "735c5c81-ee35-4841-b5c0-90d9bda99995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.type_aliases import (\n",
    "    ReplayBufferSamples,\n",
    "    RolloutBufferSamples,\n",
    ")\n",
    "from stable_baselines3.common.preprocessing import get_action_dim, get_obs_shape\n",
    "\n",
    "\n",
    "from typing import Dict, Generator, NamedTuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "1e704166-6d0a-4889-aea1-9d064ffc5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBufferSamples(NamedTuple):\n",
    "    observations: th.Tensor\n",
    "    actions: th.Tensor\n",
    "    next_observations: th.Tensor\n",
    "    dones: th.Tensor\n",
    "    rewards: th.Tensor\n",
    "    mask: th.Tensor\n",
    "    t_pow: th.Tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca30ce-f9ad-4773-830d-516d42edf659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "061ddd3d-5216-47e3-ba43-2bbaa9c0e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReplayBuffer(ReplayBuffer):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.mask = np.zeros((self.buffer_size, 32), dtype=np.float32)  # Add a new array for custom feature\n",
    "        self.t_pow = np.zeros((self.buffer_size, 1), dtype=np.float32)\n",
    "\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, done, mask,t_pow):  # Add custom value when adding data\n",
    "\n",
    "        super().add(obs, action, reward, next_obs, done,{'t':t_pow})\n",
    "\n",
    "        self.mask[self.pos] = mask\n",
    "        self.t_pow[self.pos] = t_pow\n",
    "\n",
    "    def _get_samples(self,  batch_inds: np.ndarray, env: Optional[VecNormalize] = None) -> ReplayBufferSamples:\n",
    "        # ... (rest of the _get_samples code from SB3's ReplayBuffer) ...\n",
    "        #custom_value = self.custom_feature[batch_inds] # Retrieve infos\n",
    "\n",
    "\n",
    "\n",
    "        if self.optimize_memory_usage:\n",
    "            next_obs = self._normalize_obs(self.observations[(batch_inds + 1) % self.buffer_size, 0, :], env)\n",
    "        else:\n",
    "            next_obs = self._normalize_obs(self.next_observations[batch_inds, 0, :], env)\n",
    "\n",
    "        data = (\n",
    "            self._normalize_obs(self.observations[batch_inds, 0, :], env),\n",
    "            self.actions[batch_inds, 0, :],\n",
    "            next_obs,\n",
    "            self.dones[batch_inds],\n",
    "            self._normalize_reward(self.rewards[batch_inds], env),\n",
    "            self.mask[batch_inds],\n",
    "            self.t_pow[batch_inds]\n",
    "            \n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        #data = (observations, actions, next_observations, dones, rewards, custom_value) # Include infos\n",
    "        return ReplayBufferSamples(*tuple(map(self.to_torch, data))) # Or InfoReplayBufferSamples if you define it\n",
    "\n",
    "    def sample(self, batch_size: int, env: Optional[VecNormalize] = None) -> ReplayBufferSamples: # Optionally adjust return type hint\n",
    "        # ... (rest of the sample code from SB3) ...\n",
    "        \n",
    "        if not self.optimize_memory_usage:\n",
    "            return super().sample(batch_size=batch_size, env=env)\n",
    "        # Do not sample the element with index `self.pos` as the transitions is invalid\n",
    "        # (we use only one array to store `obs` and `next_obs`)\n",
    "        if self.full:\n",
    "            batch_inds = (np.random.randint(1, self.buffer_size, size=batch_size) + self.pos) % self.buffer_size\n",
    "        else:\n",
    "            batch_inds = np.random.randint(0, self.pos, size=batch_size)      \n",
    "        \n",
    "        \n",
    "        return self._get_samples(batch_inds, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "ac0a35aa-314c-425b-a650-534420f7638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class memory_agent(Hero_agent):\n",
    "    def init_path(self):\n",
    "        self.paths = []\n",
    "    def path_que(self, dtl):\n",
    "        \n",
    "        if (len(self.paths)==self.rb_len):\n",
    "            if (self.rb_len >1):\n",
    "                self.paths.pop(random.randrange(len(self.paths)-self.args.num_episodes +1 )) # dont pop the most recent experiences and ensure rb_len > num.episodes\n",
    "            else: \n",
    "                self.paths.pop()\n",
    "        self.paths.append(dtl)\n",
    "        \n",
    "    def create_training_dataset(self):\n",
    "        self.traj_dataset =  TrajectoryDataset_2_through_episodes(self.paths)  # a dataset of dataloaders\n",
    "\n",
    "        self.traj_data_loader = DataLoader(  # only spit 1 episode a time\n",
    "            self.traj_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=self.args.shuffle,\n",
    "            pin_memory=self.args.pin_memory,\n",
    "            drop_last=self.args.drop_last,\n",
    "            pin_memory_device=self.args.pin_memory_device,\n",
    "            )\n",
    "\n",
    "\n",
    "    def init_CL_sample_store(self,num_steps,total_agents,total_phases):\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        self.total_agents = total_agents\n",
    "        self.total_phases = total_phases\n",
    "        self.DT_input = {  # self.actor_config_dict['ob_space']\n",
    "                'timestep': torch.zeros((1,\n",
    "                                        ),requires_grad =False).to(self.device,\n",
    "                        dtype=torch.int),\n",
    "                'state': torch.zeros((1,\n",
    "                                     self.state_dim),requires_grad =False).to(self.device,dtype=torch.float32),\n",
    "                'action_1': torch.zeros((1,\n",
    "                                      )).to(self.device,dtype=torch.float32),\n",
    "                'action_2': torch.zeros((1,\n",
    "                                      )).to(self.device,dtype=torch.float32),\n",
    "                \n",
    "                'return_to_go': torch.ones((1,\n",
    "                                    ),requires_grad =False).to(self.device,dtype=torch.float32) * 110,\n",
    "                }            \n",
    "    \n",
    "        self.returntogo = torch.zeros((self.num_steps,\n",
    "                1),requires_grad =False).to(self.device,dtype=torch.float32)  # self.total_agents\n",
    "        self.returntogo_pred = torch.zeros((self.num_steps,\n",
    "                1)).to(self.device,dtype=torch.float32)  # self.total_agents\n",
    "\n",
    "    def update_CL_sample_store(\n",
    "        self,\n",
    "        curr_agent_,\n",
    "        inp={'step': None, 'act_2_1': [],'act_2_2': [], 'curr_reward_list': []},\n",
    "        before_action=True,\n",
    "        ):\n",
    "        self.DT_input['return_to_go'] = self.DT_input['return_to_go'] -    inp['curr_reward_list']  # [self.hero]\n",
    "        self.returntogo[inp['step']] = self.DT_input['return_to_go']\n",
    "\n",
    "\n",
    "    #rb = ReplayBuffer(\n",
    "    #    args.buffer_size,\n",
    "    #    envs.single_observation_space,\n",
    "    #    envs.single_action_space,\n",
    "    #    device,\n",
    "    #    handle_timeout_termination=False,\n",
    "    #)\n",
    "\n",
    "#real_next_obs = next_obs.copy()\n",
    "#rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "\n",
    "\n",
    "\n",
    "    def init_buffer(self,action_space,\n",
    "                    state_space=spaces_.Box(low=0,high=np.inf,shape=(#self.states.shape[1]\n",
    "                                                                     77,),dtype = np.float32)\n",
    "                    ,buffer_size = 30000):\n",
    "        action_space.dtype = np.float32\n",
    "        \n",
    "        self.rb = CustomReplayBuffer(\n",
    "                                    buffer_size,\n",
    "                                    state_space,\n",
    "                                    action_space,\n",
    "                                    self.device,\n",
    "                                    handle_timeout_termination=False,\n",
    "                                )\n",
    "    \n",
    "    def update_train_data(\n",
    "            self,\n",
    "            step_count,\n",
    "            obs,\n",
    "            ob_space_shape,\n",
    "            rewards_2,\n",
    "            dones_2,\n",
    "            actions_1,\n",
    "            actions_2,\n",
    "            log_probs_actions_2,\n",
    "            action_masks,\n",
    "            current_agent,\n",
    "            current_agent_acting,\n",
    "            current_phase,\n",
    "            current_troops_count,\n",
    "            map_agent_phase_vector\n",
    "            ):\n",
    "        \n",
    "\n",
    "        data_ = collections.defaultdict(torch.tensor)\n",
    "        #print(obs)\n",
    "        #print(obs[:step_count].reshape(-1,\n",
    "        #                            np.prod(ob_space_shape)))\n",
    "        \n",
    "        data_['observations'] =   obs[:step_count].reshape(-1,\n",
    "                                    np.prod(ob_space_shape))\n",
    "\n",
    "        data_['returntogo'] =      self.returntogo[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['returntogo_pred'] = self.returntogo_pred[:step_count]  # torch.tensor([1,2,3,4])\n",
    "\n",
    "        data_['rewards']   =       rewards_2[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['terminals'] =       dones_2[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['actions_1'] =       actions_1[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['actions_2'] =       actions_2[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['log_probs_actions_2'] =log_probs_actions_2[:step_count]\n",
    "        data_['action_masks'] =       action_masks[:step_count]\n",
    "        data_['current_agent_acting']=current_agent_acting[:step_count]\n",
    "        data_['current_agent_simple']=current_agent[:step_count]\n",
    "        data_['current_agent'] =      map_agent_phase_vector(current_agent[:step_count],\n",
    "                                   num_classes=self.total_agents + 1)[:, 1:]\n",
    "        data_['current_phase'] =      map_agent_phase_vector(current_phase[:step_count],\n",
    "                                   num_classes=self.total_phases)\n",
    "        data_['current_troops_count']=current_troops_count[:step_count]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        self.data_ = data_\n",
    "        hero_steps = data_['current_agent_simple'] == int(self)\n",
    "        self.data_['hero_steps'] = hero_steps\n",
    "        states = torch.cat((data_['observations'], data_['action_masks'] * hero_steps,\n",
    "                              data_['current_phase'], data_['current_agent'],\n",
    "                              data_['current_troops_count'][:, #:,\n",
    "                                                        None]), axis=1)#2)  # ,torch.ones(len(action_masks))[:,None]*self.hero\n",
    "\n",
    "        self.recalculate_rewards()\n",
    "        self.states = states.to(dtype=torch.float32)\n",
    "        data_['returns_to_go_cal'] = discount_cumsum(data_['rewards'], 0.99) / 1\n",
    "        data_['current_troops_count']=(data_['current_troops_count'] - 5.2496)/1.4733\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        data_['actions'] = torch.cat( (self.data_['actions_1'][:,None], \n",
    "                          self.data_['actions_2'][:,None]),axis =-1).to(dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        \n",
    "        for i in range(len(self.states)-1):\n",
    "            if hero_steps[i]:\n",
    "                self.rb.add(self.states[i], \n",
    "                       self.states[\n",
    "                       i+self.data_['t_pow'][i]], \n",
    "                       self.data_['actions'][i], \n",
    "                       self.data_['rewards_2'][i], \n",
    "                       self.data_['terminals_2'][i], \n",
    "                       self.data_['action_masks'][i],\n",
    "                       self.data_['t_pow'][i,None])\n",
    "        \n",
    "\n",
    "\n",
    "    def recalculate_rewards(self,gamma=0.99):\n",
    "\n",
    "\n",
    "        lis1 = []\n",
    "        lis2 = []\n",
    "        \n",
    "        self.data_['rewards_2'] = self.data_['rewards']*(self.data_['hero_steps'][:,0])\n",
    "        self.data_['t_pow'] = torch.ones_like(self.data_['rewards_2'],dtype =torch.int32)\n",
    "        for i,j in enumerate(self.data_['current_agent_simple'][:-1,0]):\n",
    "            if j != self.data_['current_agent_simple'][i+1]:\n",
    "                if j == 1:\n",
    "                    lis1.append(i+1)\n",
    "                if self.data_['current_agent_simple'][i+1]==1:\n",
    "                    lis2.append(i+1)\n",
    "                    \n",
    "        for i,j in zip(lis2,lis1):\n",
    "\n",
    "            ar = self.data_['rewards'][j:i]\n",
    "            self.data_['rewards_2'][j-1\n",
    "                        ] += ( ar*np.power([0.99],np.arange(len(ar)))).sum()\n",
    "            self.data_['t_pow'][j-1] = i-j+1+1\n",
    "\n",
    "        ar = self.data_['rewards'][lis1[-1]:] \n",
    "        self.data_['rewards_2'][lis1[-1]-1] = (ar*np.power([0.99],np.arange(len(ar)))).sum()\n",
    "\n",
    "        last = (len(self.data_['hero_steps'][:,0])-1 if self.data_['hero_steps'][-1,0]\n",
    "                else lis1[-1])\n",
    "        self.data_['terminals_2'] = np.arange(len(self.data_['hero_steps'][:,0]))==last\n",
    "        self.data_['t_pow'][lis1[-1]-1] = len(ar)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "3c99599d-2d5c-4007-bd00-fbcd066864b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces as spaces_\n",
    "#env.action_space(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "86a245bf-c571-4e36-8389-1aaeab8d0a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, gamma,Torch = True):\n",
    "    if Torch:\n",
    "        disc_cumsum = torch.zeros_like(x,dtype = torch.float32)\n",
    "    else:\n",
    "        disc_cumsum = np.zeros_like(x,dtype = np.float32)\n",
    "    \n",
    "    disc_cumsum[-1] = x[-1]\n",
    "    #print(disc_cumsum[-1])\n",
    "    for t in reversed(range(x.shape[0]-1)):\n",
    "        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t+1]\n",
    "        #print(x[t],disc_cumsum[t])\n",
    "    return disc_cumsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6e76f6-d16d-4df8-9b10-4bb7b589e9f8",
   "metadata": {},
   "source": [
    "## DDQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "d5fe8e3c-01ea-488f-9f7e-14b42ec68848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hero_agent_DDQN(memory_agent):\n",
    "    #def __init__(self):\n",
    "    #    super().__init__()\n",
    "\n",
    "    def set_device(self,device='cpu'):\n",
    "        self.device = device\n",
    "\n",
    "    def current_model_in(\n",
    "        self,\n",
    "        observation,\n",
    "        curr_agent,\n",
    "        phase_mapping,\n",
    "        curr_agent_mapping,\n",
    "        env_board_agents=[],\n",
    "        ):\n",
    "\n",
    "\n",
    "\n",
    "        #single obs\n",
    "        self.model_in =  torch.hstack((observation['observation'\n",
    "                         ].reshape(-1).to(self.device),\n",
    "                         torch.tensor(observation['action_mask'\n",
    "                         ].reshape(-1)).to(self.device) * (curr_agent\n",
    "                         == self), phase_mapping.to(self.device),\n",
    "                         curr_agent_mapping.to(self.device),\n",
    "                         ( (torch.tensor([env_board_agents[self].bucket]).to(self.device) - 5.2496)/1.4733\n",
    "                                      )))[None,\n",
    "                         :].float().requires_grad_(False).to(self.device)  \n",
    "    def init_model(self,actor_config_dict,args,device, writer,run_name,agent):\n",
    "\n",
    "        self.args = args\n",
    "        self.set_device(device)\n",
    "\n",
    "        self.state_dim = actor_config_dict['ob_space'] \n",
    "        self.act_dim = actor_config_dict['action_space']\n",
    "        \n",
    "        self.actor1 = ActorDiscrete(self.state_dim,self.act_dim).to(device)\n",
    "        self.actor2 = Actor(self.state_dim,self.act_dim).to(device)\n",
    "        \n",
    "        self.target_actor1 = ActorDiscrete(self.state_dim,self.act_dim).to(device)\n",
    "        self.target_actor1.load_state_dict(self.actor1.state_dict())\n",
    "        \n",
    "        self.target_actor2 = Actor(self.state_dim,self.act_dim).to(device)\n",
    "        self.target_actor2.load_state_dict(self.actor2.state_dict())\n",
    "        \n",
    "        self.qf1 = QNetwork(self.state_dim,2).to(device)\n",
    "        self.qf1_target = QNetwork(self.state_dim,2).to(device)\n",
    "        self.qf1_target.load_state_dict(self.qf1.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "        self.init_model_config(writer=writer\n",
    "                            ,agent=agent\n",
    "                            ,run_name=run_name\n",
    "                            ,args=self.args)\n",
    "        self.init_optim()\n",
    "\n",
    "    def action_predict(self,data,mask=[],use_action_mask=False,no_grad = 1):\n",
    "        if no_grad==1:\n",
    "            with torch.no_grad():\n",
    "                a1 = self.actor1(data,action_mask=mask,use_action_mask=use_action_mask)\n",
    "                a2 = self.actor2(data,a1)\n",
    "                v = self.qf1(data,a1[:,None],a2)\n",
    "        elif no_grad ==2:\n",
    "            a1 = self.actor1(data,action_mask=mask,use_action_mask=use_action_mask)\n",
    "            a2 = self.actor2(data,a1.clone().detach())\n",
    "            v = self.qf1(data,a1[:,None],a2)\n",
    "            \n",
    "        else:\n",
    "            a1 = self.actor1(data)\n",
    "            a2 = self.actor2(data,a1)\n",
    "            v = self.qf1(data,a1[:,None],a2)\n",
    "\n",
    "        return a1,a2,v\n",
    "\n",
    "    def init_model_config(self,writer\n",
    "                            ,agent\n",
    "                            ,run_name\n",
    "                            ,args):\n",
    "        self.writer = writer\n",
    "        self.hero = agent\n",
    "        self.args = args\n",
    "        self.run_name = run_name\n",
    "        self.set_device(args['device']) # config['device']\n",
    "        self.n_blocks = args['model_config']['n_blocks']\n",
    "        self.embed_dim = args['model_config']['embed_dim']\n",
    "        self.context_len = args['model_config']['context_len']\n",
    "        self.n_heads = args['model_config']['n_heads']\n",
    "        self.dropout_p = args['model_config']['dropout_p']\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.num_steps = args['num_steps']\n",
    "        self.total_agents = args['total_agents']\n",
    "        self.total_phases = args['total_phases']\n",
    "\n",
    "        #init other dep\n",
    "        self.init_memory_config(args)\n",
    "        self.init_loss_config(args)\n",
    "        self.init_optim()\n",
    "\n",
    "\n",
    "    def init_memory_config(self,args):\n",
    "        self.rb_len = args['model_config']['rb_len']\n",
    "        self.chunk_size = args['model_config']['chunk_size']\n",
    "        self.chunk_overlap = args['model_config']['chunk_overlap']\n",
    "        \n",
    "    def init_loss_config(self,args):\n",
    "        self.lr = args['learning_rate']\n",
    "        self.warmup_epoch = args['model_config']['warmup_epoch']\n",
    "        self.total_epoch = args['model_config']['total_epoch']\n",
    "        self.initial_lr =  args['model_config']['initial_lr'] #5e-4\n",
    "        self.final_lr =  args['model_config']['final_lr'] #1e-6\n",
    "        \n",
    "        self.tau = args['model_config']['tau']        \n",
    "        self.beta = args['model_config']['beta']         #0.2 #Q_mse\n",
    "        self.alpha =args['model_config']['alpha']          #0.1  #actionloss\n",
    "        self.entropy_coeff = args['model_config']['entropy_coeff']         #0.1#0.5   #entropy loss in action\n",
    "        self.val_loss_coeff = args['model_config']['val_loss_coeff']        #0.5      #Q loss\n",
    "\n",
    "        self.gamma = args['gamma'] #0.99\n",
    "        self.policy_frequency = args['model_config']['policy_frequency'] #10\n",
    "        self.tau= args['model_config']['tau']#0.15\n",
    "\n",
    "                    \n",
    "    def init_optim(self):\n",
    "        \n",
    "        self.q_optimizer = optim.Adam(list(self.qf1.parameters()), lr=self.lr)\n",
    "        \n",
    "        self.actor1_optimizer = optim.Adam(list(self.actor1.parameters()), lr=self.lr)\n",
    "        self.actor2_optimizer = optim.Adam(list(self.actor2.parameters()), lr=self.lr)\n",
    "\n",
    "\n",
    "\n",
    "    def init_win_count_iter(self,agent_count):\n",
    "        self.count_dict = {i:0 for i in range(1,agent_count+1)}\n",
    "        self.count_draw_dict = {i:0 for i in range(1,agent_count+1)}\n",
    "        self.draw_territory_count = 0\n",
    "    \n",
    "    def init_move_count_epi(self,phases):\n",
    "        self.bad_move_count = 0\n",
    "        self.bad_move_phase_count = {i:0 for i in phases}\n",
    "        self.move_count =  {i:0 for i in phases}  \n",
    "    \n",
    "    def train_outer():\n",
    "        pass\n",
    "    \n",
    "    def train(self,epoch,iteration, batch_size = 100):\n",
    "\n",
    "        losses_ret =dict({})\n",
    "        data= self.rb.sample(batch_size)\n",
    "        with torch.no_grad():\n",
    "            next_state_actions1 = self.target_actor1(data.next_observations,action_mask=[],use_action_mask=False,give_prob=False)\n",
    "            next_state_actions2 = self.target_actor2(data.next_observations,\n",
    "                                                     next_state_actions1)\n",
    "            qf1_next_target = self.qf1_target(data.next_observations, next_state_actions1[:,None], next_state_actions2)\n",
    "            next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * np.power(self.gamma,data.t_pow.flatten()) * (qf1_next_target).view(-1)\n",
    "\n",
    "        qf1_a_values = self.qf1(data.observations, data.actions[:,0,None], data.actions[:,1,None] ).view(-1)\n",
    "        qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "\n",
    "        # optimize the model\n",
    "        self.q_optimizer.zero_grad()\n",
    "        qf1_loss.backward()\n",
    "        self.q_optimizer.step()\n",
    "        print('epoch',epoch,'qf1_loss',qf1_loss)\n",
    "        \n",
    "        losses_ret['Q_loss'] = qf1_loss.item()\n",
    "        losses_ret['action_loss_cal'] = False\n",
    "        losses_ret['qf1_values'] = qf1_a_values.mean().item()\n",
    "\n",
    "        if epoch % self.policy_frequency == 0:\n",
    "            \n",
    "            acts_1 = self.actor1(data.observations,action_mask=data.mask,use_action_mask=True,give_prob=False).to(dtype=torch.float32)\n",
    "            acts_2 = self.actor2(data.observations,acts_1)\n",
    "            print(data.observations.dtype,acts_1.dtype,acts_2.dtype)\n",
    "            actor_loss = -self.qf1(data.observations, acts_1[:,None], acts_2#.to(dtype=float)\n",
    "                                  ).mean()\n",
    "            self.actor1_optimizer.zero_grad()\n",
    "            \n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            self.actor1_optimizer.step()\n",
    "\n",
    "            self.actor2_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            \n",
    "            self.actor2_optimizer.step()\n",
    "\n",
    "            print('actor_loss',actor_loss)\n",
    "            losses_ret['action_loss_cal'] = True\n",
    "            losses_ret['policy_loss'] = actor_loss.item()\n",
    "            \n",
    "            # update the target network\n",
    "            for param, target_param in zip(self.actor1.parameters(), self.target_actor1.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor2.parameters(), self.target_actor2.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "            \n",
    "            for param, target_param in zip(self.qf1.parameters(), self.qf1_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "        return losses_ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a = Hero_agent_DDQN(1)\n",
    "a.init_properties(3,[1,2,3],cp=[1],df=[2])\n",
    "a.set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bcf599-470d-4218-8f0c-56e1deb2f429",
   "metadata": {},
   "source": [
    "# Sample and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "bdad0956-fb60-4c9d-99c0-9b8ab15fb537",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#q_optimizer = optim.Adam(list(qf1.parameters()), lr=args.learning_rate)\n",
    "\n",
    "#actor1_optimizer = optim.Adam(list(actor1.parameters()), lr=args.learning_rate)\n",
    "#actor2_optimizer = optim.Adam(list(actor2.parameters()), lr=args.learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "def agent_action(observation, reward, termination, truncation, info, phase, troops,env,agent,device='cpu'):\n",
    "    \n",
    "    action = env.action_space(agent).sample()                        \n",
    "    #part_0 =np.random.choice(np.where(env.board.calculated_action_mask(agent))[0])\n",
    "    part_0 =np.random.choice(np.where(observation['action_mask'])[0])\n",
    "    action = torch.tensor([\n",
    "                            [\n",
    "                             [part_0],\n",
    "                             [np.around(action[1],2)]\n",
    "                            ]\n",
    "                            ],requires_grad =False).to(device)\n",
    "    \n",
    "    action = action[:,:,0]\n",
    "    action_1 = action[:,0]\n",
    "    action_2 = action[:,1]\n",
    "    act_2_1 = action_1[0]\n",
    "    act_2_2 = action_2[0]\n",
    "\n",
    "    return [act_2_1.clone().detach().cpu().item(), max(act_2_2.clone().detach().cpu().item(),0.001) ]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class sample_and_train(object):\n",
    "    def __init__(self, args,seed=42):\n",
    "        self.args = args\n",
    "        \n",
    "        self.env_config = args['env_config']\n",
    "        self.env = env_risk(**self.env_config)\n",
    "        self.device = args['device']#'cpu'\n",
    "        self.total_agents = len(self.env.possible_agents)\n",
    "        self.total_phases= len(self.env.phases)\n",
    "        self.hero_agents_list_ = [1]\n",
    "        self.the_hero_agent = 1\n",
    "\n",
    "\n",
    "        self.args['total_agents'] = self.total_agents \n",
    "        self.args['total_phases'] = self.total_phases \n",
    "\n",
    "\n",
    "        self.action_shape = (2,)\n",
    "\n",
    "        self.env.reset(seed=42)\n",
    "        self.total_agents  = len(self.env.possible_agents)\n",
    "        self.total_phases = len(self.env.phases)\n",
    "        self.playe_r = 1\n",
    "\n",
    "\n",
    "\n",
    "        #new_params:\n",
    "        \n",
    "        self.episode_time_lim = args['episode_time_lim']#3000\n",
    "        self.num_iterations = args['num_iterations']#10\n",
    "        self.batch_size = args['model_config']['chunk_size']#100\n",
    "        self.num_episodes = args['num_episodes']#4\n",
    "        self.num_steps = self.num_episodes*self.num_iterations*self.episode_time_lim #120000\n",
    "        \n",
    "        self.run_name = f\"{self.args['env_id']}__{self.args['exp_name']}__{self.args['seed']}__{int(time.time())}\"\n",
    "\n",
    "        TB_log = self.args['TB_log'] \n",
    "        if TB_log:    \n",
    "            self.writer = SummaryWriter(f\"runs/{self.run_name}\")\n",
    "            self.writer.add_text(\n",
    "                \"hyperparameters\",\n",
    "                \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in self.args.items()])),\n",
    "            )\n",
    "        else:\n",
    "            self.writer = None\n",
    "\n",
    "        sample_obs = self.obs_converter(torch.tensor(self.env.last()[0]['observation']\n",
    "                                        ).to(device=self.device),\n",
    "                                        num_classes = self.total_agents+1\n",
    "                                       )\n",
    "        \n",
    "        self.ob_space_shape = sample_obs.shape #env.observation_space(playe_r)['observation'].shape\n",
    "        self.action_mask_shape = self.env.observation_space(self.playe_r)['action_mask'].shape\n",
    "        self.agent_list = list(self.env.possible_agents)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.actor_config_dict =  dict(env=self.env,\n",
    "                        action_space = self.env.observation_space(self.playe_r)['action_mask'].shape[0],\n",
    "                        ob_space=(np.prod(self.ob_space_shape)\n",
    "                         +np.prod(self.action_mask_shape)\n",
    "                         +1*( self.total_agents-1) #the current_agent +1#who actor agent was\n",
    "                         +1*(self.total_phases -1)#the current phase\n",
    "                         +1) # the number of troops\n",
    "                               )\n",
    "        self.init_hero_agent()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def init_hero_agent(self):\n",
    "        self.hero_agents_list = {i:Hero_agent_DDQN(i) for i in \n",
    "                                 self.hero_agents_list_ } # this is a list , need to pass it as an argument\n",
    "\n",
    "        \n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].init_properties(self.total_agents,self.env.phases)\n",
    "            self.hero_agents_list[i].set_device()\n",
    "\n",
    "            #init_model(self,actor_config_dict,device)\n",
    "            \n",
    "            self.hero_agents_list[i].init_model(#model_name='DDPG', \n",
    "                                                #kwarg = dict( \n",
    "                                                actor_config_dict = self.actor_config_dict,\n",
    "                                                args = self.args,\n",
    "                                                device = self.device,\n",
    "                                                writer=self.writer,\n",
    "                                                run_name=self.run_name,\n",
    "                                                agent=i#)\n",
    "                                                )\n",
    "\n",
    "\n",
    "    def init_per_epi_data(self):\n",
    "        selector = self.episode_time_lim#self.num_steps\n",
    "        obs = torch.zeros((selector,) + self.ob_space_shape, requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "        actions_1 = torch.zeros((selector,) ).to(self.device,    dtype = torch.float32)\n",
    "        actions_2 = torch.zeros( (selector,)).to(self.device,    dtype = torch.float32)\n",
    "        log_probs_actions_2 = torch.zeros( (selector,)).to(self.device,    dtype = torch.float32)\n",
    "        action_masks = torch.zeros((selector, ) + self.action_mask_shape, requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "        current_agent = torch.zeros((selector,1), requires_grad =False).to(self.device,    dtype = torch.float32)*0#-1\n",
    "        current_agent_acting = torch.ones((selector,1), requires_grad =False).to(self.device,    dtype = torch.float32)*0\n",
    "        current_phase = torch.zeros((selector,1), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "        current_troops_count = torch.zeros((selector,self.total_agents), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "        #logprobs = torch.zeros((self.num_steps, ), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "        rewards = torch.zeros((selector, self.total_agents), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "        rewards_2 = torch.zeros((selector, self.total_agents), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "        returntogo = torch.zeros((selector, self.total_agents), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "        dones = torch.zeros((selector, self.total_agents), requires_grad =False).to(self.device)\n",
    "        dones_2 = torch.zeros((selector, self.total_agents), requires_grad =False).to(self.device)\n",
    "        #values = torch.zeros((self.num_steps,  )).to(self.device)\n",
    "        episodes = torch.ones((selector, ), requires_grad =False).to(self.device,    dtype = torch.float32)*-1\n",
    "        t_next = torch.zeros((selector, self.total_agents), requires_grad =False).to(self.device,    dtype = torch.float32) \n",
    "        total_rewards = {i:0 for i in self.env.possible_agents} #i can report this\n",
    "        #trace = tensor.zeros((self.context_len,self.qnet_config_dict['ob_space']))\n",
    "        action=1\n",
    "        return (\n",
    "                    obs,actions_1,actions_2,log_probs_actions_2,action_masks,\n",
    "                    current_agent,current_agent_acting,current_phase,current_troops_count,\n",
    "                    rewards,rewards_2,returntogo,dones,dones_2,episodes,t_next,total_rewards,\n",
    "                    action \n",
    "                )\n",
    "\n",
    "    def reset_moves_hero_agents(self):\n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].init_move_count_epi(self.env.phases)\n",
    "\n",
    "\n",
    "    def run_learning_loop(self):\n",
    "\n",
    "        self.env.reset(seed=self.args['seed'])\n",
    "        random.seed(self.args['seed'])\n",
    "        np.random.seed(self.args['seed'])\n",
    "        torch.manual_seed(self.args['seed'])\n",
    "        torch.backends.cudnn.deterministic = self.args['torch_deterministic']\n",
    "\n",
    "        \n",
    "        self.train_loop_init()\n",
    "        curren_epi =0\n",
    "        self.set_memories()\n",
    "        self.training_performance_return = []\n",
    "        \n",
    "        for iteration in range(1, self.num_iterations+1):\n",
    "            curren_epi,global_break, local_break = self.sample_train(curren_epi,iteration)\n",
    "            if global_break:\n",
    "                print('global_break_3')\n",
    "                break \n",
    "\n",
    "    def set_memories(self):\n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].init_buffer(action_space = self.env.action_space(1),\n",
    "                                                 buffer_size=self.args['model_config']['rb_len'])\n",
    "    def sample_train(self,curren_epi,iteration):\n",
    "        step = 0 #steps per iteration\n",
    "        fault_condition = False\n",
    "        clear_output(wait=True)\n",
    "        phase = 0\n",
    "        \n",
    "        for episode in range(self.num_episodes):\n",
    "            \n",
    "            \n",
    "            step, fault_condition,global_break,local_break = self.sample(iteration,curren_epi,step,fault_condition,clear_output,phase)\n",
    "            self.train(curren_epi,iteration)\n",
    "            curren_epi+=1\n",
    "            \n",
    "            if global_break:\n",
    "                print('global_break_2')\n",
    "                break \n",
    "        return curren_epi, global_break, local_break\n",
    "        \n",
    "    def train(self,episode_count,iteration):\n",
    "        losses = dict({})\n",
    "        for i in self.hero_agents_list:\n",
    "            if self.hero_agents_list[i].rb.size()>=self.batch_size:\n",
    "                losses[i] = self.hero_agents_list[i].train(episode_count,iteration, batch_size = self.batch_size)\n",
    "        \n",
    "        self.write_learning(episode_count,losses)\n",
    "                \n",
    "    \n",
    "    def sample(self,iteration,curren_epi,step,fault_condition,clear_output,phase):\n",
    "\n",
    "        self.hero_agents_list[1].init_optim()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            if fault_condition:\n",
    "                self.env = env_risk(**(self.env_config  #| {\"render_mode\" : None,\"bad_mov_penalization\" : 0.01,\"render_\":False#False\n",
    "                                                        # }\n",
    "                                     ))\n",
    "                self.env.observation_space(1)['observation'].dtype =np.float32\n",
    "\n",
    "\n",
    "            \n",
    "            self.env.reset(seed=self.args['seed'])\n",
    "            \n",
    "            self.reset_moves_hero_agents()\n",
    "            fault_condition = False\n",
    "            step_count = 0\n",
    "            is_draw = 0\n",
    "\n",
    "\n",
    "            (obs,actions_1,actions_2,log_probs_actions_2,action_masks,\n",
    "            current_agent,current_agent_acting,current_phase,current_troops_count,\n",
    "            rewards,rewards_2,returntogo,dones,dones_2,episodes,t_next,total_rewards,\n",
    "            action \n",
    "            ) = self.init_per_epi_data()\n",
    "\n",
    "            for i in self.hero_agents_list:\n",
    "                self.hero_agents_list[i].init_CL_sample_store(self.num_steps\n",
    "                                                             ,self.total_agents,self.total_phases\n",
    "                                                             )\n",
    "            \n",
    "\n",
    "            #observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            #iter_ = 0\n",
    "            \n",
    "            for agent in self.env.agent_iter():\n",
    "                e_t = self.env.terminations\n",
    "                if sum(e_t.values()) <(self.env.max_num_agents-1):\n",
    "                    observation, reward, termination, truncation, info = self.env.last()\n",
    "\n",
    "                    (phase,troops,mask,observation,curr_agent,\n",
    "                     phase_mapping,curr_agent_mapping) =self.process_obs(observation, reward, termination, truncation, info,agent)\n",
    "\n",
    "                    self.hero_agents_list[self.the_hero_agent].current_model_in(observation,\n",
    "                                curr_agent,\n",
    "                                phase_mapping,\n",
    "                                curr_agent_mapping,\n",
    "                                env_board_agents=self.env.board.agents\n",
    "                                      )\n",
    "                    \n",
    "\n",
    "                    #update records\n",
    "                    episodes[step_count] = curren_epi\n",
    "                    obs[step_count] = observation['observation'] #torch.Tensor(observation['observation']).to(self.device) #sould i not add it .... meaning this is the last observation after the player dies\n",
    "                    action_masks[step_count] = torch.Tensor(observation['action_mask']).to(self.device)\n",
    "                    current_agent[step_count] = curr_agent\n",
    "                    current_phase[step_count] = phase = self.env.phase_selection\n",
    "                    current_troops_count[step_count] = torch.tensor([self.env.board.agents[i].bucket \n",
    "                                                                     for i in self.env.possible_agents],requires_grad =False).to(self.device)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    #ACTIONS\n",
    "                    if agent not in self.hero_agents_list:\n",
    "                        action_ = agent_action(observation, reward, termination, truncation, info, phase, troops,self.env,agent)\n",
    "                    else:\n",
    "                        \n",
    "                        \n",
    "                        a1,a2,v = self.hero_agents_list[self.the_hero_agent].action_predict(self.hero_agents_list[\n",
    "                                                                                                    self.the_hero_agent].model_in,\n",
    "                                                                                                    mask=mask,\n",
    "                                                                                                    use_action_mask=True, #will have to check this later\n",
    "                                                                                                    no_grad = 1)   \n",
    "                                                                                                    \n",
    "                        action_ = [int(a1),a2[0,0]]\n",
    "                        \n",
    "                        #print(action_)\n",
    "\n",
    "\n",
    "                    actions_1[step_count] = action_[0]\n",
    "                    actions_2[step_count] = action_[1]\n",
    "                    curr_agent_ = int(curr_agent)\n",
    "\n",
    "                    if not observation['action_mask'][int(action_[0])]: \n",
    "                        fault_condition =True\n",
    "                        if  curr_agent_ in self.hero_agents_list:\n",
    "                            self.hero_agents_list[curr_agent_].bad_move_count+=1\n",
    "                            self.hero_agents_list[curr_agent_].bad_move_phase_count[int(current_phase[step_count][0])]+=1  # when is the where_is_it_performing_bad_really\n",
    "                            #print('here',agent, action, observation['action_mask'])\n",
    "                    if  curr_agent_ in self.hero_agents_list:\n",
    "                        self.hero_agents_list[curr_agent_].move_count[int(current_phase[step_count][0])]+=1  \n",
    "\n",
    "\n",
    "\n",
    "                    curr_reward_list,is_draw = self.handle_episode_rewards(curr_agent,step_count,is_draw)\n",
    "\n",
    "                    rewards_2[step_count] = torch.tensor([curr_reward_list[i] for i in self.env.possible_agents]).to(self.device,dtype=torch.float32)\n",
    "                    if step >1:\n",
    "                        dones_2[step_count] = torch.tensor([ int(self.env.terminations[i]) - dones_2[step_count-1,i-1]  for i in self.env.possible_agents]).to(self.device)\n",
    "                    else:\n",
    "                        dones_2[step_count] = torch.tensor([self.env.terminations[i] for i in self.env.possible_agents]).to(self.device)\n",
    "\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    #env step\n",
    "                    self.env.step(action_)\n",
    "\n",
    "                    #increase step\n",
    "                    for age_i in self.env.possible_agents:\n",
    "                        total_rewards[age_i]+=curr_reward_list[age_i] #env.curr_rewards[age_i] if (step_count != episode_time_lim) else -100\n",
    "                                   \n",
    "                    \n",
    "                    step +=1\n",
    "                    self.global_step+=1\n",
    "\n",
    "                else:\n",
    "                    self.training_performance_return.append(total_rewards[1])\n",
    "                    print('done:',self.env.terminations,#env.terminations.values(),\n",
    "                           \",total_reward:\",total_rewards, ',iteration:',iteration,\",episode:\", curren_epi )\n",
    "                    print(self.env.board.territories)\n",
    "                    break\n",
    "                    \n",
    "                step_count+=1\n",
    "\n",
    "                if self.global_step == self.num_steps:\n",
    "                    print('global_break_1')\n",
    "                    print('done:',self.env.terminations,#env.terminations.values(),\n",
    "                           \",total_reward:\",total_rewards, ',iteration:',iteration,\",episode:\", curren_epi )\n",
    "                    print(self.env.board.territories)\n",
    "                    self.training_performance_return.append(total_rewards[1])\n",
    "                    break\n",
    "\n",
    "                if step_count == self.episode_time_lim:\n",
    "                    print('local_break_1')\n",
    "                    print('done:',self.env.terminations,#env.terminations.values(),\n",
    "                           \",total_reward:\",total_rewards, ',iteration:',iteration,\",episode:\", curren_epi )\n",
    "                    print(self.env.board.territories)\n",
    "                    self.training_performance_return.append(total_rewards[1])\n",
    "                    break \n",
    "                \n",
    "\n",
    "                cur_epi_list = (episodes == curren_epi)\n",
    "\n",
    "            #print(obs)\n",
    "            self.find_player_position(total_rewards)\n",
    "            \n",
    "            if self.args['TB_log']:\n",
    "                    self.write_exploring(is_draw,#position,\n",
    "                            curren_epi,step,\n",
    "                            step_count,\n",
    "                            total_rewards,#bad_move_count\n",
    "                            #,bad_move_phase_count,\n",
    "                            #move_count,\n",
    "                            observation,\n",
    "                            self.env,\n",
    "                            cur_epi_list,\n",
    "                            current_agent_acting)\n",
    "\n",
    "            \n",
    "            \n",
    "            print(len(actions_1[:step_count]), len(actions_1[cur_epi_list]))\n",
    "            for i in self.hero_agents_list:\n",
    "                #print(obs)\n",
    "                self.hero_agents_list[i].update_train_data(\n",
    "                         step_count,\n",
    "                         obs,\n",
    "                            self.ob_space_shape,\n",
    "                            rewards_2[:,i-1][:step_count],\n",
    "                            dones_2[:,i-1][:step_count],\n",
    "                            actions_1[:step_count],\n",
    "                            actions_2[:step_count],\n",
    "                            log_probs_actions_2[:step_count],\n",
    "                            action_masks[:step_count],\n",
    "                            current_agent[:step_count],\n",
    "                            current_agent_acting[:step_count],\n",
    "                            current_phase[:step_count],\n",
    "                            current_troops_count[:,i-1][:step_count],\n",
    "                            map_agent_phase_vector = self.map_agent_phase_vector,\n",
    "                            \n",
    "                         )\n",
    "\n",
    "\n",
    "        return step, fault_condition, (self.global_step == self.num_steps),(step_count == self.num_steps)\n",
    "                #print(iter_)\n",
    "                #iter_+=1\n",
    "\n",
    "    def handle_episode_rewards(self,curr_agent,step_count,is_draw):\n",
    "        curr_reward_list =  self.env.curr_rewards\n",
    "                            \n",
    "        if (step_count == (self.episode_time_lim-1))   or (self.global_step == (self.num_steps-1)): # draw reward\n",
    "            is_draw=1\n",
    "            curr_reward_list = {i:-100 for i in self.env.possible_agents }\n",
    "\n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].update_CL_sample_store(curr_agent_=curr_agent,\n",
    "                          inp = {'step':step_count,\n",
    "                                 'act_2_1':None ,\n",
    "                                 'act_2_2':None ,\n",
    "                                 'curr_reward_list':curr_reward_list[i]\n",
    "                          },before_action=4)\n",
    "\n",
    "        return curr_reward_list,is_draw\n",
    "        \n",
    "\n",
    "    def find_player_position(self,total_rewards):\n",
    "        for i in self.hero_agents_list:\n",
    "                    self.hero_agents_list[i].position =self.total_agents\n",
    "                    \n",
    "                    \n",
    "        #[ position = 3 for i in ] \n",
    "        for k_,(i_,j_) in enumerate(sorted([(b_,a_) for a_,b_ in total_rewards.items()],reverse=True) \n",
    "              ):\n",
    "            if int(j_) in self.hero_agents_list:\n",
    "                self.hero_agents_list[int(j_)].position = k_+1\n",
    "                print(j_,self.hero_agents_list[int(j_)].position)\n",
    "    \n",
    "    \n",
    "    def process_obs(self,observation, reward, termination, truncation, info,agent):\n",
    "\n",
    "        phase = self.env.phase_selection\n",
    "        troops = self.env.board.agents[agent-1].bucket\n",
    "        \n",
    "        mask = observation[\"action_mask\"]\n",
    "\n",
    "\n",
    "        observation['observation'] =  self.obs_converter(\n",
    "                                            torch.tensor(\n",
    "                                                observation['observation']\n",
    "                                            ).to(self.device,dtype=torch.float32),\n",
    "                                            num_classes = self.total_agents+1)\n",
    "        observation['observation'][:,-1]  =  (observation['observation'][:,-1] - 5.2496)/1.4733\n",
    "        curr_agent = agent\n",
    "        phase_mapping = self.map_agent_phase_hot(phase,num_classes = self.total_phases).float()\n",
    "        curr_agent_mapping = self.map_agent_phase_hot(int(curr_agent)-1,\n",
    "                                                          num_classes = self.total_agents \n",
    "                                                         ).float()\n",
    "\n",
    "        return (phase,troops,mask,observation,curr_agent,\n",
    "                phase_mapping,curr_agent_mapping)\n",
    "\n",
    "        \n",
    "    \n",
    "    def obs_converter(self,  data, num_classes = 4, col =0 ):\n",
    "\n",
    "        if col != None:\n",
    "\n",
    "            #print(data.device)\n",
    "            #print(nn.functional.one_hot(data[:4,col].detach().long(), \n",
    "            #                                            num_classes = num_classes).to(self.device))\n",
    "            return torch.concat((nn.functional.one_hot(data[:,col].detach().long(), \n",
    "                                                        num_classes = num_classes).to(self.device),\n",
    "                                      data[:,~col,None]\n",
    "                                ),axis=1\n",
    "                               )[:,1:].to(self.device)\n",
    "    \n",
    "    def map_agent_phase_hot(self, data,num_classes = 3):\n",
    "        with torch.no_grad():\n",
    "            return nn.functional.one_hot(torch.tensor(data),num_classes = num_classes)[1:].to(self.device)\n",
    "    \n",
    "    def map_agent_phase_vector(self, data,num_classes = 3):\n",
    "        with torch.no_grad():\n",
    "            return nn.functional.one_hot(data[:,0].long(), \n",
    "                                         num_classes = num_classes)[:,1:].to(self.device)\n",
    "\n",
    "\n",
    "    def train_loop_init(self):\n",
    "        self.gamma_t = {i:0 for i in self.env.possible_agents}\n",
    "        \n",
    "        \n",
    "        self.draw_count = 0\n",
    "\n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].init_win_count_iter(self.total_agents )\n",
    "            self.hero_agents_list[i].init_path()\n",
    "        \n",
    "        self.start_time = time.time()\n",
    "        self.global_step = 0\n",
    "        #self.faulting_player = \"\"\n",
    "\n",
    "    def write_learning(self,episode,losses):\n",
    "        \n",
    "        for i in losses:\n",
    "            self.writer.add_scalar(\"charts/Q_learning_rate\", self.hero_agents_list[i].q_optimizer.param_groups[0][\"lr\"], episode)   \n",
    "            self.writer.add_scalar(\"charts/A1_learning_rate_value\", self.hero_agents_list[i].actor1_optimizer.param_groups[0][\"lr\"], episode)   \n",
    "            self.writer.add_scalar(\"charts/A2_learning_rate_value\", self.hero_agents_list[i].actor2_optimizer.param_groups[0][\"lr\"], episode) \n",
    "\n",
    "            #self.writer.add_scalar(\"total_loss\", np.mean(total_loss_list), episode)\n",
    "            #self.writer.add_scalar(\"Q_TD\", np.mean(Q_TD_list), episode)\n",
    "            self.writer.add_scalar(\"losses/qf1_values\", losses[i]['qf1_values'], episode)\n",
    "            self.writer.add_scalar(\"losses/Q_loss\", losses[i]['Q_loss'], episode)\n",
    "            if losses[i]['action_loss_cal']:\n",
    "                self.writer.add_scalar(\"losses/policy_loss\", losses[i]['policy_loss'], episode)\n",
    "            #self.writer.add_scalar(\"policy_loss\", np.mean(policy_loss_list), episode)    \n",
    "                    \n",
    "        \n",
    "        \n",
    "\n",
    "    def write_exploring(self,is_draw,#position,\n",
    "                        curren_epi,step,\n",
    "                        step_count,\n",
    "                        total_rewards,#bad_move_count\n",
    "                        #,bad_move_phase_count,\n",
    "                        #move_count,\n",
    "                        observation,\n",
    "                        env,\n",
    "                        cur_epi_list,\n",
    "                        current_agent_acting):\n",
    "\n",
    "        if is_draw:\n",
    "            self.draw_count +=1\n",
    "\n",
    "            for i in self.hero_agents_list:\n",
    "\n",
    "                self.writer.add_scalar(f\"draw_charts_agent_{i}/position_draw\",self.hero_agents_list[i].position\n",
    "                                                                                            ,self.draw_count) #draw_count is the number of draw episodes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                self.hero_agents_list[i].draw_territory_count = int((self.env.board.territories[:,0] ==i).sum()) #this is the total number of states\n",
    "                \n",
    "                self.hero_agents_list[i].count_draw_dict[\n",
    "                                                        self.hero_agents_list[i].position\n",
    "                                                        ] +=1\n",
    "                self.writer.add_scalar(f\"draw_charts_agent_{i}/draw_territory_count\",\n",
    "                                                                               self.hero_agents_list[i].draw_territory_count,\n",
    "                                                                               self.draw_count)#self.global_step)\n",
    "                \n",
    "                for j in self.hero_agents_list[i].count_draw_dict:\n",
    "                    self.writer.add_scalar(f\"draw_charts_agent_{i}/{j}_position_prop_draw\",int(\n",
    "                                                                                            self.hero_agents_list[i].position==j\n",
    "                                                                                            ),self.draw_count)\n",
    "\n",
    "                    if j not in [1,2]:\n",
    "                        self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_all_prop\",int(\n",
    "                                                                                            self.hero_agents_list[i].position==j\n",
    "                                                                                            ),(curren_epi+1))\n",
    "\n",
    "                        self.writer.add_scalar(f\"draw_charts_agent_{i}/{j}_place_in_draw\",\n",
    "                                                                   self.hero_agents_list[i].count_draw_dict[j],\n",
    "                                                                   self.draw_count)\n",
    "\n",
    "                        self.writer.add_scalar(f\"draw_charts_agent_{i}/{j}_place_in_draw_ratio\",\n",
    "                                                                   self.hero_agents_list[i].count_draw_dict[j]/self.draw_count,\n",
    "                                                                   self.draw_count)\n",
    "                        \n",
    "                    \n",
    "            self.writer.add_scalar(\"draw_charts/draw_count\",self.draw_count,(curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(\"draw_charts/draw\",1,(curren_epi+1))\n",
    "            self.writer.add_scalar(\"draw_charts/draw_to_total_count\",self.draw_count/(curren_epi +1+0.000001),(curren_epi +1))#self.global_step)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            non_draw_count =(curren_epi-self.draw_count+1+0.000001)\n",
    "            for i in self.hero_agents_list:\n",
    "\n",
    "                self.writer.add_scalar(f\"win_charts_agent_{i}/position_win\",self.hero_agents_list[i].position\n",
    "                                                                                            ,(curren_epi+1))\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                self.hero_agents_list[i].count_dict[self.hero_agents_list[i].position\n",
    "                                               ] +=1\n",
    "\n",
    "                \n",
    "                for j in self.hero_agents_list[i].count_dict:\n",
    "                    self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_prop\",int(\n",
    "                                                            self.hero_agents_list[i].position==j\n",
    "                                                            ),(curren_epi+1))\n",
    "\n",
    "                    self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position\",self.hero_agents_list[i].count_dict[j],\n",
    "                                           (curren_epi+1))\n",
    "\n",
    "                    self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_to_total_terminated\",self.hero_agents_list[i].count_dict[j]/non_draw_count,(curren_epi+1))\n",
    "\n",
    "                    if j not in [1,2]:\n",
    "                        self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_all_prop\",int(\n",
    "                                                                                            self.hero_agents_list[i].position==j\n",
    "                                                                                            ),(curren_epi+1))\n",
    "\n",
    "\n",
    "            self.writer.add_scalar(\"draw_charts/draw\",0,(curren_epi+1))\n",
    "            \n",
    "        for i in self.hero_agents_list:\n",
    "\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/model_move_count_per_episode\",   sum(current_agent_acting[:step_count] ==i)  ,  (curren_epi +1))\n",
    "\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/model_2_total_move_count_per_episode\",   sum(current_agent_acting[:step_count] ==i)/sum( self.hero_agents_list[i].move_count.values()),  (curren_epi +1))\n",
    "\n",
    "            self.writer.add_scalar(f\"win_charts_agent_{i}/position_all\",self.hero_agents_list[i].position\n",
    "                                                                                            ,(curren_epi+1))\n",
    "\n",
    "            \n",
    "            for j in self.hero_agents_list[i].count_dict:\n",
    "                self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_to_total\",(\n",
    "                                                            self.hero_agents_list[i].count_dict[j]+\n",
    "                                                            self.hero_agents_list[i].count_draw_dict[j]\n",
    "                                                        \n",
    "                                                            )/(curren_epi +1+0.00001 ),(curren_epi+1))#global_step)\n",
    "\n",
    "            \n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_count_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_count,            (curren_epi +1))#self.global_step)\n",
    "                \n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_count_position_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[0],            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_count_attack_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[1],            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_count_fortify_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[2],            (curren_epi +1))#self.global_step)\n",
    "            \n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/total_moves\",sum(\n",
    "                                                    self.hero_agents_list[i].move_count.values()),            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_to_step_count_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_count/(sum(\n",
    "                                                   self.hero_agents_list[i].move_count.values())+1),            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_to_step_position_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[0]/( \n",
    "                                                   self.hero_agents_list[i].move_count[0]+1),            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_to_step_attack_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[1]/( \n",
    "                                                   self.hero_agents_list[i].move_count[1]+1),            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_to_step_fortify_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[2]/( \n",
    "                                                   self.hero_agents_list[i].move_count[2]+1),            (curren_epi +1))#self.global_step)\n",
    "\n",
    "        \n",
    "        self.writer.add_scalar(\"charts/epsilon\",(curren_epi/((self.num_iterations*self.num_episodes)/10)),(curren_epi +1))#self.global_step)\n",
    "        self.writer.add_scalar(\"charts/avg_per_epi_total_reward\", np.mean(list(total_rewards.values())), (curren_epi +1))#self.global_step)\n",
    "\n",
    "        \n",
    "\n",
    "        #values_total = {i:0 for i in self.env.possible_agents}\n",
    "        \n",
    "\n",
    "        \n",
    "        self.writer.add_scalar(\"charts/episodic_length\", cur_epi_list[:step].sum(), (curren_epi +1))#self.global_step)\n",
    "        \n",
    "        for i in env.possible_agents:\n",
    "            #cur_index = torch.where((current_agent[:,0] == i) &( cur_epi_list ))[0]\n",
    "\n",
    "            #values_total[i] = values[cur_index].mean()\n",
    "            #writer.add_scalar(\"charts/mean_value_per_epi_agent_\"+str(i), values_total[i], global_step)\n",
    "            \n",
    "            self.writer.add_scalar(\"charts/total_reward_per_epi_agent_\"+str(i), total_rewards[i], (curren_epi +1))#self.global_step)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "976cc85f-dfe0-4434-81db-377221ecfe23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=object)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smp.env.board.territories[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "45d43ab9-3b84-4f25-9c56-157d2cc494dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000000"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "500*10*3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "f356ac27-2501-4b00-8f30-54998c06e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp_12 = dict(\n",
    "exp_name = '_exp2_ddpg_4_agents_1_hero_norm_small',\n",
    "env_id = 'Tiny_risk',\n",
    "learning_rate = 0.003,#5e-4,#0.003,#0.0003,\n",
    "batch_size = 2,\n",
    "gamma = 0.99,\n",
    "seed = 42,\n",
    "\n",
    "torch_deterministic= True,\n",
    "num_steps=1600000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 3000,#10000,\n",
    "hero_agent_maping = [1,0,0],\n",
    "model_name={1:\"ddpg\"}#,2:'transformer_model'}\n",
    ",entropy=True,\n",
    "return_prob=2,\n",
    "actor_wt = 0.5,\n",
    "CE_wt = 0.01,\n",
    "small = True,\n",
    "num_episodes = 10,\n",
    "context_len = 256,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=False,#False,\n",
    "drop_last=True,\n",
    "TB_log=True,\n",
    "learning_starts =100,\n",
    "update_epochs = 1,\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "pin_memory_device= (\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "    \n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 3#4\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.01\n",
    "                 )\n",
    ",model_config = dict(\n",
    "                    n_blocks      =   3,\n",
    "                    embed_dim     =   64,#128 ,\n",
    "                    context_len   =   256,#256  ,\n",
    "                    n_heads       =   1,\n",
    "                    dropout_p     =   0.1,\n",
    "                    wt_decay      =   0.0001,\n",
    "                    warmup_steps  =   100   ,\n",
    "                    tau           =   0.15, #0.95\n",
    "                    chunk_size    =   2000,#64,#8,#32,#64\n",
    "                    chunk_overlap =   1,\n",
    "                    rb_len        =   20*3000, #20\n",
    "\n",
    "\n",
    "                    policy_frequency  =2,\n",
    "                \n",
    "                    warmup_epoch  =   5,  \n",
    "                    total_epoch   =   20,\n",
    "                    initial_lr    =   5e-4,\n",
    "                    final_lr      =   1e-6,\n",
    "\n",
    "                    beta           = 0.5,#args.model_config['beta']         #0.2 #Q_mse\n",
    "                    alpha          = 1,#args.model_config['alpha']          #0.1  #actionloss\n",
    "                    entropy_coeff  = 0.2,#args.model_config['entropy_coeff']         #0.1#0.5   #entropy loss in action\n",
    "                    val_loss_coeff = 0.5#args.model_config['val_loss_coeff']        #0.5      #Q loss\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "                    )\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "9346805a-1ea2-429c-9a6c-234a60e94d88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_break_1\n",
      "done: {1: False, 2: False, 3: True} ,total_reward: {1: -94.81999999999995, 2: -100.21, 3: -301.13} ,iteration: 500 ,episode: 4990\n",
      "[[1 tensor(4.)]\n",
      " [1 tensor(1.)]\n",
      " [2 tensor(5.)]\n",
      " [2 tensor(6.)]\n",
      " [1 tensor(16.)]\n",
      " [1 tensor(6.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(20.)]]\n",
      "1 1\n",
      "3000 2999\n",
      "epoch 4990 qf1_loss tensor(191.4311, grad_fn=<MseLossBackward0>)\n",
      "torch.float32 torch.float32 torch.float32\n",
      "actor_loss tensor(-81.0288, grad_fn=<NegBackward0>)\n",
      "done: {1: True, 2: True, 3: True} ,total_reward: {1: 106.97000000000004, 2: -200.01999999999998, 3: -103.22999999999999} ,iteration: 500 ,episode: 4991\n",
      "[[1 tensor(20.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(9.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(15.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(1.)]]\n",
      "1 1\n",
      "2138 2138\n",
      "epoch 4991 qf1_loss tensor(22.9652, grad_fn=<MseLossBackward0>)\n",
      "local_break_1\n",
      "done: {1: False, 2: False, 3: True} ,total_reward: {1: -99.04999999999994, 2: -98.22, 3: -200.01999999999998} ,iteration: 500 ,episode: 4992\n",
      "[[2 tensor(2.)]\n",
      " [2 tensor(2.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(17.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(42.)]\n",
      " [2 tensor(1.)]\n",
      " [2 tensor(4.)]\n",
      " [2 tensor(6.)]\n",
      " [2 tensor(9.)]]\n",
      "1 2\n",
      "3000 2999\n",
      "epoch 4992 qf1_loss tensor(175.2324, grad_fn=<MseLossBackward0>)\n",
      "torch.float32 torch.float32 torch.float32\n",
      "actor_loss tensor(-79.2373, grad_fn=<NegBackward0>)\n",
      "done: {1: True, 2: True, 3: False} ,total_reward: {1: -200.34, 2: -100.35999999999999, 3: 7.630000000000008} ,iteration: 500 ,episode: 4993\n",
      "[[0 0.0]\n",
      " [3 4.0]\n",
      " [3 12.0]\n",
      " [3 3.0]\n",
      " [3 3.0]\n",
      " [3 1.0]\n",
      " [3 1.0]\n",
      " [3 1.0]\n",
      " [3 1.0]\n",
      " [0 0.0]]\n",
      "1 3\n",
      "2275 2275\n",
      "epoch 4993 qf1_loss tensor(31.5149, grad_fn=<MseLossBackward0>)\n",
      "done: {1: True, 2: True, 3: True} ,total_reward: {1: 107.79000000000002, 2: -100.14999999999999, 3: -302.0} ,iteration: 500 ,episode: 4994\n",
      "[[1 tensor(2.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(18.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(14.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(1.)]]\n",
      "1 1\n",
      "1722 1722\n",
      "epoch 4994 qf1_loss tensor(196.9799, grad_fn=<MseLossBackward0>)\n",
      "torch.float32 torch.float32 torch.float32\n",
      "actor_loss tensor(-80.0540, grad_fn=<NegBackward0>)\n",
      "local_break_1\n",
      "done: {1: True, 2: False, 3: False} ,total_reward: {1: -200.16, 2: -95.47999999999999, 3: -95.58999999999999} ,iteration: 500 ,episode: 4995\n",
      "[[3 tensor(2.)]\n",
      " [3 tensor(1.)]\n",
      " [3 tensor(1.)]\n",
      " [3 tensor(2.)]\n",
      " [3 tensor(15.)]\n",
      " [0 tensor(0.)]\n",
      " [2 tensor(5.)]\n",
      " [2 tensor(1.)]\n",
      " [2 tensor(3.)]\n",
      " [2 tensor(1.)]]\n",
      "1 3\n",
      "3000 2999\n",
      "epoch 4995 qf1_loss tensor(24.0129, grad_fn=<MseLossBackward0>)\n",
      "done: {1: True, 2: True, 3: True} ,total_reward: {1: -200.45, 2: -98.0, 3: 106.98} ,iteration: 500 ,episode: 4996\n",
      "[[0 0]\n",
      " [3 tensor(1.)]\n",
      " [3 tensor(1.)]\n",
      " [3 tensor(6.)]\n",
      " [0 tensor(0.)]\n",
      " [0 tensor(0.)]\n",
      " [3 tensor(2.)]\n",
      " [3 tensor(1.)]\n",
      " [3 tensor(1.)]\n",
      " [3 1.0]]\n",
      "1 3\n",
      "357 357\n",
      "epoch 4996 qf1_loss tensor(190.3379, grad_fn=<MseLossBackward0>)\n",
      "torch.float32 torch.float32 torch.float32\n",
      "actor_loss tensor(-79.6327, grad_fn=<NegBackward0>)\n",
      "local_break_1\n",
      "done: {1: False, 2: True, 3: False} ,total_reward: {1: -97.06999999999995, 2: -200.01, 3: -102.33} ,iteration: 500 ,episode: 4997\n",
      "[[1 tensor(1.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(8.)]\n",
      " [3 tensor(2.)]\n",
      " [1 tensor(1.)]\n",
      " [3 tensor(1.)]\n",
      " [3 tensor(1.)]\n",
      " [1 tensor(24.)]\n",
      " [3 tensor(1.)]]\n",
      "1 1\n",
      "3000 2999\n",
      "epoch 4997 qf1_loss tensor(25.5382, grad_fn=<MseLossBackward0>)\n",
      "local_break_1\n",
      "done: {1: True, 2: False, 3: False} ,total_reward: {1: -201.51, 2: -97.28, 3: -97.55999999999999} ,iteration: 500 ,episode: 4998\n",
      "[[2 tensor(5.)]\n",
      " [2 tensor(1.)]\n",
      " [3 tensor(1.)]\n",
      " [3 tensor(1.)]\n",
      " [3 tensor(2.)]\n",
      " [3 tensor(11.)]\n",
      " [3 tensor(3.)]\n",
      " [2 tensor(2.)]\n",
      " [2 tensor(1.)]\n",
      " [0 tensor(0.)]]\n",
      "1 3\n",
      "3000 2999\n",
      "epoch 4998 qf1_loss tensor(191.2246, grad_fn=<MseLossBackward0>)\n",
      "torch.float32 torch.float32 torch.float32\n",
      "actor_loss tensor(-81.2442, grad_fn=<NegBackward0>)\n",
      "local_break_1\n",
      "done: {1: False, 2: True, 3: False} ,total_reward: {1: -99.88999999999997, 2: -200.03, 3: -94.36999999999999} ,iteration: 500 ,episode: 4999\n",
      "[[3 tensor(9.)]\n",
      " [3 tensor(3.)]\n",
      " [3 tensor(2.)]\n",
      " [3 tensor(8.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(1.)]\n",
      " [1 tensor(28.)]\n",
      " [3 tensor(1.)]\n",
      " [3 tensor(6.)]\n",
      " [3 tensor(11.)]]\n",
      "1 2\n",
      "3000 2999\n",
      "epoch 4999 qf1_loss tensor(29.8174, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "smp = sample_and_train(args=exp_12)\n",
    "smp.run_learning_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "5f8de85c-d055-430e-8939-8fadef3b11df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000,  0.0000,  0.0000, -0.8482,  1.0000,  0.0000,  0.0000, -2.8844,\n",
      "          1.0000,  0.0000,  0.0000, -1.5269,  1.0000,  0.0000,  0.0000, -1.5269,\n",
      "          0.0000,  0.0000,  1.0000, -0.1694,  1.0000,  0.0000,  0.0000, -0.1694,\n",
      "          1.0000,  0.0000,  0.0000, -2.2057,  0.0000,  0.0000,  0.0000, -3.5632,\n",
      "          1.0000,  0.0000,  0.0000, -1.5269,  1.0000,  0.0000,  0.0000, -2.8844,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  1.0000,\n",
      "          1.0000,  0.0000,  0.0000,  0.0000, -2.2057]])\n",
      "{'observation': tensor([[ 1.0000,  0.0000,  0.0000, -0.8482],\n",
      "        [ 1.0000,  0.0000,  0.0000, -2.8844],\n",
      "        [ 1.0000,  0.0000,  0.0000, -1.5269],\n",
      "        [ 1.0000,  0.0000,  0.0000, -1.5269],\n",
      "        [ 0.0000,  0.0000,  1.0000, -0.1694],\n",
      "        [ 1.0000,  0.0000,  0.0000, -0.1694],\n",
      "        [ 1.0000,  0.0000,  0.0000, -2.2057],\n",
      "        [ 0.0000,  0.0000,  0.0000, -3.5632],\n",
      "        [ 1.0000,  0.0000,  0.0000, -1.5269],\n",
      "        [ 1.0000,  0.0000,  0.0000, -2.8844]]), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       1, 0, 0, 0, 0, 0, 1, 0, 1, 1], dtype=int8)}\n",
      "{1: True, 2: True, 3: True}\n"
     ]
    }
   ],
   "source": [
    "smp.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "64059f6c-813f-40ba-80fc-47e93abe9207",
   "metadata": {},
   "outputs": [],
   "source": [
    "smp.hero_agents_list[1].train(1, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf4122-9b3c-48b7-9252-55859091ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.repeat(repeats=(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7a8ae1-090d-42d1-a2bf-fa5d0eb95170",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    data= smp.hero_agents_list[smp.the_hero_agent].model_in.repeat(repeats=(2,1))\n",
    "    a1 = smp.hero_agents_list[smp.the_hero_agent].actor1(data)\n",
    "    print(data.shape,a1.shape)\n",
    "    \n",
    "    \n",
    "    a2 = smp.hero_agents_list[smp.the_hero_agent].actor2(data,a1)\n",
    "    print(a1,a2)\n",
    "    v = smp.hero_agents_list[smp.the_hero_agent].qf1(data,a1[:,None],a2)\n",
    "    print(v)\n",
    "    \n",
    "    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be6c01-b9ba-446e-9fc2-c1dde08a7630",
   "metadata": {},
   "source": [
    "# model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78002dd-96de-469c-b5f1-611253cfbf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import stable_baselines3 as sb3\n",
    "\n",
    "    if sb3.__version__ < \"2.0\":\n",
    "        raise ValueError(\n",
    "            \"\"\"Ongoing migration: run the following command to install the new dependencies:\n",
    "poetry run pip install \"stable_baselines3==2.0.0a1\"\n",
    "\"\"\"\n",
    "        )\n",
    "    args = tyro.cli(Args)\n",
    "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    if args.track:\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    # TRY NOT TO MODIFY: seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])\n",
    "    assert isinstance(envs.single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n",
    "\n",
    "    actor = Actor(envs).to(device)\n",
    "    qf1 = QNetwork(envs).to(device)\n",
    "    qf1_target = QNetwork(envs).to(device)\n",
    "    target_actor = Actor(envs).to(device)\n",
    "    target_actor.load_state_dict(actor.state_dict())\n",
    "    qf1_target.load_state_dict(qf1.state_dict())\n",
    "    q_optimizer = optim.Adam(list(qf1.parameters()), lr=args.learning_rate)\n",
    "    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.learning_rate)\n",
    "\n",
    "    envs.single_observation_space.dtype = np.float32\n",
    "    rb = ReplayBuffer(\n",
    "        args.buffer_size,\n",
    "        envs.single_observation_space,\n",
    "        envs.single_action_space,\n",
    "        device,\n",
    "        handle_timeout_termination=False,\n",
    "    )\n",
    "    start_time = time.time()\n",
    "\n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    obs, _ = envs.reset(seed=args.seed)\n",
    "    for global_step in range(args.total_timesteps):\n",
    "        # ALGO LOGIC: put action logic here\n",
    "        if global_step < args.learning_starts:\n",
    "            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                actions = actor(torch.Tensor(obs).to(device))\n",
    "                actions += torch.normal(0, actor.action_scale * args.exploration_noise)\n",
    "                actions = actions.cpu().numpy().clip(envs.single_action_space.low, envs.single_action_space.high)\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        if \"final_info\" in infos:\n",
    "            for info in infos[\"final_info\"]:\n",
    "                print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "                break\n",
    "\n",
    "        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "        real_next_obs = next_obs.copy()\n",
    "        for idx, trunc in enumerate(truncations):\n",
    "            if trunc:\n",
    "                real_next_obs[idx] = infos[\"final_observation\"][idx]\n",
    "        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "\n",
    "        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "        obs = next_obs\n",
    "\n",
    "        # ALGO LOGIC: training.\n",
    "        if global_step > args.learning_starts:\n",
    "            data = rb.sample(args.batch_size)\n",
    "            with torch.no_grad():\n",
    "                next_state_actions = target_actor(data.next_observations)\n",
    "                qf1_next_target = qf1_target(data.next_observations, next_state_actions)\n",
    "                next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)\n",
    "\n",
    "            qf1_a_values = qf1(data.observations, data.actions).view(-1)\n",
    "            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "\n",
    "            # optimize the model\n",
    "            q_optimizer.zero_grad()\n",
    "            qf1_loss.backward()\n",
    "            q_optimizer.step()\n",
    "\n",
    "            if global_step % args.policy_frequency == 0:\n",
    "                actor_loss = -qf1(data.observations, actor(data.observations)).mean()\n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "\n",
    "                # update the target network\n",
    "                for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
    "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "            if global_step % 100 == 0:\n",
    "                writer.add_scalar(\"losses/qf1_values\", qf1_a_values.mean().item(), global_step)\n",
    "                writer.add_scalar(\"losses/qf1_loss\", qf1_loss.item(), global_step)\n",
    "                writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), global_step)\n",
    "                print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "                writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "    if args.save_model:\n",
    "        model_path = f\"runs/{run_name}/{args.exp_name}.cleanrl_model\"\n",
    "        torch.save((actor.state_dict(), qf1.state_dict()), model_path)\n",
    "        print(f\"model saved to {model_path}\")\n",
    "        from cleanrl_utils.evals.ddpg_eval import evaluate\n",
    "\n",
    "        episodic_returns = evaluate(\n",
    "            model_path,\n",
    "            make_env,\n",
    "            args.env_id,\n",
    "            eval_episodes=10,\n",
    "            run_name=f\"{run_name}-eval\",\n",
    "            Model=(Actor, QNetwork),\n",
    "            device=device,\n",
    "            exploration_noise=args.exploration_noise,\n",
    "        )\n",
    "        for idx, episodic_return in enumerate(episodic_returns):\n",
    "            writer.add_scalar(\"eval/episodic_return\", episodic_return, idx)\n",
    "\n",
    "        if args.upload_model:\n",
    "            from cleanrl_utils.huggingface import push_to_hub\n",
    "\n",
    "            repo_name = f\"{args.env_id}-{args.exp_name}-seed{args.seed}\"\n",
    "            repo_id = f\"{args.hf_entity}/{repo_name}\" if args.hf_entity else repo_name\n",
    "            push_to_hub(args, episodic_returns, repo_id, \"DDPG\", f\"runs/{run_name}\", f\"videos/{run_name}-eval\")\n",
    "\n",
    "    envs.close()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612ea30-c070-46e7-99ae-9ec016b8b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    actor1 = Actor(obs).to(device)\n",
    "    actor2 = ActorDiscrete(obs+1,action).to(device)\n",
    "\n",
    "\n",
    "    target_actor1 = Actor(obs).to(device)\n",
    "    target_actor1.load_state_dict(actor1.state_dict())\n",
    "\n",
    "    target_actor2 = ActorDiscrete(obs+1,action).to(device)\n",
    "    target_actor2.load_state_dict(actor2.state_dict())\n",
    "\n",
    "    qf1 = QNetwork(obs,action+1).to(device)\n",
    "    qf1_target = QNetwork(obs,action+1).to(device)\n",
    "\n",
    "    qf1_target.load_state_dict(qf1.state_dict())\n",
    "    q_optimizer = optim.Adam(list(qf1.parameters()), lr=args.learning_rate)\n",
    "\n",
    "    actor1_optimizer = optim.Adam(list(actor1.parameters()), lr=args.learning_rate)\n",
    "    actor2_optimizer = optim.Adam(list(actor2.parameters()), lr=args.learning_rate)\n",
    "\n",
    "    envs.single_observation_space.dtype = np.float32\n",
    "    #set observation space type\n",
    "    #setup replay buffer\n",
    "\n",
    "    #rb = ReplayBuffer(\n",
    "    #    args.buffer_size,\n",
    "    #    envs.single_observation_space,\n",
    "    #    envs.single_action_space,\n",
    "    #    device,\n",
    "    #    handle_timeout_termination=False,\n",
    "    #)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    #obs, _ = envs.reset(seed=args.seed)\n",
    "\n",
    "    #for global steps \n",
    "\n",
    "\n",
    "        #EXPLORATION and exp collection\n",
    "        #till learning starts random action\n",
    "\n",
    "        #else:\n",
    "        #    with torch.no_grad():\n",
    "        #        actions = actor(torch.Tensor(obs).to(device))\n",
    "        #        actions += torch.normal(0, actor.action_scale * args.exploration_noise)\n",
    "        #        actions = actions.cpu().numpy().clip(envs.single_action_space.low, envs.single_action_space.high) # action cliping\n",
    "        #\n",
    "        ## TRY NOT TO MODIFY: execute the game and log data.\n",
    "        #next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "\n",
    "\n",
    "        \n",
    "        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "        #real_next_obs = next_obs.copy()\n",
    "        #for idx, trunc in enumerate(truncations):\n",
    "        #    if trunc:\n",
    "        #        real_next_obs[idx] = infos[\"final_observation\"][idx]\n",
    "        #rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "\n",
    "        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "        #obs = next_obs\n",
    "\n",
    "\n",
    "\n",
    "        # ALGO LOGIC: training.\n",
    "        if global_step > args.learning_starts:\n",
    "            # get data batch\n",
    "            #data = rb.sample(args.batch_size)\n",
    "\n",
    "            # get \n",
    "            with torch.no_grad():\n",
    "                next_state_actions1 = target_actor1(data.next_observations)\n",
    "                next_state_actions2 = target_actor2(data.next_observations,next_state_actions1)\n",
    "                qf1_next_target = qf1_target(data.next_observations, next_state_actions1, next_state_actions2)\n",
    "                next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)\n",
    "\n",
    "            qf1_a_values = qf1(data.observations, data.actions1, data.actions2 ).view(-1)\n",
    "            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "\n",
    "            # optimize the model\n",
    "            q_optimizer.zero_grad()\n",
    "            qf1_loss.backward()\n",
    "            q_optimizer.step()\n",
    "\n",
    "            if global_step % args.policy_frequency == 0:\n",
    "                acts_1 = actor1(data.observations)\n",
    "                acts_2 = actor2(data.observations,acts_1)\n",
    "                actor_loss = -qf1(data.observations, acts_1, acts_2).mean()\n",
    "                actor1_optimizer.zero_grad()\n",
    "                \n",
    "                actor_loss.backward(retain_graph=True)\n",
    "                actor1_optimizer.step()\n",
    "\n",
    "                actor2_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                \n",
    "                actor2_optimizer.step()\n",
    "\n",
    "                \n",
    "                # update the target network\n",
    "                for param, target_param in zip(actor1.parameters(), target_actor1.parameters()):\n",
    "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "                for param, target_param in zip(actor2.parameters(), target_actor2.parameters()):\n",
    "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "                \n",
    "                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
    "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "\n",
    "\n",
    "    def train_write(self, iteration, print_=False):\n",
    "\n",
    "        total_loss_list = []\n",
    "        Q_TD_list = []\n",
    "        Q_MSE_list = []\n",
    "        Q_loss_list = []\n",
    "        policy_loss_list = []\n",
    "\n",
    "        if self.args.TB_log:\n",
    "            self.writer.add_scalar(\"charts/learning_rate\", self.optimizer_1.param_groups[0][\"lr\"], iteration)        \n",
    "        \n",
    "        for epoch in range(self.args.update_epochs):\n",
    "            for i,batch in enumerate(self.traj_data_loader):\n",
    "                total_loss = 0\n",
    "                Q_TD = 0\n",
    "                Q_MSE = 0\n",
    "                Q_loss = 0\n",
    "                policy_loss = 0\n",
    "                print(i)\n",
    "\n",
    "\n",
    "                batch_len = batch[0].shape[1]#2840\n",
    "                \n",
    "                if batch_len > self.chunk_size:\n",
    "                    a_ = [(i,i+self.chunk_size) for i in range(0, batch_len     -self.chunk_size,self.chunk_size-self.chunk_overlap)  ]\n",
    "                    a_  =a_+[ (a_[-1][1] -self.chunk_overlap ,batch_len) ]\n",
    "                else:\n",
    "                    a_ = [(0,batch_len)]\n",
    "    \n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                divi = len(a_)\n",
    "\n",
    "                \n",
    "                print('divi',divi,'chunk_size',self.chunk_size,'batch_shape',batch[0].shape)\n",
    "                \n",
    "                for (chunk_id, i) in enumerate(a_):#range(0,batch[0].shape[1] - self.chunk_size + 1,self.chunk_size - self.chunk_overlap)):\n",
    "\n",
    "                    # print(i,(i + self.chunk_size))\n",
    "                    total_loss_chunk, Q_TD_chunk, Q_MSE_chunk, Q_loss_chunk, policy_loss_chunk = self.train_write_smaller_chunk((tens[:, i[0]:i[1]] for tens in batch),\n",
    "                                                                                                                                    iteration, epoch, chunk_id, print_=print_,divi=divi)\n",
    "\n",
    "                    \n",
    "                    total_loss= total_loss + total_loss_chunk\n",
    "                    Q_TD = Q_TD+ Q_TD_chunk\n",
    "                    Q_MSE = Q_MSE+ Q_MSE_chunk\n",
    "                    Q_loss = Q_loss+ Q_loss_chunk\n",
    "                    policy_loss = policy_loss+ policy_loss_chunk\n",
    "\n",
    "                total_loss_list.append(total_loss)\n",
    "                Q_TD_list.append(Q_TD)\n",
    "                Q_MSE_list.append(Q_MSE)\n",
    "                Q_loss_list.append(Q_loss)\n",
    "                policy_loss_list.append(policy_loss)            \n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "        \n",
    "        if iteration < self.warmup_epoch:\n",
    "            # Linear warmup: Gradually increase learning rate during warmup\n",
    "            lr = self.initial_lr * iteration / self.warmup_epoch\n",
    "            for param_group in self.optimizer_1.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        else:\n",
    "            self.scheduler.step()\n",
    "\n",
    "        \n",
    "        print(\"total_loss\", np.mean(total_loss_list),\n",
    "              \"Q_TD\", np.mean(Q_TD_list),\n",
    "                \"Q_MSE\", np.mean(Q_MSE_list),\n",
    "                \"Q_loss\", np.mean(Q_loss_list),\n",
    "                \"policy_loss\", np.mean(policy_loss_list))\n",
    "\n",
    "        if self.args.TB_log:\n",
    "        \n",
    "            self.writer.add_scalar(\"total_loss\", np.mean(total_loss_list), iteration)\n",
    "            self.writer.add_scalar(\"Q_TD\", np.mean(Q_TD_list), iteration)\n",
    "            self.writer.add_scalar(\"Q_MSE\", np.mean(Q_MSE_list), iteration)\n",
    "            self.writer.add_scalar(\"Q_loss\", np.mean(Q_loss_list), iteration)\n",
    "            self.writer.add_scalar(\"policy_loss\", np.mean(policy_loss_list), iteration)    \n",
    "                    \n",
    "                \n",
    "                \n",
    "        \n",
    "        self.copy_wt()\n",
    "\n",
    "\n",
    "    def train_write_smaller_chunk(\n",
    "        self,\n",
    "        data,\n",
    "        iteration,\n",
    "        epoch,\n",
    "        chunk_id,\n",
    "        print_=False,divi = 1\n",
    "        ):\n",
    "\n",
    "        (\n",
    "            timesteps,\n",
    "            states,\n",
    "            actions_1,actions_2,log_probs_actions_2,\n",
    "            returntogo,\n",
    "            returns_to_go_cal,\n",
    "            returntogo_pred,\n",
    "            reward,\n",
    "            traj_mask,\n",
    "            action_masks,\n",
    "            current_agent_acting,\n",
    "            current_agent_simple,\n",
    "            current_agent,\n",
    "            current_phase,\n",
    "            current_troops_count,\n",
    "            ) = data\n",
    "\n",
    "\n",
    "        if len(timesteps[0].shape) ==0:\n",
    "            print( timesteps.shape )\n",
    "\n",
    "        \n",
    "        timesteps = timesteps[0].to(self.device)  # B x T\n",
    "        states = states[0].to(self.device)  # B x T x state_dim\n",
    "        actions_1 = actions_1[0].to(self.device)  # B x T x act_dim\n",
    "        actions_2 = actions_2[0].to(self.device)  # B x T x act_dim\n",
    "        log_probs_actions_2 = log_probs_actions_2[0].to(self.device)\n",
    "        reward = reward[0].to(self.device)\n",
    "        returns_to_go_cal =             returns_to_go_cal[0].to(self.device).unsqueeze(dim=-1)  # B x T x 1\n",
    "        returntogo = returntogo[0].to(self.device)\n",
    "        returntogo_pred = returntogo_pred[0].to(self.device)\n",
    "        traj_mask = traj_mask[0].to(self.device)  # B x T\n",
    "        action_masks = action_masks[0].to(self.device)\n",
    "        current_agent_acting = current_agent_acting[0].to(self.device)\n",
    "        current_agent_simple = current_agent_simple[0].to(self.device)\n",
    "        current_agent = current_agent[0].to(self.device)\n",
    "        current_phase = current_phase[0].to(self.device)\n",
    "        current_troops_count = current_troops_count[0].to(self.device)\n",
    "\n",
    "        info = dict({})\n",
    "\n",
    "\n",
    "        \n",
    "        hero_steps = current_agent_simple == self.hero\n",
    "\n",
    "        states = torch.cat((states, action_masks * hero_steps,\n",
    "                              current_phase, current_agent,\n",
    "                              current_troops_count[:, :, None]), axis=2)  # ,torch.ones(len(action_masks))[:,None]*self.hero\n",
    "\n",
    "\n",
    "\n",
    "        #print(actions_1.requires_grad,actions_2.requires_grad)\n",
    "\n",
    "        (action_pred_1,action_pred_2) =  (actions_1* hero_steps[:,0],\n",
    "                                                        actions_2* hero_steps[:,0]\n",
    "                                                         )\n",
    "\n",
    "        # state_preds, action_preds, return_preds = self.model.forward(\n",
    "        #                                                timesteps=timesteps,\n",
    "        #                                                states=states,\n",
    "        #                                                actions=actions,\n",
    "        #                                                returns_to_go=returns_to_go\n",
    "        #                                                #,info = info\n",
    "        #                                            )\n",
    "\n",
    "        #so this return to go is for value loss estimation\n",
    "\n",
    "        _,_, returntogo_pred_v,_ = self.model.forward(\n",
    "                                                        timesteps=timesteps,\n",
    "                                                        states=states,\n",
    "                                                        actions_1=action_pred_1,actions_2=action_pred_2,\n",
    "                                                        returns_to_go=returntogo,\n",
    "                                                            print_=print_\n",
    "                                                        #,info = info\n",
    "                                                    )\n",
    "        \n",
    "        # this is also for value function loss estimation\n",
    "\n",
    "        (#state_preds_target, \n",
    "         _,_,return_preds_target,_) =  self.target_model.forward(timesteps=timesteps,\n",
    "                                                                                states=states, actions_1=action_pred_1,actions_2=action_pred_2,\n",
    "                                                                                returns_to_go=returntogo, print_=print_)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        (#state_preds_target, \n",
    "         return_preds_target) = (return_preds_target.detach())\n",
    "\n",
    "\n",
    "        if  len(reward[:, -1].squeeze().view(-1).shape) == 0:\n",
    "            \n",
    "            print('return_preds_last',return_preds_target.shape)\n",
    "            print('reward_last',reward.shape)\n",
    "            print('returns_to_go_cal_last',returns_to_go_cal.shape)\n",
    "\n",
    "            print('return_preds_last',return_preds_target)\n",
    "            print('reward_last',reward)\n",
    "            print('returns_to_go_cal_last',returns_to_go_cal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        total_loss, (Q_TD, Q_MSE, Q_loss, policy_loss) = self.calculate_loss(\n",
    "                                                                                timesteps,\n",
    "                                                                                return_preds_target,#target model predictions \n",
    "                                                                                returns_to_go_cal, # Q calculated\n",
    "                                                                                hero_steps    ,#current_agent_acting,\n",
    "                                                                                states,\n",
    "                                                                                actions_1,actions_2,log_probs_actions_2,\n",
    "                                                                                returntogo_pred,\n",
    "                                                                                returntogo_pred_v,\n",
    "                                                                                reward,#actual rewards \n",
    "                                                                                returntogo,  #actual R2G given to the model\n",
    "                                                                                action_masks,\n",
    "                                                                                print_,\n",
    "                                                                                chunk_id,divi\n",
    "                                                                                )\n",
    "        #if chunk_id == 0:\n",
    "        #    print (chunk_id, total_loss)\n",
    "\n",
    "        # action_loss = #nn.CrossEntropyLoss().forward(action_preds_2,action_target)#F.mse_loss(action_preds_2, action_target.float(), reduction='mean')\n",
    "\n",
    "        self.optimizer_1.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.25)\n",
    "        self.optimizer_1.step()\n",
    "        #self.scheduler.step()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # policy_loss.backward(retain_graph=True)\n",
    "\n",
    "        return total_loss.detach().item() ,Q_TD.detach().item(), Q_MSE.detach().item(), Q_loss.detach().item(), policy_loss.detach().item()\n",
    "\n",
    "\n",
    "    def init_path(self):\n",
    "        self.paths = []\n",
    "\n",
    "    def init_CL_sample_store(self):\n",
    "\n",
    "\n",
    "        if self.context_len > 1: # transformer model\n",
    "            self.DT_input = {  # self.actor_config_dict['ob_space']\n",
    "                'timestep': torch.zeros((1,\n",
    "                                        self.context_len),requires_grad =False).to(self.device,\n",
    "                        dtype=torch.int),\n",
    "                'state': torch.zeros((1, self.context_len,\n",
    "                                     self.state_dim),requires_grad =False).to(self.device,dtype=torch.float32),\n",
    "                'action_1': torch.zeros((1, self.context_len\n",
    "                                      )).to(self.device,dtype=torch.float32),\n",
    "                'action_2': torch.zeros((1, self.context_len\n",
    "                                      )).to(self.device,dtype=torch.float32),\n",
    "                \n",
    "                'return_to_go': torch.ones((1,\n",
    "                        self.context_len),requires_grad =False).to(self.device,dtype=torch.float32) * 110,\n",
    "                }\n",
    "        else: # single traje models\n",
    "            self.DT_input = {  # self.actor_config_dict['ob_space']\n",
    "                'timestep': torch.zeros((1,\n",
    "                                        ),requires_grad =False).to(self.device,\n",
    "                        dtype=torch.int),\n",
    "                'state': torch.zeros((1,\n",
    "                                     self.state_dim),requires_grad =False).to(self.device,dtype=torch.float32),\n",
    "                'action_1': torch.zeros((1,\n",
    "                                      )).to(self.device,dtype=torch.float32),\n",
    "                'action_2': torch.zeros((1,\n",
    "                                      )).to(self.device,dtype=torch.float32),\n",
    "                \n",
    "                'return_to_go': torch.ones((1,\n",
    "                                    ),requires_grad =False).to(self.device,dtype=torch.float32) * 110,\n",
    "                }            \n",
    "    \n",
    "        self.returntogo = torch.zeros((self.num_steps,\n",
    "                1),requires_grad =False).to(self.device,dtype=torch.float32)  # self.total_agents\n",
    "        self.returntogo_pred = torch.zeros((self.num_steps,\n",
    "                1)).to(self.device,dtype=torch.float32)  # self.total_agents\n",
    "\n",
    "    #here\n",
    "    def current_model_in(\n",
    "        self,\n",
    "        observation,\n",
    "        curr_agent,\n",
    "        phase_mapping,\n",
    "        curr_agent_mapping,\n",
    "        env_board_agents=[],\n",
    "        ):\n",
    "\n",
    "\n",
    "\n",
    "        #single obs\n",
    "        self.model_in =  torch.hstack((observation['observation'\n",
    "                         ].reshape(-1).to(self.device),\n",
    "                         torch.tensor(observation['action_mask'\n",
    "                         ].reshape(-1)).to(self.device) * (curr_agent\n",
    "                         == self.hero), phase_mapping.to(self.device),\n",
    "                         curr_agent_mapping.to(self.device),\n",
    "                         ( (torch.tensor([env_board_agents[self.hero].bucket]).to(self.device) - 5.2496)/1.4733\n",
    "                                      )))[None,\n",
    "                         :].float().requires_grad_(False).to(self.device)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def update_CL_sample_store(\n",
    "        self,\n",
    "        curr_agent_,\n",
    "        inp={'step': None, 'act_2_1': [],'act_2_2': [], 'curr_reward_list': []},\n",
    "        before_action=True,\n",
    "        ):\n",
    "\n",
    "        if self.context_len > 1:\n",
    "            \n",
    "            if before_action == 1 :\n",
    "    \n",
    "                if inp['step'] == 0:\n",
    "    \n",
    "                    # print(self.model_in.shape)\n",
    "                    # print(self.model_in.repeat(self.context_len).shape)\n",
    "    \n",
    "                    self.DT_input['state'] = self.model_in.repeat(self.context_len,\n",
    "                            1).to(self.device)[None, :]\n",
    "                else:\n",
    "    \n",
    "                # if step<self.context_len:\n",
    "    \n",
    "                #    trace[step] = model_in\n",
    "    \n",
    "                    self.DT_input['state'][0, 0:-1] = self.DT_input['state'\n",
    "                            ][0, 1:].clone()\n",
    "                    self.DT_input['state'][0, -1] = self.model_in\n",
    "                    self.DT_input['timestep'][0, 0:-1] = self.DT_input['timestep'][0, 1:].clone()\n",
    "                    self.DT_input['timestep'][0, -1] = inp['step']\n",
    "                    self.DT_input['action_1'][0, 0:-1] = self.DT_input['action_1'][0, 1:].clone()\n",
    "                    self.DT_input['action_2'][0, 0:-1] = self.DT_input['action_2'][0, 1:].clone()\n",
    "                    \n",
    "                    self.DT_input['return_to_go'][0, 0:-1] = self.DT_input['return_to_go'][0, 1:].clone()\n",
    "            elif before_action == 2 :\n",
    "                if self.hero == curr_agent_:\n",
    "                    self.DT_input['action_1'][0, -1] = inp['act_2_1']\n",
    "                else:\n",
    "                    self.DT_input['action_1'][0, -1] = 0\n",
    "    \n",
    "            elif before_action == 3  :\n",
    "                if self.hero == curr_agent_:\n",
    "                    self.DT_input['action_2'][0, -1] = inp['act_2_2']\n",
    "                else:\n",
    "                    self.DT_input['action_2'][0, -1] = 0\n",
    "            else:\n",
    "                    \n",
    "                self.DT_input['return_to_go'][0, -1] = self.DT_input['return_to_go'][0, -1] -    inp['curr_reward_list']  # [self.hero]\n",
    "                self.returntogo[inp['step']] = self.DT_input['return_to_go'\n",
    "                        ][0, -1]\n",
    "\n",
    "        else:\n",
    "            if before_action == 1 :\n",
    "                self.DT_input['state'][0] = self.model_in\n",
    "                self.DT_input['timestep'][0] = inp['step']\n",
    "            elif before_action == 2 :\n",
    "                if self.hero == curr_agent_:\n",
    "                    self.DT_input['action_1'][0] = inp['act_2_1']\n",
    "                else:\n",
    "                    self.DT_input['action_1'][0] = 0                \n",
    "            \n",
    "            elif before_action == 3  :\n",
    "                if self.hero == curr_agent_:\n",
    "                    self.DT_input['action_2'][0] = inp['act_2_2']\n",
    "                else:\n",
    "                    self.DT_input['action_2'][0] = 0\n",
    "            else:\n",
    "                    \n",
    "                self.DT_input['return_to_go'][0] = self.DT_input['return_to_go'][0] -    inp['curr_reward_list']  # [self.hero]\n",
    "                self.returntogo[inp['step']] = self.DT_input['return_to_go'][0]               \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def update_train_data(\n",
    "        self,\n",
    "        step_count,\n",
    "        obs,\n",
    "        ob_space_shape,\n",
    "        rewards_2,\n",
    "        dones_2,\n",
    "        actions_1,\n",
    "        actions_2,\n",
    "        log_probs_actions_2,\n",
    "        action_masks,\n",
    "        current_agent,\n",
    "        current_agent_acting,\n",
    "        current_phase,\n",
    "        current_troops_count,\n",
    "        map_agent_phase_vector,\n",
    "        ):\n",
    "\n",
    "        \n",
    "        \n",
    "        data_ = collections.defaultdict(torch.tensor)\n",
    "        data_['observations'] =             obs[:step_count].reshape(-1,\n",
    "                np.prod(ob_space_shape))\n",
    "\n",
    "                # data_['next_observations'] = obs[1:step_count+1].to(device =self.args.pin_memory_device).reshape(-1,np.prod(T.ob_space_shape)) #torch.tensor([1,2,3,4])\n",
    "                # this return to go is the actual input\n",
    "\n",
    "        data_['returntogo'] =           self.returntogo[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['returntogo_pred'] =      self.returntogo_pred[:step_count]  # torch.tensor([1,2,3,4])\n",
    "\n",
    "        data_['rewards'] =              rewards_2[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['terminals'] =            dones_2[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['actions_1'] =            actions_1[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['actions_2'] =            actions_2[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['log_probs_actions_2'] =  log_probs_actions_2[:step_count]\n",
    "        data_['action_masks'] =         action_masks[:step_count]\n",
    "        data_['current_agent_acting'] = current_agent_acting[:step_count]\n",
    "        data_['current_agent_simple'] = current_agent[:step_count]\n",
    "        data_['current_agent'] =        map_agent_phase_vector(current_agent[:step_count],\n",
    "                                               num_classes=self.total_agents + 1)[:, 1:]\n",
    "        data_['current_phase'] =        map_agent_phase_vector(current_phase[:step_count],\n",
    "                                               num_classes=self.total_phases)\n",
    "        data_['current_troops_count'] = current_troops_count[:step_count]\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        self.datase = TrajectoryDataset_per_episode([data_], #len(data[0]['observations'])<self.args.context_len\n",
    "                context_len=self.args.context_len,rtg_scale=self.args.rtg_scale,dev = self.args.pin_memory_device,\n",
    "                 gamma=self.args.gamma)\n",
    "\n",
    "        self.path_que(DataLoader(self.datase, batch_size=len(self.datase)))\n",
    "    def path_que(self, dtl):\n",
    "        \n",
    "        if (len(self.paths)==self.rb_len):\n",
    "            if (self.rb_len >1):\n",
    "                self.paths.pop(random.randrange(len(self.paths)-self.args.num_episodes +1 )) # dont pop the most recent experiences and ensure rb_len > num.episodes\n",
    "            else: \n",
    "                self.paths.pop()\n",
    "        self.paths.append(dtl)\n",
    "        \n",
    "    def create_training_dataset(self):\n",
    "        self.traj_dataset =  TrajectoryDataset_2_through_episodes(self.paths)  # a dataset of dataloaders\n",
    "\n",
    "        self.traj_data_loader = DataLoader(  # only spit 1 episode a time\n",
    "            self.traj_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=self.args.shuffle,\n",
    "            pin_memory=self.args.pin_memory,\n",
    "            drop_last=self.args.drop_last,\n",
    "            pin_memory_device=self.args.pin_memory_device,\n",
    "            )\n",
    "\n",
    "    \n",
    "    def action_predict(self, save_R=True, return_R=False,shift=1,action_masks = [],return_log_prob_a2 = False): \n",
    "        (#s, \n",
    "         a_1,a_2, R,lp) = self.model(timesteps=self.DT_input['timestep'],\n",
    "                               states=self.DT_input['state'],\n",
    "                               actions_1=self.DT_input['action_1'],\n",
    "                               actions_2=self.DT_input['action_2'],\n",
    "                               returns_to_go=( self.DT_input['return_to_go'][:, :, None] if self.context_len>1 else self.DT_input['return_to_go'][:, None]), \n",
    "                                return_log_prob_a2 = return_log_prob_a2)\n",
    "\n",
    "\n",
    "        took_action = False\n",
    "\n",
    "        \n",
    "\n",
    "        if self.context_len>1: # handling transformer model\n",
    "            # I need to handle for non transformer actions\n",
    "            if len(action_masks) > 0:\n",
    "                #handling when all the probability of masked actions are zero.... #have to force the model to pick 1st valid action.\n",
    "                masked_action = (a_1[0, -1, :]*action_masks)\n",
    "                #valid_ind = torch.nonzero(action_masks).squeeze()\n",
    "                if torch.any(masked_action !=0):\n",
    "                    took_action = True\n",
    "                    \n",
    "                    action_1 = (masked_action).argmax()[None]\n",
    "                else: #im hoping that slowly and steadily the model would learn to predict non zero probabilities for action mask\n",
    "                    \n",
    "                    action_1 = torch.tensor(np.random.choice(torch.where(action_masks)[0]))[None]  #(masked_action + 0.0000001*action_masks).argmax()[None]\n",
    "            else:\n",
    "                action_1 = a_1[0, -1, :].argmax()[None]\n",
    "                \n",
    "            action_2 = a_2[0,-1, 0][None]\n",
    "            if return_log_prob_a2:\n",
    "                log_prob = lp[0,-1, 0][None]\n",
    "            else:\n",
    "                log_prob = None\n",
    "            \n",
    "            if save_R:\n",
    "                self.returntogo_pred[self.DT_input['timestep'][0, -1]] =                 R[0, -1]  # R\n",
    "    \n",
    "            if return_R:\n",
    "                return (action_1, action_2, R[0, -1],took_action,log_prob)\n",
    "            else:\n",
    "                return action_1, action_2,took_action,log_prob\n",
    "\n",
    "\n",
    "        else: # not we are talking about the single obs models\n",
    "\n",
    "            # I need to handle for non transformer actions\n",
    "            if len(action_masks) > 0:\n",
    "                #handling when all the probability of masked actions are zero.... #have to force the model to pick 1st valid action.\n",
    "                masked_action = (a_1[0, :]*action_masks)\n",
    "                #valid_ind = torch.nonzero(action_masks).squeeze()\n",
    "                if torch.any(masked_action !=0):\n",
    "                    took_action = True\n",
    "                    \n",
    "                    action_1 = (masked_action).argmax()[None]\n",
    "                else: #im hoping that slowly and steadily the model would learn to predict non zero probabilities for action mask\n",
    "                    \n",
    "                    action_1 = torch.tensor(np.random.choice(torch.where(action_masks)[0]))[None]  #(masked_action + 0.0000001*action_masks).argmax()[None]\n",
    "            else:\n",
    "                action_1 = a_1[0, :].argmax()[None]\n",
    "                \n",
    "            action_2 = a_2[0, 0][None]\n",
    "            if return_log_prob_a2:\n",
    "                log_prob = lp[0, 0][None]\n",
    "            else:\n",
    "                log_prob = None\n",
    "            \n",
    "            if save_R:\n",
    "                self.returntogo_pred[self.DT_input['timestep'][0]] =                 R[0]  # R\n",
    "    \n",
    "            if return_R:\n",
    "                return (action_1, action_2, R[0],took_action,log_prob)\n",
    "            else:\n",
    "                return action_1, action_2,took_action,log_prob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "    def action_predict_direct(self, data, return_R=False,return_log_prob_a2 = False):\n",
    "        (timesteps, states, actions_1,actions_2, returns_to_go) = data\n",
    "        (#s, \n",
    "         a_1,a_2, R,lp) = self.model(timesteps=timesteps, states=states,\n",
    "                               actions_1=actions_1,actions_2=actions_2,\n",
    "                               returns_to_go=returns_to_go,return_log_prob_a2 = return_log_prob_a2)\n",
    "        if return_R:\n",
    "            return (a_1,a_2, R,lp)\n",
    "        else:\n",
    "            return a_1,a_2,lp\n",
    "\n",
    "    def save_models(self):\n",
    "        newpath = r'./models/' + self.run_name + '/' + str(self.hero)\n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "        torch.save(self.model.state_dict(), newpath\n",
    "                   + '/a2c_transformer.pt')\n",
    "\n",
    "    def load_models(self):\n",
    "        newpath = r'./models/' + self.run_name + '/' + str(self.hero)\n",
    "        self.model.load_state_dict(torch.load(newpath\n",
    "                                   + '/a2c_transformer.pt'))\n",
    "        self.target_model.load_state_dict(torch.load(newpath\n",
    "                + '/a2c_transformer.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f792b735-4d0a-4d59-94a7-2b596d1fcd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones((1),requires_grad =False)[0] * 110"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d15b02-1aed-4522-9b7a-7e56e3c64385",
   "metadata": {},
   "source": [
    "## model_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f6a37-4598-42ff-bce2-ee9c441cd9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "#referrence for loss : https://github.com/vy007vikas/PyTorch-ActorCriticRL/blob/master/train.py\n",
    "\n",
    "#reference invalid actions ignore\n",
    "#https://ai.stackexchange.com/questions/2980/how-should-i-handle-invalid-actions-when-using-reinforce\n",
    "#ttps://arxiv.org/abs/2006.14171\n",
    "\n",
    "\n",
    "class main_model:\n",
    "\n",
    "    def __init__(  # ,config,args,hero = 1,\n",
    "        self,\n",
    "        qnet_config_dict,\n",
    "        actor_config_dict,\n",
    "        args,\n",
    "        device,\n",
    "        writer,\n",
    "        run_name,\n",
    "        agent,\n",
    "        ):\n",
    "\n",
    "        self.writer = writer\n",
    "        self.hero = agent\n",
    "        self.args = args\n",
    "        self.run_name = run_name\n",
    "        self.device = device  # config['device']\n",
    "        self.state_dim = actor_config_dict['ob_space']  # config['observation_space']#.shape[0]\n",
    "        self.act_dim = actor_config_dict['action_space']  # config['action_space']#.n #3#1 #env.action_space.shape[0]\n",
    "        self.n_blocks = args.model_config['n_blocks']\n",
    "        self.embed_dim = args.model_config['embed_dim']\n",
    "        self.context_len = args.model_config['context_len']\n",
    "        self.n_heads = args.model_config['n_heads']\n",
    "        self.dropout_p = args.model_config['dropout_p']\n",
    "        self.lr = args.learning_rate\n",
    "        self.wt_decay = args.model_config['wt_decay']\n",
    "        self.rb_len = args.model_config['rb_len']\n",
    "\n",
    "        # self.steps        =            config['steps']\n",
    "\n",
    "        self.warmup_epoch = args.model_config['warmup_epoch']\n",
    "        self.total_epoch = args.model_config['total_epoch']\n",
    "        self.initial_lr =  args.model_config['initial_lr'] #5e-4\n",
    "        self.final_lr =  args.model_config['final_lr'] #1e-6\n",
    "        \n",
    "        self.chunk_size = args.model_config['chunk_size']\n",
    "        self.chunk_overlap = args.model_config['chunk_overlap']\n",
    "        #self.max_d4rl_score = -1000.0\n",
    "        #self.total_updates = 0\n",
    "        self.tau = args.model_config['tau']\n",
    "        self.num_steps = args.num_steps\n",
    "        self.total_agents = args.total_agents\n",
    "        self.total_phases = args.total_phases\n",
    "        self.beta = args.model_config['beta']         #0.2 #Q_mse\n",
    "        self.alpha =args.model_config['alpha']          #0.1  #actionloss\n",
    "        self.entropy_coeff = args.model_config['entropy_coeff']         #0.1#0.5   #entropy loss in action\n",
    "        self.val_loss_coeff = args.model_config['val_loss_coeff']        #0.5      #Q loss\n",
    "        \n",
    "\n",
    "        # context_len_=200\n",
    "\n",
    "        self.model = DecisionTransformer(\n",
    "            state_dim=self.state_dim,\n",
    "            act_dim=self.act_dim,\n",
    "            n_blocks=self.n_blocks,\n",
    "            h_dim=self.embed_dim,\n",
    "            context_len=self.context_len,\n",
    "            n_heads=self.n_heads,\n",
    "            drop_p=self.dropout_p,\n",
    "            max_timestep=self.num_steps,\n",
    "            ).to(self.device)\n",
    "\n",
    "        self.target_model = DecisionTransformer(\n",
    "            state_dim=self.state_dim,\n",
    "            act_dim=self.act_dim,\n",
    "            n_blocks=self.n_blocks,\n",
    "            h_dim=self.embed_dim,\n",
    "            context_len=self.context_len,\n",
    "            n_heads=self.n_heads,\n",
    "            drop_p=self.dropout_p,\n",
    "            max_timestep=self.num_steps,\n",
    "            ).to(self.device)\n",
    "\n",
    "        # Set target network parameters to not require gradients\n",
    "        for param in self.target_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.copy_wt()\n",
    "        self.optimizer_2 = torch.optim.AdamW(self.model.parameters(),\n",
    "                lr=0.000005, weight_decay=self.wt_decay)  # lr,\n",
    "\n",
    "        # lr = 0.00001\n",
    "        \n",
    "        self.optimizer_1 = torch.optim.AdamW(self.model.parameters(),\n",
    "                lr=self.lr, weight_decay=self.wt_decay)  # lr,\n",
    "\n",
    "        #self.scheduler =             torch.optim.lr_scheduler.LambdaLR(self.optimizer_1, lambda steps: min((steps + 1) / self.warmup_steps, 1))\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer_1,T_max = self.total_epoch - self.warmup_epoch, eta_min=self.final_lr)\n",
    "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_1, T_0=150, T_mult=2, eta_min=0.01, last_epoch=-1)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def copy_wt(self):\n",
    "\n",
    "        # target_param.load_state_dict(param.state_dict())\n",
    "\n",
    "        for (param, target_param) in zip(self.model.parameters(),\n",
    "                self.target_model.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1\n",
    "                                    - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "    def action_loss_fn_3(\n",
    "        self,\n",
    "        timesteps,\n",
    "        states,\n",
    "        actions_1,actions_2,log_probs_actions_2,\n",
    "        returns_to_go,\n",
    "        return_preds_last,\n",
    "        returns_to_go_cal_last,\n",
    "        print_,\n",
    "        action_masks\n",
    "        ):\n",
    "\n",
    "                         # action_preds_2,\n",
    "                         # action_mask,\n",
    "                         # return_preds_2,\n",
    "                         # returns_target,\n",
    "                         # beta = 0.2\n",
    "\n",
    "        # model : calculate the action for the states\n",
    "        # create a new dataset with last action replaced by a_pred_module\n",
    "        # predict value of the action using model critic\n",
    "        # torch sum value....\n",
    "\n",
    "        # pred_a1 = self.actor.forward(s1)\n",
    "        # loss_actor = -1*torch.sum(self.critic.forward(s1, pred_a1))\n",
    "        # self.actor_optimizer.zero_grad()\n",
    "        # loss_actor.backward()\n",
    "        # self.actor_optimizer.step()\n",
    "\n",
    "        # so what should i do ..... hmmm ... yeah i only need time steps where agent is the hero agent...\n",
    "\n",
    "        ################################\n",
    "        #no epoch no re prediction .... \n",
    "        \n",
    "    \n",
    "        if True: # this does not look into old values\n",
    "            ( #_, \n",
    "             action_logit_model_1,#action_model_2_dir\n",
    "                _,_,logp_pi_a_2_dir,dist_entropy_a_1) =   self.model.forward(timesteps=timesteps, states=states,\n",
    "                                   actions_1=actions_1,actions_2=actions_2,\n",
    "                                   returns_to_go=returns_to_go, print_=2,return_logit=True,return_og_log_prob_a2=True)  # print_\n",
    "    \n",
    "                                                            # ,info = info\n",
    "    \n",
    "            actions_1_ = actions_1.clone().detach()#, requires_grad=False)\n",
    "    \n",
    "            # self.actions_ = actions_\n",
    "            # self.action_preds_model_ = action_preds_model\n",
    "            # actions_[:,:,-1] = action_preds_model\n",
    "    \n",
    "\n",
    "            log_probs_1 = -torch.nn.functional.cross_entropy(action_logit_model_1[:, -1, :], \n",
    "                                           actions_1_[:, -1].long(), reduction=\"none\")\n",
    "            \n",
    "            #(a1_[:,-1,:][hero_steps[:,-1,0]]*action_masks[:,-1,:][hero_steps[:,-1,0]])\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            #actions_1_[:, -1] =     ((action_logit_model_1[:, -1, :] + 0.000001*action_masks) *action_masks).argmax(axis=1).clone().detach() #i will fix this later, there is an issue.... when probabilities for all legal actions are 0, the product is same as illigal actions \n",
    "            actions_1_[:, -1] =     (action_logit_model_1[:, -1, :] *action_masks).argmax(axis=1).clone().detach() #i will fix this later, there is an issue.... when probabilities for all legal actions are 0, the product is same as illigal actions \n",
    "\n",
    "            ( #_, \n",
    "             _,#action_logit_model_2\n",
    "                _,_,logp_pi_a_2,_,) =   self.model.forward(timesteps=timesteps, states=states,\n",
    "                                   actions_1=actions_1_,actions_2=actions_2,\n",
    "                                   returns_to_go=returns_to_go, print_=2,return_logit=True,return_og_log_prob_a2=True)\n",
    "            \n",
    "\n",
    "            \n",
    "            #if False:\n",
    "            #with torch.no_grad():\n",
    "            #    ( #_, \n",
    "            #     _,_,values_) =   self.model.forward(timesteps=timesteps, states=states,\n",
    "            #                       actions_1=actions_1_,actions_2=actions_2,\n",
    "            #                       returns_to_go=returns_to_go, print_=2,return_logit=False)\n",
    "\n",
    "            advantages  = returns_to_go_cal_last - return_preds_last.clone().detach()\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            #if (not torch.isfinite(action_logit_model_1[:, -1,:]).all()) :\n",
    "            #    print('inf_error', 'action_logit_model_1')\n",
    "            #    print('action_logit_model_1',action_logit_model_1[:, -1,:])\n",
    "            #    print('log_probs_1',log_probs_1)\n",
    "            #    print('actions_1_',actions_1_[:, -1].long())\n",
    "            #    a()\n",
    "            #    \n",
    "            #if (not torch.isfinite(log_probs_1).all()):\n",
    "            #    print('inf_error', 'log_probs_1')\n",
    "            #    print('action_logit_model_1',action_logit_model_1[:, -1,:])\n",
    "            #    print('log_probs_1',log_probs_1)\n",
    "            #    print('actions_1_',actions_1_[:, -1].long())\n",
    "            #    a()\n",
    "            #    \n",
    "            #if (not torch.isfinite(actions_1_[:, -1].long() ).all())  :\n",
    "            #    print('inf_error','actions_1_')\n",
    "            #    print('action_logit_model_1',action_logit_model_1[:, -1,:])\n",
    "            #    print('log_probs_1',log_probs_1)\n",
    "            #    print('actions_1_',actions_1_[:, -1].long())\n",
    "            #    a()\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "            try:\n",
    "                #the model is sending everyone .... 1. .... there must be an issue .... lower the weight\n",
    "                log_probs_2_dir = -torch.nn.BCELoss(reduction ='none')(\n",
    "\n",
    "                                               #logp_pi_a_2_dir[:, -1,0],\n",
    "                                               torch.clamp(logp_pi_a_2_dir[:, -1,0], min=0.00001, max=0.9999),\n",
    "                                               #torch.clamp(action_model_2_dir[:, -1,0], min=0.00001, max=0.9999), \n",
    "                                               torch.clamp(log_probs_actions_2[:, -1], min=0.00001, max=0.9999))\n",
    "\n",
    "                #if (not torch.isfinite(action_model_2_dir[:, -1,0]).all()) or (action_model_2_dir[:, -1,0] ==1).any() or (action_model_2_dir[:, -1,0] ==0).any():\n",
    "                #    print('inf_error', 'action_model_2_dir')\n",
    "                #    print('action_model_2_dir',action_model_2_dir[:, -1,0])\n",
    "                #    print('log_probs_2_dir',log_probs_2_dir)\n",
    "                #    print('actions_2',actions_2[:, -1])\n",
    "                #    a()\n",
    "                #    \n",
    "                #if (not torch.isfinite(log_probs_2_dir).all()):\n",
    "                #    print('inf_error', 'log_probs_1')\n",
    "                #    print('action_model_2_dir',action_model_2_dir[:, -1,0])\n",
    "                #    print('log_probs_2_dir',log_probs_2_dir)\n",
    "                #    print('actions_2',actions_2[:, -1])\n",
    "                #    a()\n",
    "                #\n",
    "                #if (not torch.isfinite(actions_2[:, -1]).all()):\n",
    "                #    print('inf_error', 'actions_2')\n",
    "                #    print('action_model_2_dir',action_model_2_dir[:, -1,0])\n",
    "                #    print('log_probs_2_dir',log_probs_2_dir)\n",
    "                #    print('actions_2',actions_2[:, -1])\n",
    "                #    a()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(logp_pi_a_2_dir[:, -1,0])\n",
    "                print(log_probs_actions_2[:, -1])\n",
    "                a()\n",
    "\n",
    "            log_probs_2 = -torch.nn.BCELoss(reduction ='none')(#logp_pi_a_2[:, -1,0],#action_logit_model_2[:, -1,0], \n",
    "                                                               torch.clamp(logp_pi_a_2[:, -1,0], min=0.00001, max=0.9999),\n",
    "                                           torch.clamp(log_probs_actions_2[:, -1], min=0.00001, max=0.9999)\n",
    "                                            )\n",
    "            \n",
    "            #if (not torch.isfinite(log_probs_2).all()):\n",
    "            #    #pass\n",
    "            #    print('inf_error', 'log_probs_2')\n",
    "            #    print('log_probs_2',log_probs_2)\n",
    "            #    a()\n",
    "\n",
    "            #if (not torch.isfinite(action_logit_model_2[:, -1,0]).all()) or (action_logit_model_2[:, -1,0] ==1).any() or (action_logit_model_2[:, -1,0] ==0).any():\n",
    "            #        print('inf_error', 'action_logit_model_2')\n",
    "            #        print('action_logit_model_2',action_logit_model_2[:, -1,0])\n",
    "            #        a()\n",
    "\n",
    "            #print(logp_pi_a_2_dir[:,-1,0],logp_pi_a_2[:,-1,0])\n",
    "\n",
    "            pi_loss = -((log_probs_1 +log_probs_2 + log_probs_2_dir)*(advantages)).mean() - self.entropy_coeff*dist_entropy_a_1.mean()\n",
    "\n",
    "            \n",
    "            \n",
    "            return pi_loss\n",
    "\n",
    "    def value_loss_fn_3(\n",
    "        self,\n",
    "        reward_last,\n",
    "        return_preds_last,\n",
    "        returns_target_last,\n",
    "        returns_to_go_cal_last,\n",
    "        beta=0.2,\n",
    "        alpha=2,\n",
    "        gamma=0.99,\n",
    "        device='cpu',\n",
    "        ):\n",
    "        \n",
    "        RT1 = reward_last[:-1] + gamma * returns_target_last[1:]\n",
    "        try:\n",
    "            Q_TD = torch.nn.functional.smooth_l1_loss(return_preds_last[:-1],RT1) \n",
    "            \n",
    "            Q_TD=Q_TD+torch.nn.functional.smooth_l1_loss(return_preds_last[[-1]], reward_last[[-1]])\n",
    "            \n",
    "            Q_MSE = torch.nn.functional.smooth_l1_loss(return_preds_last, returns_to_go_cal_last)\n",
    "            return (Q_TD ,  Q_MSE)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('in-v-loss')\n",
    "            print( 'return_preds_last' ,return_preds_last)\n",
    "            print('RT1' ,RT1)\n",
    "            print('reward_last' ,reward_last)\n",
    "            print('returns_to_go_cal_last' ,returns_to_go_cal_last)\n",
    "            a()\n",
    "\n",
    "        \n",
    "\n",
    "    def calculate_loss(\n",
    "        self,\n",
    "        timesteps,\n",
    "        returns_target, #target model predictions \n",
    "        returns_to_go_cal,  # Q calculated\n",
    "        hero_steps,#current_agent_acting,\n",
    "        states,\n",
    "        actions_1,actions_2,log_probs_actions_2,\n",
    "        return_preds,\n",
    "        return_preds_v        \n",
    "        ,reward, #actual rewards \n",
    "        returns_to_go,  #actual R2G given to the model\n",
    "        action_masks,\n",
    "        print_,\n",
    "        chunk_id,\n",
    "        divi\n",
    "        ):\n",
    "\n",
    "\n",
    "        \n",
    "        # only consider non padded elements\n",
    "\n",
    "        reward_last = reward[:, -1].squeeze().view(-1).to(self.device,dtype=torch.float32) #require the last reward only\n",
    "        return_preds_last = return_preds[:, -1,-1].squeeze().view(-1).to(self.device,dtype=torch.float32)  # act_dim , this is what out model predicted\n",
    "        return_preds_v_last = return_preds_v[:, -1,-1].squeeze().view(-1).to(self.device,dtype=torch.float32)  # act_dim , this is what out model predicted\n",
    "       \n",
    "        returns_target_last = returns_target[:, -1,-1].squeeze().view(-1).to(self.device,dtype=torch.float32) # target model prediction ... should be the 1st prediction\n",
    "       \n",
    "        returns_to_go_cal_last = returns_to_go_cal[:, -1,-1].squeeze().view(-1).to(self.device,dtype=torch.float32) # we need the Q value of the last action\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        hero_step_filter = hero_steps[:,-1,0] # current_agent_acting[:, -1, 0] == self.hero #last action is out heros\n",
    "\n",
    "        if sum(hero_step_filter) == 0:\n",
    "            policy_loss = torch.tensor(0,device=self.device)\n",
    "        else:\n",
    "            policy_loss = self.action_loss_fn_3(timesteps[hero_step_filter],\n",
    "                                                states[hero_step_filter],\n",
    "                                                actions_1[hero_step_filter],actions_2[hero_step_filter],log_probs_actions_2[hero_step_filter],\n",
    "                                                returns_to_go[hero_step_filter],\n",
    "                                                \n",
    "                                                return_preds_last[hero_step_filter],\n",
    "                                                \n",
    "                                                returns_to_go_cal_last[hero_step_filter], print_,\n",
    "                                               \n",
    "                                                action_masks[:,-1,:][hero_step_filter])\n",
    "\n",
    "        # but we need to figure out a little more\n",
    "\n",
    "\n",
    "\n",
    "        if  len(reward_last) == 0:\n",
    "            print('chunk_id',chunk_id)\n",
    "            print('reward',reward.shape)\n",
    "            print('reward_last',reward)\n",
    "            print('return_preds_last',return_preds_last.shape)\n",
    "\n",
    "\n",
    "            print('return_preds_last',return_preds_last)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        if True:  # self.value_cal_loss:\n",
    "            (Q_TD , Q_MSE) = self.value_loss_fn_3(  # value_loss_fn(return_preds_2,return_target,action_mask,beta=1)\n",
    "                                    reward_last,\n",
    "                                    return_preds_v_last,\n",
    "                                    returns_target_last,\n",
    "                                    returns_to_go_cal_last,\n",
    "                                    beta=0.5,\n",
    "                                    alpha=2,\n",
    "                                    gamma=0.99\n",
    "                                )\n",
    "\n",
    "            Q_TD = Q_TD/divi\n",
    "            Q_MSE = Q_MSE/divi\n",
    "            Q_loss = Q_TD + self.beta*Q_MSE\n",
    "            \n",
    "            policy_loss =  policy_loss/divi\n",
    "\n",
    "            \n",
    "            total_loss = self.val_loss_coeff*Q_loss + self.alpha*policy_loss\n",
    "\n",
    "            if (not torch.isfinite(policy_loss)) or (not torch.isfinite(Q_TD)) or (not torch.isfinite(Q_MSE)):\n",
    "                print(divi, 'policy_loss',policy_loss,'Q_TD',Q_TD,'Q_MSE',Q_MSE)\n",
    "                a()\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        return total_loss, (Q_TD, Q_MSE, Q_loss, policy_loss)\n",
    "\n",
    "    def train_write(self, iteration, print_=False):\n",
    "\n",
    "        total_loss_list = []\n",
    "        Q_TD_list = []\n",
    "        Q_MSE_list = []\n",
    "        Q_loss_list = []\n",
    "        policy_loss_list = []\n",
    "\n",
    "        if self.args.TB_log:\n",
    "            self.writer.add_scalar(\"charts/learning_rate\", self.optimizer_1.param_groups[0][\"lr\"], iteration)        \n",
    "        \n",
    "        for epoch in range(self.args.update_epochs):\n",
    "            for i,batch in enumerate(self.traj_data_loader):\n",
    "                total_loss = 0\n",
    "                Q_TD = 0\n",
    "                Q_MSE = 0\n",
    "                Q_loss = 0\n",
    "                policy_loss = 0\n",
    "                print(i)\n",
    "\n",
    "\n",
    "                batch_len = batch[0].shape[1]#2840\n",
    "                \n",
    "                if batch_len > self.chunk_size:\n",
    "                    a_ = [(i,i+self.chunk_size) for i in range(0, batch_len     -self.chunk_size,self.chunk_size-self.chunk_overlap)  ]\n",
    "                    a_  =a_+[ (a_[-1][1] -self.chunk_overlap ,batch_len) ]\n",
    "                else:\n",
    "                    a_ = [(0,batch_len)]\n",
    "    \n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                divi = len(a_)\n",
    "\n",
    "                \n",
    "                print('divi',divi,'chunk_size',self.chunk_size,'batch_shape',batch[0].shape)\n",
    "                \n",
    "                for (chunk_id, i) in enumerate(a_):#range(0,batch[0].shape[1] - self.chunk_size + 1,self.chunk_size - self.chunk_overlap)):\n",
    "\n",
    "                    # print(i,(i + self.chunk_size))\n",
    "                    total_loss_chunk, Q_TD_chunk, Q_MSE_chunk, Q_loss_chunk, policy_loss_chunk = self.train_write_smaller_chunk((tens[:, i[0]:i[1]] for tens in batch),\n",
    "                                                                                                                                    iteration, epoch, chunk_id, print_=print_,divi=divi)\n",
    "\n",
    "                    \n",
    "                    total_loss= total_loss + total_loss_chunk\n",
    "                    Q_TD = Q_TD+ Q_TD_chunk\n",
    "                    Q_MSE = Q_MSE+ Q_MSE_chunk\n",
    "                    Q_loss = Q_loss+ Q_loss_chunk\n",
    "                    policy_loss = policy_loss+ policy_loss_chunk\n",
    "\n",
    "                total_loss_list.append(total_loss)\n",
    "                Q_TD_list.append(Q_TD)\n",
    "                Q_MSE_list.append(Q_MSE)\n",
    "                Q_loss_list.append(Q_loss)\n",
    "                policy_loss_list.append(policy_loss)            \n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "        \n",
    "        if iteration < self.warmup_epoch:\n",
    "            # Linear warmup: Gradually increase learning rate during warmup\n",
    "            lr = self.initial_lr * iteration / self.warmup_epoch\n",
    "            for param_group in self.optimizer_1.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        else:\n",
    "            self.scheduler.step()\n",
    "\n",
    "        \n",
    "        print(\"total_loss\", np.mean(total_loss_list),\n",
    "              \"Q_TD\", np.mean(Q_TD_list),\n",
    "                \"Q_MSE\", np.mean(Q_MSE_list),\n",
    "                \"Q_loss\", np.mean(Q_loss_list),\n",
    "                \"policy_loss\", np.mean(policy_loss_list))\n",
    "\n",
    "        if self.args.TB_log:\n",
    "        \n",
    "            self.writer.add_scalar(\"total_loss\", np.mean(total_loss_list), iteration)\n",
    "            self.writer.add_scalar(\"Q_TD\", np.mean(Q_TD_list), iteration)\n",
    "            self.writer.add_scalar(\"Q_MSE\", np.mean(Q_MSE_list), iteration)\n",
    "            self.writer.add_scalar(\"Q_loss\", np.mean(Q_loss_list), iteration)\n",
    "            self.writer.add_scalar(\"policy_loss\", np.mean(policy_loss_list), iteration)    \n",
    "                    \n",
    "                \n",
    "                \n",
    "        \n",
    "        self.copy_wt()\n",
    "\n",
    "\n",
    "    def train_write_smaller_chunk(\n",
    "        self,\n",
    "        data,\n",
    "        iteration,\n",
    "        epoch,\n",
    "        chunk_id,\n",
    "        print_=False,divi = 1\n",
    "        ):\n",
    "\n",
    "        (\n",
    "            timesteps,\n",
    "            states,\n",
    "            actions_1,actions_2,log_probs_actions_2,\n",
    "            returntogo,\n",
    "            returns_to_go_cal,\n",
    "            returntogo_pred,\n",
    "            reward,\n",
    "            traj_mask,\n",
    "            action_masks,\n",
    "            current_agent_acting,\n",
    "            current_agent_simple,\n",
    "            current_agent,\n",
    "            current_phase,\n",
    "            current_troops_count,\n",
    "            ) = data\n",
    "\n",
    "\n",
    "        if len(timesteps[0].shape) ==0:\n",
    "            print( timesteps.shape )\n",
    "\n",
    "        \n",
    "        timesteps = timesteps[0].to(self.device)  # B x T\n",
    "        states = states[0].to(self.device)  # B x T x state_dim\n",
    "        actions_1 = actions_1[0].to(self.device)  # B x T x act_dim\n",
    "        actions_2 = actions_2[0].to(self.device)  # B x T x act_dim\n",
    "        log_probs_actions_2 = log_probs_actions_2[0].to(self.device)\n",
    "        reward = reward[0].to(self.device)\n",
    "        returns_to_go_cal =             returns_to_go_cal[0].to(self.device).unsqueeze(dim=-1)  # B x T x 1\n",
    "        returntogo = returntogo[0].to(self.device)\n",
    "        returntogo_pred = returntogo_pred[0].to(self.device)\n",
    "        traj_mask = traj_mask[0].to(self.device)  # B x T\n",
    "        action_masks = action_masks[0].to(self.device)\n",
    "        current_agent_acting = current_agent_acting[0].to(self.device)\n",
    "        current_agent_simple = current_agent_simple[0].to(self.device)\n",
    "        current_agent = current_agent[0].to(self.device)\n",
    "        current_phase = current_phase[0].to(self.device)\n",
    "        current_troops_count = current_troops_count[0].to(self.device)\n",
    "\n",
    "        info = dict({})\n",
    "\n",
    "\n",
    "        \n",
    "        hero_steps = current_agent_simple == self.hero\n",
    "\n",
    "        states = torch.cat((states, action_masks * hero_steps,\n",
    "                              current_phase, current_agent,\n",
    "                              current_troops_count[:, :, None]), axis=2)  # ,torch.ones(len(action_masks))[:,None]*self.hero\n",
    "        (action_pred_1,action_pred_2) =  (actions_1* hero_steps[:,0],\n",
    "                                                        actions_2* hero_steps[:,0]\n",
    "                                                         )\n",
    "\n",
    "\n",
    "    \n",
    "    def train_write_smaller_chunk_(\n",
    "        self,\n",
    "        data,\n",
    "        iteration,\n",
    "        epoch,\n",
    "        chunk_id,\n",
    "        print_=False,divi = 1\n",
    "        ):\n",
    "\n",
    "        (\n",
    "            timesteps,\n",
    "            states,\n",
    "            actions_1,actions_2,log_probs_actions_2,\n",
    "            returntogo,\n",
    "            returns_to_go_cal,\n",
    "            returntogo_pred,\n",
    "            reward,\n",
    "            traj_mask,\n",
    "            action_masks,\n",
    "            current_agent_acting,\n",
    "            current_agent_simple,\n",
    "            current_agent,\n",
    "            current_phase,\n",
    "            current_troops_count,\n",
    "            ) = data\n",
    "\n",
    "\n",
    "        if len(timesteps[0].shape) ==0:\n",
    "            print( timesteps.shape )\n",
    "\n",
    "        \n",
    "        timesteps = timesteps[0].to(self.device)  # B x T\n",
    "        states = states[0].to(self.device)  # B x T x state_dim\n",
    "        actions_1 = actions_1[0].to(self.device)  # B x T x act_dim\n",
    "        actions_2 = actions_2[0].to(self.device)  # B x T x act_dim\n",
    "        log_probs_actions_2 = log_probs_actions_2[0].to(self.device)\n",
    "        reward = reward[0].to(self.device)\n",
    "        returns_to_go_cal =             returns_to_go_cal[0].to(self.device).unsqueeze(dim=-1)  # B x T x 1\n",
    "        returntogo = returntogo[0].to(self.device)\n",
    "        returntogo_pred = returntogo_pred[0].to(self.device)\n",
    "        traj_mask = traj_mask[0].to(self.device)  # B x T\n",
    "        action_masks = action_masks[0].to(self.device)\n",
    "        current_agent_acting = current_agent_acting[0].to(self.device)\n",
    "        current_agent_simple = current_agent_simple[0].to(self.device)\n",
    "        current_agent = current_agent[0].to(self.device)\n",
    "        current_phase = current_phase[0].to(self.device)\n",
    "        current_troops_count = current_troops_count[0].to(self.device)\n",
    "\n",
    "        info = dict({})\n",
    "\n",
    "\n",
    "        \n",
    "        hero_steps = current_agent_simple == self.hero\n",
    "\n",
    "        states = torch.cat((states, action_masks * hero_steps,\n",
    "                              current_phase, current_agent,\n",
    "                              current_troops_count[:, :, None]), axis=2)  # ,torch.ones(len(action_masks))[:,None]*self.hero\n",
    "\n",
    "\n",
    "\n",
    "        #print(actions_1.requires_grad,actions_2.requires_grad)\n",
    "\n",
    "        (action_pred_1,action_pred_2) =  (actions_1* hero_steps[:,0],\n",
    "                                                        actions_2* hero_steps[:,0]\n",
    "                                                         )\n",
    "\n",
    "\n",
    "        #so this return to go is for value loss estimation\n",
    "\n",
    "        _,_, returntogo_pred_v,_ = self.model.forward(\n",
    "                                                        timesteps=timesteps,\n",
    "                                                        states=states,\n",
    "                                                        actions_1=action_pred_1,actions_2=action_pred_2,\n",
    "                                                        returns_to_go=returntogo,\n",
    "                                                            print_=print_\n",
    "                                                        #,info = info\n",
    "                                                    )\n",
    "        \n",
    "        # this is also for value function loss estimation\n",
    "\n",
    "        (#state_preds_target, \n",
    "         _,_,return_preds_target,_) =  self.target_model.forward(timesteps=timesteps,\n",
    "                                                                                states=states, actions_1=action_pred_1,actions_2=action_pred_2,\n",
    "                                                                                returns_to_go=returntogo, print_=print_)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        (#state_preds_target, \n",
    "         return_preds_target) = (return_preds_target.detach())\n",
    "\n",
    "\n",
    "        if  len(reward[:, -1].squeeze().view(-1).shape) == 0:\n",
    "            \n",
    "            print('return_preds_last',return_preds_target.shape)\n",
    "            print('reward_last',reward.shape)\n",
    "            print('returns_to_go_cal_last',returns_to_go_cal.shape)\n",
    "\n",
    "            print('return_preds_last',return_preds_target)\n",
    "            print('reward_last',reward)\n",
    "            print('returns_to_go_cal_last',returns_to_go_cal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        total_loss, (Q_TD, Q_MSE, Q_loss, policy_loss) = self.calculate_loss(\n",
    "                                                                                timesteps,\n",
    "                                                                                return_preds_target,#target model predictions \n",
    "                                                                                returns_to_go_cal, # Q calculated\n",
    "                                                                                hero_steps    ,#current_agent_acting,\n",
    "                                                                                states,\n",
    "                                                                                actions_1,actions_2,log_probs_actions_2,\n",
    "                                                                                returntogo_pred,\n",
    "                                                                                returntogo_pred_v,\n",
    "                                                                                reward,#actual rewards \n",
    "                                                                                returntogo,  #actual R2G given to the model\n",
    "                                                                                action_masks,\n",
    "                                                                                print_,\n",
    "                                                                                chunk_id,divi\n",
    "                                                                                )\n",
    "        #if chunk_id == 0:\n",
    "        #    print (chunk_id, total_loss)\n",
    "\n",
    "        # action_loss = #nn.CrossEntropyLoss().forward(action_preds_2,action_target)#F.mse_loss(action_preds_2, action_target.float(), reduction='mean')\n",
    "\n",
    "        self.optimizer_1.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.25)\n",
    "        self.optimizer_1.step()\n",
    "        #self.scheduler.step()\n",
    "        \n",
    "        # policy_loss.backward(retain_graph=True)\n",
    "\n",
    "        return total_loss.detach().item() ,Q_TD.detach().item(), Q_MSE.detach().item(), Q_loss.detach().item(), policy_loss.detach().item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def init_path(self):\n",
    "        self.paths = []\n",
    "        \n",
    "\n",
    "    def init_CL_sample_store(self):\n",
    "\n",
    "\n",
    "        if self.context_len > 1: # transformer model\n",
    "            self.DT_input = {  # self.actor_config_dict['ob_space']\n",
    "                'timestep': torch.zeros((1,\n",
    "                                        self.context_len),requires_grad =False).to(self.device,\n",
    "                        dtype=torch.int),\n",
    "                'state': torch.zeros((1, self.context_len,\n",
    "                                     self.state_dim),requires_grad =False).to(self.device,dtype=torch.float32),\n",
    "                'action_1': torch.zeros((1, self.context_len\n",
    "                                      )).to(self.device,dtype=torch.float32),\n",
    "                'action_2': torch.zeros((1, self.context_len\n",
    "                                      )).to(self.device,dtype=torch.float32),\n",
    "                \n",
    "                'return_to_go': torch.ones((1,\n",
    "                        self.context_len),requires_grad =False).to(self.device,dtype=torch.float32) * 110,\n",
    "                }\n",
    "        else: # single traje models\n",
    "            self.DT_input = {  # self.actor_config_dict['ob_space']\n",
    "                'timestep': torch.zeros((1,\n",
    "                                        ),requires_grad =False).to(self.device,\n",
    "                        dtype=torch.int),\n",
    "                'state': torch.zeros((1,\n",
    "                                     self.state_dim),requires_grad =False).to(self.device,dtype=torch.float32),\n",
    "                'action_1': torch.zeros((1,\n",
    "                                      )).to(self.device,dtype=torch.float32),\n",
    "                'action_2': torch.zeros((1,\n",
    "                                      )).to(self.device,dtype=torch.float32),\n",
    "                \n",
    "                'return_to_go': torch.ones((1,\n",
    "                                    ),requires_grad =False).to(self.device,dtype=torch.float32) * 110,\n",
    "                }            \n",
    "    \n",
    "        self.returntogo = torch.zeros((self.num_steps,\n",
    "                1),requires_grad =False).to(self.device,dtype=torch.float32)  # self.total_agents\n",
    "        self.returntogo_pred = torch.zeros((self.num_steps,\n",
    "                1)).to(self.device,dtype=torch.float32)  # self.total_agents\n",
    "\n",
    "    #here\n",
    "    def current_model_in(\n",
    "        self,\n",
    "        observation,\n",
    "        curr_agent,\n",
    "        phase_mapping,\n",
    "        curr_agent_mapping,\n",
    "        env_board_agents=[],\n",
    "        ):\n",
    "\n",
    "\n",
    "\n",
    "        #single obs\n",
    "        self.model_in =  torch.hstack((observation['observation'\n",
    "                         ].reshape(-1).to(self.device),\n",
    "                         torch.tensor(observation['action_mask'\n",
    "                         ].reshape(-1)).to(self.device) * (curr_agent\n",
    "                         == self.hero), phase_mapping.to(self.device),\n",
    "                         curr_agent_mapping.to(self.device),\n",
    "                         ( (torch.tensor([env_board_agents[self.hero].bucket]).to(self.device) - 5.2496)/1.4733\n",
    "                                      )))[None,\n",
    "                         :].float().requires_grad_(False).to(self.device)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def update_CL_sample_store(\n",
    "        self,\n",
    "        curr_agent_,\n",
    "        inp={'step': None, 'act_2_1': [],'act_2_2': [], 'curr_reward_list': []},\n",
    "        before_action=True,\n",
    "        ):\n",
    "\n",
    "        if self.context_len > 1:\n",
    "            \n",
    "            if before_action == 1 :\n",
    "    \n",
    "                if inp['step'] == 0:\n",
    "    \n",
    "                    # print(self.model_in.shape)\n",
    "                    # print(self.model_in.repeat(self.context_len).shape)\n",
    "    \n",
    "                    self.DT_input['state'] = self.model_in.repeat(self.context_len,\n",
    "                            1).to(self.device)[None, :]\n",
    "                else:\n",
    "    \n",
    "                # if step<self.context_len:\n",
    "    \n",
    "                #    trace[step] = model_in\n",
    "    \n",
    "                    self.DT_input['state'][0, 0:-1] = self.DT_input['state'\n",
    "                            ][0, 1:].clone()\n",
    "                    self.DT_input['state'][0, -1] = self.model_in\n",
    "                    self.DT_input['timestep'][0, 0:-1] = self.DT_input['timestep'][0, 1:].clone()\n",
    "                    self.DT_input['timestep'][0, -1] = inp['step']\n",
    "                    self.DT_input['action_1'][0, 0:-1] = self.DT_input['action_1'][0, 1:].clone()\n",
    "                    self.DT_input['action_2'][0, 0:-1] = self.DT_input['action_2'][0, 1:].clone()\n",
    "                    \n",
    "                    self.DT_input['return_to_go'][0, 0:-1] = self.DT_input['return_to_go'][0, 1:].clone()\n",
    "            elif before_action == 2 :\n",
    "                if self.hero == curr_agent_:\n",
    "                    self.DT_input['action_1'][0, -1] = inp['act_2_1']\n",
    "                else:\n",
    "                    self.DT_input['action_1'][0, -1] = 0\n",
    "    \n",
    "            elif before_action == 3  :\n",
    "                if self.hero == curr_agent_:\n",
    "                    self.DT_input['action_2'][0, -1] = inp['act_2_2']\n",
    "                else:\n",
    "                    self.DT_input['action_2'][0, -1] = 0\n",
    "            else:\n",
    "                    \n",
    "                self.DT_input['return_to_go'][0, -1] = self.DT_input['return_to_go'][0, -1] -    inp['curr_reward_list']  # [self.hero]\n",
    "                self.returntogo[inp['step']] = self.DT_input['return_to_go'\n",
    "                        ][0, -1]\n",
    "\n",
    "        else:\n",
    "            if before_action == 1 :\n",
    "                self.DT_input['state'][0] = self.model_in\n",
    "                self.DT_input['timestep'][0] = inp['step']\n",
    "            elif before_action == 2 :\n",
    "                if self.hero == curr_agent_:\n",
    "                    self.DT_input['action_1'][0] = inp['act_2_1']\n",
    "                else:\n",
    "                    self.DT_input['action_1'][0] = 0                \n",
    "            \n",
    "            elif before_action == 3  :\n",
    "                if self.hero == curr_agent_:\n",
    "                    self.DT_input['action_2'][0] = inp['act_2_2']\n",
    "                else:\n",
    "                    self.DT_input['action_2'][0] = 0\n",
    "            else:\n",
    "                    \n",
    "                self.DT_input['return_to_go'][0] = self.DT_input['return_to_go'][0] -    inp['curr_reward_list']  # [self.hero]\n",
    "                self.returntogo[inp['step']] = self.DT_input['return_to_go'][0]               \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def update_train_data(\n",
    "        self,\n",
    "        step_count,\n",
    "        obs,\n",
    "        ob_space_shape,\n",
    "        rewards_2,\n",
    "        dones_2,\n",
    "        actions_1,\n",
    "        actions_2,\n",
    "        log_probs_actions_2,\n",
    "        action_masks,\n",
    "        current_agent,\n",
    "        current_agent_acting,\n",
    "        current_phase,\n",
    "        current_troops_count,\n",
    "        map_agent_phase_vector,\n",
    "        ):\n",
    "\n",
    "        \n",
    "        \n",
    "        data_ = collections.defaultdict(torch.tensor)\n",
    "        data_['observations'] =             obs[:step_count].reshape(-1,\n",
    "                np.prod(ob_space_shape))\n",
    "\n",
    "                # data_['next_observations'] = obs[1:step_count+1].to(device =self.args.pin_memory_device).reshape(-1,np.prod(T.ob_space_shape)) #torch.tensor([1,2,3,4])\n",
    "                # this return to go is the actual input\n",
    "\n",
    "        data_['returntogo'] =           self.returntogo[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['returntogo_pred'] =      self.returntogo_pred[:step_count]  # torch.tensor([1,2,3,4])\n",
    "\n",
    "        data_['rewards'] =              rewards_2[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['terminals'] =            dones_2[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['actions_1'] =            actions_1[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['actions_2'] =            actions_2[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['log_probs_actions_2'] =  log_probs_actions_2[:step_count]\n",
    "        data_['action_masks'] =         action_masks[:step_count]\n",
    "        data_['current_agent_acting'] = current_agent_acting[:step_count]\n",
    "        data_['current_agent_simple'] = current_agent[:step_count]\n",
    "        data_['current_agent'] =        map_agent_phase_vector(current_agent[:step_count],\n",
    "                                               num_classes=self.total_agents + 1)[:, 1:]\n",
    "        data_['current_phase'] =        map_agent_phase_vector(current_phase[:step_count],\n",
    "                                               num_classes=self.total_phases)\n",
    "        data_['current_troops_count'] = current_troops_count[:step_count]\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        self.datase = TrajectoryDataset_per_episode([data_], #len(data[0]['observations'])<self.args.context_len\n",
    "                context_len=1,#self.args.context_len,\n",
    "                rtg_scale=1,#self.args.rtg_scale,\n",
    "                dev = self.device,#self.args.pin_memory_device,\n",
    "                 gamma=0.99#self.args.gamma\n",
    "                        )\n",
    "\n",
    "        self.path_que(DataLoader(self.datase, batch_size=len(self.datase)))\n",
    "    def path_que(self, dtl):\n",
    "        \n",
    "        if (len(self.paths)==self.rb_len):\n",
    "            if (self.rb_len >1):\n",
    "                self.paths.pop(random.randrange(len(self.paths)-self.args.num_episodes +1 )) # dont pop the most recent experiences and ensure rb_len > num.episodes\n",
    "            else: \n",
    "                self.paths.pop()\n",
    "        self.paths.append(dtl)\n",
    "        \n",
    "    def create_training_dataset(self):\n",
    "        self.traj_dataset =  TrajectoryDataset_2_through_episodes(self.paths)  # a dataset of dataloaders\n",
    "\n",
    "        self.traj_data_loader = DataLoader(  # only spit 1 episode a time\n",
    "            self.traj_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=self.args.shuffle,\n",
    "            pin_memory=self.args.pin_memory,\n",
    "            drop_last=self.args.drop_last,\n",
    "            pin_memory_device=self.args.pin_memory_device,\n",
    "            )\n",
    "\n",
    "    \n",
    "    def action_predict(self, save_R=True, return_R=False,shift=1,action_masks = [],return_log_prob_a2 = False): \n",
    "        (#s, \n",
    "         a_1,a_2, R,lp) = self.model(timesteps=self.DT_input['timestep'],\n",
    "                               states=self.DT_input['state'],\n",
    "                               actions_1=self.DT_input['action_1'],\n",
    "                               actions_2=self.DT_input['action_2'],\n",
    "                               returns_to_go=( self.DT_input['return_to_go'][:, :, None] if self.context_len>1 else self.DT_input['return_to_go'][:, None]), \n",
    "                                return_log_prob_a2 = return_log_prob_a2)\n",
    "\n",
    "\n",
    "        took_action = False\n",
    "\n",
    "        \n",
    "\n",
    "        if self.context_len>1: # handling transformer model\n",
    "            # I need to handle for non transformer actions\n",
    "            if len(action_masks) > 0:\n",
    "                #handling when all the probability of masked actions are zero.... #have to force the model to pick 1st valid action.\n",
    "                masked_action = (a_1[0, -1, :]*action_masks)\n",
    "                #valid_ind = torch.nonzero(action_masks).squeeze()\n",
    "                if torch.any(masked_action !=0):\n",
    "                    took_action = True\n",
    "                    \n",
    "                    action_1 = (masked_action).argmax()[None]\n",
    "                else: #im hoping that slowly and steadily the model would learn to predict non zero probabilities for action mask\n",
    "                    \n",
    "                    action_1 = torch.tensor(np.random.choice(torch.where(action_masks)[0]))[None]  #(masked_action + 0.0000001*action_masks).argmax()[None]\n",
    "            else:\n",
    "                action_1 = a_1[0, -1, :].argmax()[None]\n",
    "                \n",
    "            action_2 = a_2[0,-1, 0][None]\n",
    "            if return_log_prob_a2:\n",
    "                log_prob = lp[0,-1, 0][None]\n",
    "            else:\n",
    "                log_prob = None\n",
    "            \n",
    "            if save_R:\n",
    "                self.returntogo_pred[self.DT_input['timestep'][0, -1]] =                 R[0, -1]  # R\n",
    "    \n",
    "            if return_R:\n",
    "                return (action_1, action_2, R[0, -1],took_action,log_prob)\n",
    "            else:\n",
    "                return action_1, action_2,took_action,log_prob\n",
    "\n",
    "\n",
    "        else: # not we are talking about the single obs models\n",
    "\n",
    "            # I need to handle for non transformer actions\n",
    "            if len(action_masks) > 0:\n",
    "                #handling when all the probability of masked actions are zero.... #have to force the model to pick 1st valid action.\n",
    "                masked_action = (a_1[0, :]*action_masks)\n",
    "                #valid_ind = torch.nonzero(action_masks).squeeze()\n",
    "                if torch.any(masked_action !=0):\n",
    "                    took_action = True\n",
    "                    \n",
    "                    action_1 = (masked_action).argmax()[None]\n",
    "                else: #im hoping that slowly and steadily the model would learn to predict non zero probabilities for action mask\n",
    "                    \n",
    "                    action_1 = torch.tensor(np.random.choice(torch.where(action_masks)[0]))[None]  #(masked_action + 0.0000001*action_masks).argmax()[None]\n",
    "            else:\n",
    "                action_1 = a_1[0, :].argmax()[None]\n",
    "                \n",
    "            action_2 = a_2[0, 0][None]\n",
    "            if return_log_prob_a2:\n",
    "                log_prob = lp[0, 0][None]\n",
    "            else:\n",
    "                log_prob = None\n",
    "            \n",
    "            if save_R:\n",
    "                self.returntogo_pred[self.DT_input['timestep'][0]] =                 R[0]  # R\n",
    "    \n",
    "            if return_R:\n",
    "                return (action_1, action_2, R[0],took_action,log_prob)\n",
    "            else:\n",
    "                return action_1, action_2,took_action,log_prob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "    def action_predict_direct(self, data, return_R=False,return_log_prob_a2 = False):\n",
    "        (timesteps, states, actions_1,actions_2, returns_to_go) = data\n",
    "        (#s, \n",
    "         a_1,a_2, R,lp) = self.model(timesteps=timesteps, states=states,\n",
    "                               actions_1=actions_1,actions_2=actions_2,\n",
    "                               returns_to_go=returns_to_go,return_log_prob_a2 = return_log_prob_a2)\n",
    "        if return_R:\n",
    "            return (a_1,a_2, R,lp)\n",
    "        else:\n",
    "            return a_1,a_2,lp\n",
    "\n",
    "    def save_models(self):\n",
    "        newpath = r'./models/' + self.run_name + '/' + str(self.hero)\n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "        torch.save(self.model.state_dict(), newpath\n",
    "                   + '/a2c_transformer.pt')\n",
    "\n",
    "    def load_models(self):\n",
    "        newpath = r'./models/' + self.run_name + '/' + str(self.hero)\n",
    "        self.model.load_state_dict(torch.load(newpath\n",
    "                                   + '/a2c_transformer.pt'))\n",
    "        self.target_model.load_state_dict(torch.load(newpath\n",
    "                + '/a2c_transformer.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056dfe34-b872-459a-a0ab-2fed6ee1b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc6e4d-f4d1-4016-9b83-3e73a2dd03ce",
   "metadata": {},
   "source": [
    "# hero agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb54a04-3ca6-4b9c-a5dd-976f958bb6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Hero_agent(int):\n",
    "    def init_properties(self,agent_count,phases,cp=[],df=[],direct_action=True):\n",
    "        #self.draw_count = 0\n",
    "        self.init_win_count_iter(agent_count)\n",
    "        self.init_move_count_epi(phases)\n",
    "        self.cp = cp\n",
    "        self.df = df\n",
    "        self.direct_action = direct_action\n",
    "        self.init_reward_concern(agent_count,cp=cp,df=df)\n",
    "        \n",
    "    def init_reward_concern(self,agent_count,cp=[],df=[]):\n",
    "        if len(cp)==0:\n",
    "            cp = [int(self)]\n",
    "        self.concern=torch.tensor([(1 if i in cp \n",
    "                             else \n",
    "                             (-1 if i in df \n",
    "                                  else 0)) for i in range(1,agent_count+1) ])\n",
    "        #self.concern_2 = self.concern\n",
    "        #self.concern_2[self-1] =0\n",
    "        \n",
    "        self.multi_dependency = (sum(self.concern !=0)>1)\n",
    "        \n",
    "        \n",
    "    def init_model(self,model_name=\"DDQN_module\",\n",
    "                   kwarg = dict({})):\n",
    "        self.model = model_selector(model_name=model_name, \n",
    "                                    kwarg = kwarg)\n",
    "\n",
    "        \n",
    "    def init_win_count_iter(self,agent_count):\n",
    "        self.count_dict = {i:0 for i in range(1,agent_count+1)}\n",
    "        self.count_draw_dict = {i:0 for i in range(1,agent_count+1)}\n",
    "        self.draw_territory_count = 0\n",
    "    def init_move_count_epi(self,phases):\n",
    "        self.bad_move_count = 0\n",
    "        self.bad_move_phase_count = {i:0 for i in phases}\n",
    "        self.move_count =  {i:0 for i in phases}        \n",
    "    \n",
    "    def model_def(self, model):\n",
    "        self.model =model\n",
    "\n",
    "    def action_predict(self,save_R=True,return_R = False,action_masks = [],return_log_prob_a2 = False):\n",
    "        return self.model.action_predict(save_R=save_R,return_R = return_R, action_masks = action_masks,return_log_prob_a2 = return_log_prob_a2)\n",
    "\n",
    "    def action_predict_direct(self,data,return_R = False,return_log_prob_a2 = False):\n",
    "        return self.model.action_predict_direct(data,return_R = return_R,return_log_prob_a2 = return_log_prob_a2)\n",
    "    def save_models(self):\n",
    "        self.model.save_models()\n",
    "\n",
    "    def process_reward(self,rewards,step,hero_steps):\n",
    "        if self.multi_dependency and self.direct_action:\n",
    "            return (rewards*self.concern.to(rewards.device)).sum(-1)[:step][hero_steps][:,None]\n",
    "        elif self.multi_dependency and not self.direct_action:\n",
    "            base_rew = torch.zeros( rewards[:step,self-1][hero_steps].shape,require_grad=False)\n",
    "            #print(base_rew)\n",
    "\n",
    "            \n",
    "            hero_step_list  = np.arange(0,step)[hero_steps]\n",
    "            for i,j in zip(hero_step_list[:-1],hero_step_list[1:]):\n",
    "                if j-i>1:\n",
    "                    #print(j,i,rewards[i:j],(rewards[i:j]*self.concern),(rewards[i:j]*self.concern).sum())\n",
    "                    base_rew[i]+= (rewards[i:j]*self.concern).sum()\n",
    "            #print(base_rew,rewards[hero_step_list[-1]:],(rewards[hero_step_list[-1]:]*self.concern))\n",
    "            base_rew[-1]+= (rewards[hero_step_list[-1]:]*self.concern).sum()\n",
    "            \n",
    "            return base_rew[:,None]\n",
    "            \n",
    "        else:\n",
    "            return rewards[:step][hero_steps][:,None]\n",
    "    \n",
    "    #def model_forward_call(self,name,kwarg):\n",
    "    #    return self.model_dict[name](**kwarg)\n",
    "        \n",
    "\n",
    "a = Hero_agent(1)\n",
    "a.init_properties(3,[1,2,3],cp=[1],df=[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93987dc-f231-4bb2-b391-daacb339f6e9",
   "metadata": {},
   "source": [
    "# replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb52f02c-108a-429c-84c8-616be46650ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "class TrajectoryDataset_per_episode(Dataset): #this should have only 1 trajectory no matter what\n",
    "    def __init__(self, trajectories, context_len, rtg_scale,dev,gamma=0.99,min_len = 10**6):\n",
    "        self.trajectories = trajectories\n",
    "        self.context_len = context_len\n",
    "        self.dev = dev\n",
    "        min_len = min(min_len, len(self.trajectories[0]['observations'])) ##len(data[0]['observations'])<self.args.context_len\n",
    "\n",
    "\n",
    "        \n",
    "        #states = []\n",
    "        #for traj in self.trajectories:\n",
    "        #    traj_len = traj['observations'].shape[0]\n",
    "        #    min_len = min(min_len, traj_len)\n",
    "        #    states.append(traj['observations'])\n",
    "        #    # calculate returns to go and rescale them \n",
    "\n",
    "        self.pad_init()\n",
    "        \n",
    "        self.trajectories[0]['returns_to_go_cal'] = discount_cumsum(self.trajectories[0]['rewards'], gamma) / rtg_scale\n",
    "\n",
    "        self.trajectories[0]['current_troops_count'] = (self.trajectories[0]['current_troops_count'] - 5.2496)/1.4733\n",
    "        \n",
    "        #print(min_len)\n",
    "        \n",
    "        # used for input normalization\n",
    "        \n",
    "        #states = torch.concatenate(states, axis=0).to(dtype = torch.float32)\n",
    "        #self.state_mean, self.state_std = torch.mean(self.trajectories[0]['observations'], axis=0\n",
    "        #                                            ), torch.std(self.trajectories[0]['observations'], axis=0) + 1e-6\n",
    "\n",
    "        # normalize states\n",
    "        #for traj in self.trajectories:\n",
    "\n",
    "            #self.trajectories[0]['current_troops_count']\n",
    "            #traj['observations'] = (traj['observations'].to(dtype=torch.float32) - self.state_mean) / self.state_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def pad_init(self):\n",
    "\n",
    "\n",
    "\n",
    "        #observations : torch.Size([255, 40]) torch.Size([2442, 40]) \n",
    "        # returntogo : torch.Size([255, 1]) torch.Size([2442, 1]) \n",
    "        # returntogo_pred : torch.Size([255, 1]) torch.Size([2442, 1]) \n",
    "        # rewards : torch.Size([255]) torch.Size([2442]) \n",
    "        # terminals : torch.Size([255]) torch.Size([2442]) \n",
    "        # actions :\n",
    "        #torch.Size([255, 2]) torch.Size([2442, 2]) \n",
    "        # action_masks : torch.Size([255, 32]) torch.Size([2442, 32]) \n",
    "        # current_agent_simple : torch.Size([255, 1]) torch.Size([2442, 1]) \n",
    "        # current_agent : torch.Size([255, 2]) torch.Size([2442, 2]) \n",
    "        # current_phase : torch.Size([255, 2]) torch.Size([2442, 2]) \n",
    "        # current_troops_count : torch.Size([255]) torch.Size([2442])\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        self.trajectories[0]['observations'] =  torch.cat( (\n",
    "                                              self.trajectories[0]['observations'][0].repeat([self.context_len -1]+[1 for i in range(len(self.trajectories[0]['observations'].shape)-1) ]),\n",
    "                                              self.trajectories[0]['observations']\n",
    "                                              \n",
    "                                            ),dim=0)          \n",
    "        \n",
    "\n",
    "        self.trajectories[0]['returntogo'] =      torch.cat( (\n",
    "                                              self.trajectories[0]['returntogo'][[0]].repeat([self.context_len -1]+[1 for i in range(len(self.trajectories[0]['returntogo'].shape)-1) ]),\n",
    "                                              self.trajectories[0]['returntogo']\n",
    "                                              \n",
    "                                            ),dim=0)         # torch.tensor([1,2,3,4])\n",
    "        self.trajectories[0]['returntogo_pred'] =             torch.cat( (\n",
    "                                              self.trajectories[0]['returntogo_pred'][[0]].repeat([self.context_len -1]+[1 for i in range(len(self.trajectories[0]['returntogo_pred'].shape)-1) ]),\n",
    "                                              self.trajectories[0]['returntogo_pred']\n",
    "                                              \n",
    "                                            ),dim=0)   # torch.tensor([1,2,3,4])\n",
    "\n",
    "        self.trajectories[0]['rewards'] =    torch.cat( (\n",
    "                                              torch.zeros([self.context_len -1]).to(device=self.dev),\n",
    "                                              self.trajectories[0]['rewards']\n",
    "                                              \n",
    "                                            ),dim=0)  # torch.tensor([1,2,3,4])\n",
    "        self.trajectories[0]['terminals'] =    torch.cat( (\n",
    "                                              self.trajectories[0]['terminals'][[0]].repeat([self.context_len -1]+[1 for i in range(len(self.trajectories[0]['terminals'].shape)-1) ]),\n",
    "                                              self.trajectories[0]['terminals']\n",
    "                                              \n",
    "                                            ),dim=0)  # torch.tensor([1,2,3,4])\n",
    "\n",
    "        \n",
    "        self.trajectories[0]['actions_1'] =      torch.cat( (\n",
    "                                              torch.zeros([self.context_len -1]+list(self.trajectories[0]['actions_1'].shape[1:])).to(device=self.dev),\n",
    "                                              self.trajectories[0]['actions_1']\n",
    "                                              \n",
    "                                            ),dim=0)\n",
    "        \n",
    "        self.trajectories[0]['actions_2'] =      torch.cat( (\n",
    "                                              torch.zeros([self.context_len -1]+list(self.trajectories[0]['actions_2'].shape[1:])).to(device=self.dev),\n",
    "                                              self.trajectories[0]['actions_2']\n",
    "                                              \n",
    "                                            ),dim=0)\n",
    "\n",
    "        self.trajectories[0]['log_probs_actions_2'] = torch.cat( (\n",
    "                                              torch.zeros([self.context_len -1]+list(self.trajectories[0]['log_probs_actions_2'].shape[1:])).to(device=self.dev),\n",
    "                                              self.trajectories[0]['log_probs_actions_2']\n",
    "                                              \n",
    "                                            ),dim=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # torch.tensor([1,2,3,4])\n",
    "        self.trajectories[0]['action_masks'] =    torch.cat( (\n",
    "                                              torch.zeros([self.context_len -1]+list(self.trajectories[0]['action_masks'].shape[1:])).to(device=self.dev,dtype=torch.float32),\n",
    "                                              self.trajectories[0]['action_masks']\n",
    "                                              \n",
    "                                            ),dim=0)\n",
    "\n",
    "\n",
    "\n",
    "        self.trajectories[0]['current_agent_acting'] =     torch.cat( (\n",
    "                                                              torch.zeros([self.context_len -1]+list(self.trajectories[0]['current_agent_acting'].shape[1:])).to(device=self.dev,dtype=torch.float32),\n",
    "                                                              self.trajectories[0]['current_agent_acting']\n",
    "                                                              \n",
    "                                                            ),dim=0)\n",
    "        \n",
    "        self.trajectories[0]['current_agent_simple'] =     torch.cat( (\n",
    "                                                              torch.zeros([self.context_len -1]+list(self.trajectories[0]['current_agent_simple'].shape[1:])).to(device=self.dev,dtype=torch.float32),\n",
    "                                                              self.trajectories[0]['current_agent_simple']\n",
    "                                                              \n",
    "                                                            ),dim=0)\n",
    "\n",
    "        \n",
    "        self.trajectories[0]['current_agent'] =             torch.cat( (\n",
    "                                                              torch.zeros([self.context_len -1]+list(self.trajectories[0]['current_agent'].shape[1:])).to(device=self.dev,dtype=torch.float32),\n",
    "                                                              self.trajectories[0]['current_agent']\n",
    "                                                              \n",
    "                                                            ),dim=0)\n",
    "\n",
    "        \n",
    "        self.trajectories[0]['current_phase'] =             torch.cat( (\n",
    "                                                          self.trajectories[0]['current_phase'][0].repeat([self.context_len -1]+[1 for i in range(len(self.trajectories[0]['current_phase'].shape)-1) ]),\n",
    "                                                          self.trajectories[0]['current_phase']\n",
    "                                                          \n",
    "                                                        ),dim=0) \n",
    "        self.trajectories[0]['current_troops_count'] =      torch.cat( (\n",
    "                                              self.trajectories[0]['current_troops_count'][0].repeat([self.context_len -1]),\n",
    "                                              self.trajectories[0]['current_troops_count']\n",
    "                                              \n",
    "                                            ),dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        #print(len(self.trajectories),(self.trajectories[0].shape),len(self.trajectories[0])- self.context_len + 1 )\n",
    "        return sum(max(0, len(traj['observations'])- self.context_len + 1\n",
    "                      ) for traj in self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        total_len = 0\n",
    "        for traj in self.trajectories:\n",
    "            \n",
    "            #print(total_len, idx - total_len, total_len + len(traj), - self.context_len + 1)\n",
    "            \n",
    "            if total_len  <= idx < total_len + len(traj['observations']) - self.context_len + 1    :\n",
    "                si = idx - total_len\n",
    "                \n",
    "                #context = traj[si:si + self.context_length]\n",
    "                states = (traj['observations'][si : si + self.context_len])\n",
    "                \n",
    "                actions_1 = traj['actions_1'][si : si + self.context_len]#torch.cat((traj['actions_1'][si : si + self.context_len-1].clone().detach(),   traj['actions_1'][[si + self.context_len-1]]      ),dim =0)\n",
    "                actions_2 = traj['actions_2'][si : si + self.context_len]#torch.cat((traj['actions_2'][si : si + self.context_len-1].clone().detach(),   traj['actions_2'][[si + self.context_len-1]]      ),dim =0)\n",
    "\n",
    "                log_probs_actions_2 = traj['log_probs_actions_2'][si : si + self.context_len]\n",
    "                \n",
    "                reward =  (traj['rewards'][si : si + self.context_len])\n",
    "                returntogo = (traj['returntogo'][si : si + self.context_len])\n",
    "                returns_to_go_cal = (traj['returns_to_go_cal'][si : si + self.context_len])\n",
    "                returntogo_pred = torch.cat((traj['returntogo_pred'][si : si + self.context_len-1].clone().detach(),  traj['returntogo_pred'][[ si + self.context_len-1]]       ),dim =0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                action_masks = (traj['action_masks'][si : si + self.context_len])\n",
    "\n",
    "                current_agent_acting = (traj['current_agent_acting'][si : si + self.context_len])\n",
    "                current_agent = (traj['current_agent'][si : si + self.context_len])\n",
    "                current_agent_simple = (traj['current_agent_simple'][si : si + self.context_len])\n",
    "                current_phase = (traj['current_phase'][si : si + self.context_len])\n",
    "                current_troops_count = (traj['current_troops_count'][si : si + self.context_len])\n",
    "    \n",
    "                \n",
    "                timesteps = torch.arange(start=si, end=si+self.context_len, step=1)\n",
    "    \n",
    "                # all ones since no padding\n",
    "                traj_mask = torch.ones(self.context_len, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "                if self.context_len> len(states):\n",
    "                    padding_len = self.context_len - len(states)\n",
    "    \n",
    "                    states                = torch.cat([states,\n",
    "                                    torch.zeros(([padding_len] + list(states.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)  \n",
    "                    actions_1               = torch.cat([actions_1,\n",
    "                                    torch.zeros(([padding_len] + list(actions_1.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "                    \n",
    "                    actions_2               = torch.cat([actions_2,\n",
    "                                    torch.zeros(([padding_len] + list(actions_2.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "\n",
    "                    log_probs_actions_2 =  torch.cat([log_probs_actions_2,\n",
    "                                    torch.zeros(([padding_len] + list(log_probs_actions_2.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "                    \n",
    "                    reward                = torch.cat([reward,\n",
    "                                    torch.zeros(([padding_len] + list(reward.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)  \n",
    "\n",
    "\n",
    "                    returntogo            = torch.cat([returntogo,\n",
    "                                    torch.zeros(([padding_len] + list(returntogo.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "                    returns_to_go_cal     = torch.cat([returns_to_go_cal,\n",
    "                                    torch.zeros(([padding_len] + list(returns_to_go_cal.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "                    returntogo_pred       = torch.cat([returntogo_pred,\n",
    "                                    torch.zeros(([padding_len] + list(returntogo_pred.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                    action_masks          = torch.cat([action_masks,\n",
    "                                    torch.zeros(([padding_len] + list(action_masks.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)        \n",
    "\n",
    "                    current_agent_acting = torch.cat([current_agent_acting,\n",
    "                                    torch.zeros(([padding_len] + list(current_agent_acting.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "                    \n",
    "                    current_agent         = torch.cat([current_agent,\n",
    "                                    torch.zeros(([padding_len] + list(current_agent.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "                    \n",
    "                    current_troops_count = torch.cat([current_troops_count,\n",
    "                                        torch.zeros(([padding_len] + list(current_troops_count.shape[1:])),\n",
    "                                        dtype=current_troops_count.dtype\n",
    "                                                   )], \n",
    "                                       dim=0)\n",
    "                    current_phase         = torch.cat([current_phase,\n",
    "                                    torch.zeros(([padding_len] + list(current_phase.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)         \n",
    "                    current_troops_count  = torch.cat([current_troops_count,\n",
    "                                    torch.zeros(([padding_len] + list(current_troops_count.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)                \n",
    "                    traj_mask             = torch.cat([traj_mask,\n",
    "                                    torch.zeros(([padding_len] + list(traj_mask.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                return  timesteps, states, actions_1,actions_2,log_probs_actions_2, returntogo, returns_to_go_cal, returntogo_pred,reward, traj_mask ,action_masks,current_agent_acting,current_agent_simple,current_agent,current_phase,current_troops_count\n",
    "                \n",
    "                #return pad(torch.tensor(context), (0,(self.context_length - len(context))),mode=\"constant\"), [1]\n",
    "\n",
    "            total_len += len(traj) - self.context_len + 1\n",
    "\n",
    "        raise IndexError(\"Index out of range 1\")\n",
    "\n",
    "\n",
    "class TrajectoryDataset_2_through_episodes(Dataset):\n",
    "    def __init__(self, trajectories):\n",
    "        self.trajectories = trajectories\n",
    "\n",
    "        #all_obs = torch.concat([ traj.dataset.trajectories[0]['observations'] for traj in self.trajectories],axis=0)\n",
    "        #self.state_mean = torch.mean(all_obs,axis =0)\n",
    "        #self.state_std = torch.std(all_obs,axis =0) + 1e-6\n",
    "        \n",
    "        #for traj in self.trajectories:\n",
    "        #    traj.dataset.trajectories[0]['observations'] = (traj.dataset.trajectories[0]['observations'].to(dtype=torch.float32) - self.state_mean) / self.state_std\n",
    "\n",
    "\n",
    "        #print(self.state_mean,self.state_std)\n",
    "        \n",
    "    #def get_state_stats(self):\n",
    "        #return self.state_mean, self.state_std        \n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        total_len = 0\n",
    "        if total_len  <= idx < total_len + len(self.trajectories)  :\n",
    "            return [batch for batch in self.trajectories[idx] ][0]\n",
    "\n",
    "\n",
    "        raise IndexError(\"Index out of range 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff383a25-6ff6-4efd-805e-68181b016dd8",
   "metadata": {},
   "source": [
    "# trainer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "442272e1-3d31-49aa-9a78-7758b6dd1348",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ## Trainer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Args,param_dict =dict({})\n",
    "                 ):\n",
    "        # #### Configurations\n",
    "\n",
    "        self.args = Args()#tyro.cli(Args)\n",
    "        self.param_dict = param_dict\n",
    "        self.update_arg(param_dict=param_dict)\n",
    "        self.device = self.args.device#torch.device(\"cuda\" if torch.cuda.is_available() and self.args.cuda else \"cpu\")\n",
    "        \n",
    "        \n",
    "        #self.args.batch_size = int(self.args.num_envs * self.args.num_steps)\n",
    "        self.args.minibatch_size = int(self.args.batch_size // self.args.num_minibatches)\n",
    "        #self.args.num_iterations = self.args.total_timesteps // self.args.batch_size\n",
    "        self.gam = self.args.gamma\n",
    "        #self.args.minibatch_size = 256#128 \n",
    "        self.num_steps = self.args.num_steps#120000#1000000\n",
    "        self.num_iterations = self.args.num_iterations\n",
    "        self.episode_time_lim = self.args.episode_time_lim\n",
    "        self.hero_agent_count = self.args.hero_agent_count\n",
    "        self.env_config = self.args.env_config\n",
    "        self.num_episodes = self.args.num_episodes\n",
    "        self.context_len=self.args.model_config['context_len'] #200\n",
    "\n",
    "        self.training_performance_return = []\n",
    "        \n",
    "        #self.env_config = dict(render_mode = 'rgb_array', default_attack_all  = True,\n",
    "        #                    agent_count  = 4\n",
    "        #                       ,use_placement_perc=True,render_=False)        \n",
    "        \n",
    "        self.run_name = f\"{self.args.env_id}__{self.args.exp_name}__{self.args.seed}__{int(time.time())}\"\n",
    "\n",
    "        \n",
    "\n",
    "        TB_log = self.args.TB_log \n",
    "        if TB_log:    \n",
    "            self.writer = SummaryWriter(f\"runs/{self.run_name}\")\n",
    "            self.writer.add_text(\n",
    "                \"hyperparameters\",\n",
    "                \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(self.args).items()])),\n",
    "            )\n",
    "        else:\n",
    "            self.writer = None\n",
    "        \n",
    "        # TRY NOT TO MODIFY: seeding\n",
    "        random.seed(self.args.seed)\n",
    "        np.random.seed(self.args.seed)\n",
    "        #torch.manual_seed(self.args.seed)\n",
    "        \n",
    "        torch.backends.cudnn.deterministic = self.args.torch_deterministic\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.playe_r = 1#\"agent_1\" #\n",
    "        \n",
    "\n",
    "        \n",
    "        self.action_shape = (2,)\n",
    "\n",
    "\n",
    "\n",
    "        self.env = env_risk(**self.env_config)\n",
    "        \n",
    "        self.env.reset(seed=42)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        self.args.total_agents = self.total_agents  = len(self.env.possible_agents)\n",
    "        self.args.total_phases = self.total_phases = len(self.env.phases)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        print(torch.tensor(self.env.last()[0]['observation']\n",
    "                          ))\n",
    "\n",
    "        print(torch.tensor(self.env.last()[0]['observation']\n",
    "                          ).to(device=self.device))\n",
    "        sample_obs = self.obs_converter(torch.tensor(self.env.last()[0]['observation']\n",
    "                                                    ).to(device=self.device),\n",
    "                                        num_classes = self.total_agents+1\n",
    "                                       )\n",
    "        \n",
    "        self.ob_space_shape = sample_obs.shape #env.observation_space(playe_r)['observation'].shape\n",
    "        self.action_mask_shape = self.env.observation_space(self.playe_r)['action_mask'].shape\n",
    "        \n",
    "        #self.device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "        \n",
    "        \n",
    "        self.agent_list = list(self.env.possible_agents)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        self.the_hero_agent = 1\n",
    "\n",
    "        \n",
    "        self.qnet_config_dict = dict(action_space = self.env.action_space(self.playe_r\n",
    "                                                                         ).shape[0],\n",
    "                                    ob_space=(np.prod(self.ob_space_shape\n",
    "                                                    )+np.prod(self.action_mask_shape)\n",
    "                                         +1*( self.total_agents -1) #the current_agent +1#who actor agent was\n",
    "                                         +1*(self.total_phases -1)#the current phase\n",
    "                                         +1 )# the number of troops\n",
    "                               )\n",
    "        self.actor_config_dict =  dict(env=self.env,\n",
    "                        action_space = self.env.observation_space(self.playe_r)['action_mask'].shape[0],\n",
    "                        ob_space=(np.prod(self.ob_space_shape)\n",
    "                                         +np.prod(self.action_mask_shape)\n",
    "                                         +1*( self.total_agents-1) #the current_agent +1#who actor agent was\n",
    "                                         +1*(self.total_phases -1)#the current phase\n",
    "                                         +1) # the number of troops\n",
    "                               )\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #torch.Tensor(torch.hstack((observation['observation'].reshape(-1),\n",
    "        #                            torch.tensor(observation['action_mask'].reshape(-1)).to(self.device),\n",
    "        #                                   phase_mapping,\n",
    "        #                                    curr_agent_mapping,\n",
    "        #                                   torch.tensor([env.board.agents[curr_agent].bucket ]).to(self.device)))[None,:]#.repeat(3,axis = 0)\n",
    "        #                                        ).float()\n",
    "                        \n",
    "        \n",
    "        self.hero_agents_list = {i:Hero_agent(i) for i in range(1,self.hero_agent_count+1) } # this is a list , need to pass it as an argument\n",
    "        \n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].init_properties(self.total_agents,self.env.phases)        \n",
    "\n",
    "\n",
    "            print(len(self.args.model_name[i]))\n",
    "            \n",
    "            self.hero_agents_list[i].init_model(model_name=self.args.model_name[i], kwarg = dict(\n",
    "\n",
    "                                                qnet_config_dict = self.qnet_config_dict, \n",
    "                                                actor_config_dict = self.actor_config_dict,\n",
    "                                                args = self.args,\n",
    "                                                device = self.device,\n",
    "                                                writer=self.writer,\n",
    "                                                run_name=self.run_name,\n",
    "                                                agent=i)\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            #self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "            #self.qf1_target.load_state_dict(self.qf1.state_dict())\n",
    "            #self.q_optimizer = optim.Adam(list(self.qf1.parameters()), lr=self.args.learning_rate)\n",
    "            #self.actor_optimizer = optim.Adam(list(self.actor.parameters()), lr=self.args.learning_rate)\n",
    "\n",
    "    def update_arg(self,param_dict=dict({})):\n",
    "       for i,j in param_dict.items():\n",
    "           setattr(self.args,i,j)\n",
    "\n",
    "        \n",
    "\n",
    "    def obs_converter(self,  data, num_classes = 4, col =0 ):\n",
    "\n",
    "        if col != None:\n",
    "\n",
    "            #print(data.device)\n",
    "            #print(nn.functional.one_hot(data[:4,col].detach().long(), \n",
    "            #                                            num_classes = num_classes).to(self.device))\n",
    "            return torch.concat((nn.functional.one_hot(data[:,col].detach().long(), \n",
    "                                                        num_classes = num_classes).to(self.device),\n",
    "                                      data[:,~col,None]\n",
    "                                ),axis=1\n",
    "                               )[:,1:].to(self.device)\n",
    "    \n",
    "    def map_agent_phase_hot(self, data,num_classes = 3):\n",
    "        with torch.no_grad():\n",
    "            return nn.functional.one_hot(torch.tensor(data),num_classes = num_classes)[1:].to(self.device)\n",
    "    \n",
    "    def map_agent_phase_vector(self, data,num_classes = 3):\n",
    "        with torch.no_grad():\n",
    "            return nn.functional.one_hot(data[:,0].long(), \n",
    "                                                            num_classes = num_classes)[:,1:].to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def train_loop_init(self):\n",
    "        self.gamma_t = {i:0 for i in self.env.possible_agents}\n",
    "        \n",
    "        \n",
    "        self.draw_count = 0\n",
    "\n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].init_win_count_iter(self.total_agents )\n",
    "            self.hero_agents_list[i].model.init_path()\n",
    "        \n",
    "         \n",
    "        #self.first_count = 0\n",
    "        #self.second_count = 0\n",
    "        #self.third_count = 0\n",
    "        #self.third_count_draw = 0\n",
    "        \n",
    "        self.start_time = time.time()\n",
    "        self.global_step = 0\n",
    "        #self.faulting_player = \"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    def run_training_loop(self):\n",
    "        \"\"\"\n",
    "        ### Run training loop\n",
    "        \"\"\"\n",
    "\n",
    "        # last 100 episode information\n",
    "        #tracker.set_queue('reward', 100, True)\n",
    "        #tracker.set_queue('length', 100, True)\n",
    "\n",
    "\n",
    "        for i in self.hero_agents_list: # each agent has his own buffer, this is kinda pain because now this information is stored and not discarded\n",
    "    \n",
    "            self.hero_agents_list[i].rb = ReplayBuffer(\n",
    "                    self.args.buffer_size,\n",
    "                    Box(low =0, high=2000, shape =(self.qnet_config_dict['ob_space']+1,), dtype=np.float32),\n",
    "                    Box(low =0, high=2000, shape =(2,), dtype=np.float32),\n",
    "                    self.device,\n",
    "                    handle_timeout_termination=False,\n",
    "                )\n",
    "\n",
    "        env = env_risk(**(self.env_config #| {\"render_mode\" : None, \"bad_mov_penalization\" : 0.01,\"render_\":False}\n",
    "                         ))\n",
    "        env.reset(42)\n",
    "        \n",
    "        self.train_loop_init()\n",
    "        #self.paths=[]\n",
    "        \n",
    "        self.training_performance_return = []\n",
    "        \n",
    "        for iteration in range(1, self.num_iterations+1):\n",
    "\n",
    "\n",
    "            #1st you sample data\n",
    "            self.sample(\n",
    "                                env,iteration,\n",
    "                                \n",
    "                                \n",
    "                 \n",
    "                            )\n",
    "\n",
    "            \n",
    "            #2nd you create a new dataset\n",
    "            for i in self.hero_agents_list:\n",
    "                    self.hero_agents_list[i].model.create_training_dataset()\n",
    "\n",
    "            \n",
    "            #3rd you train the model\n",
    "            self.train(iteration)\n",
    "            #break\n",
    "\n",
    "            \n",
    "            \n",
    "            #if self.global_step%100 ==0:\n",
    "            #    SPS = int(self.global_step / (time.time() - self.start_time))\n",
    "            #    print(\"SPS:\", SPS)       \n",
    "            #    self.writer.add_scalar(\"charts/SPS\", SPS, self.global_step)\n",
    "        \n",
    "            \n",
    "            self.save_models()\n",
    "\n",
    "    def train(self,iteration):\n",
    "\n",
    "        if True:#self.global_step > self.args.learning_starts:\n",
    "\n",
    "            for i in self.hero_agents_list:\n",
    "                self.hero_agents_list[i].model.train_write(\n",
    "                        iteration,print_=True)\n",
    "                \n",
    "                        \n",
    "                    \n",
    "    \n",
    "    def train_(self,iteration):\n",
    "        \n",
    "        for epoch in range(self.args.update_epochs):\n",
    "            \n",
    "            if self.global_step > self.args.learning_starts:\n",
    "\n",
    "                for i in self.hero_agents_list:\n",
    "                    self.hero_agents_list[i].model.train_write(self.hero_agents_list[i].rb.sample(self.args.batch_size)\n",
    "                                                         ,iteration,epoch)\n",
    "                    \n",
    "    def save_models(self):\n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].save_models()  \n",
    "\n",
    "    def reset_moves_hero_agents(self):\n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].init_move_count_epi(self.env.phases)\n",
    "\n",
    "\n",
    "    def set_episode_recorders(self):\n",
    "        obs = torch.zeros((self.num_steps,) + self.ob_space_shape, requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "        \n",
    "        actions_1 = torch.zeros((self.num_steps,) ).to(self.device,    dtype = torch.float32)\n",
    "        \n",
    "        actions_2 = torch.zeros( (self.num_steps,)).to(self.device,    dtype = torch.float32)\n",
    "        log_probs_actions_2 = torch.zeros( (self.num_steps,)).to(self.device,    dtype = torch.float32)\n",
    "        \n",
    "        action_masks = torch.zeros((self.num_steps, ) + self.action_mask_shape, requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "        current_agent = torch.zeros((self.num_steps,1), requires_grad =False).to(self.device,    dtype = torch.float32)*0#-1\n",
    "        current_agent_acting = torch.ones((self.num_steps,1), requires_grad =False).to(self.device,    dtype = torch.float32)*0\n",
    "        current_phase = torch.zeros((self.num_steps,1), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "        current_troops_count = torch.zeros((self.num_steps,self.total_agents), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "        #logprobs = torch.zeros((self.num_steps, ), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "        rewards = torch.zeros((self.num_steps, self.total_agents), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "        rewards_2 = torch.zeros((self.num_steps, self.total_agents), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "        \n",
    "        returntogo = torch.zeros((self.num_steps, self.total_agents), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "        dones = torch.zeros((self.num_steps, self.total_agents), requires_grad =False).to(self.device)\n",
    "        dones_2 = torch.zeros((self.num_steps, self.total_agents), requires_grad =False).to(self.device)\n",
    "        #values = torch.zeros((self.num_steps,  )).to(self.device)\n",
    "        episodes = torch.ones((self.num_steps, ), requires_grad =False).to(self.device,    dtype = torch.float32)*-1\n",
    "        t_next = torch.zeros((self.num_steps, self.total_agents), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "            \n",
    "        total_rewards = {i:0 for i in env.possible_agents} #i can report this\n",
    "\n",
    "        return (\n",
    "                obs\n",
    "                ,actions_1\n",
    "                ,actions_2\n",
    "                ,log_probs_actions_2\n",
    "                ,action_masks\n",
    "                ,current_agent\n",
    "                ,current_agent_acting\n",
    "                ,current_phase\n",
    "                ,current_troops_count\n",
    "                ,rewards\n",
    "                ,rewards_2\n",
    "                ,returntogo\n",
    "                ,dones\n",
    "                ,dones_2\n",
    "                ,episodes\n",
    "                ,t_next\n",
    "                ,total_rewards\n",
    "                )\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    def sample(self,env,iteration\n",
    "                                ):\n",
    "        \n",
    "\n",
    "        #for i in self.hero_agents_list:\n",
    "        #    self.hero_agents_list[i].model.init_path()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        #if True:\n",
    "            # sample `worker_steps` from each worker\n",
    "            #there are no worker steps... rather there are full episodes\n",
    "\n",
    "            step = 0\n",
    "            fault_condition = False\n",
    "            clear_output(wait=True)\n",
    "            phase = 0\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            for episode in range(self.num_episodes):#num_episodes):\n",
    "                #num_steps is the maximum number of steps before the game is stopped\n",
    "\n",
    "\n",
    "                (\n",
    "                obs\n",
    "                ,actions_1\n",
    "                ,actions_2\n",
    "                ,log_probs_actions_2\n",
    "                ,action_masks\n",
    "                ,current_agent\n",
    "                ,current_agent_acting\n",
    "                ,current_phase\n",
    "                ,current_troops_count\n",
    "                ,rewards\n",
    "                ,rewards_2\n",
    "                ,returntogo\n",
    "                ,dones\n",
    "                ,dones_2\n",
    "                ,episodes\n",
    "                ,t_next\n",
    "                ,total_rewards\n",
    "                ) = self.set_episode_recorders()\n",
    "                \n",
    "                #trace = tensor.zeros((self.context_len,self.qnet_config_dict['ob_space']))\n",
    "                action=1\n",
    "                #return2g = 110\n",
    "                \n",
    "                \n",
    "                #init model sample store : but do i need it now?\n",
    "                for i in self.hero_agents_list:\n",
    "                    self.hero_agents_list[i].model.init_CL_sample_store()\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "                if fault_condition:\n",
    "                    env = env_risk(**(self.env_config  #| {\"render_mode\" : None,\"bad_mov_penalization\" : 0.01,\"render_\":False#False\n",
    "                                                        # }\n",
    "                                     )\n",
    "                                      )#game.env(render_mode=None)\n",
    "\n",
    "                curren_epi = episode + (iteration-1)*self.num_episodes\n",
    "                env.reset(curren_epi) #for riplication\n",
    "                fault_condition = False\n",
    "                step_count = 0\n",
    "                \n",
    "                self.reset_moves_hero_agents()\n",
    "                is_draw = 0\n",
    "                \n",
    "                #draw_territory_count = 0\n",
    "                #is_third = 0\n",
    "\n",
    "                for agent in env.agent_iter(): # episode loop\n",
    "                    e_t = env.terminations\n",
    "                    if sum(e_t.values()) <(self.total_agents-1):\n",
    "                        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "                        observation['observation'] =  self.obs_converter(\n",
    "                                                        torch.tensor(\n",
    "                                                            observation['observation']\n",
    "                                                        ).to(self.device,dtype=torch.float32),\n",
    "                                                        num_classes = self.total_agents+1)\n",
    "                        \n",
    "                        observation['observation'][:,-1]  =  (observation['observation'][:,-1] - 5.2496)/1.4733\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        episodes[step_count] = curren_epi\n",
    "                        obs[step_count] = observation['observation'] #torch.Tensor(observation['observation']).to(self.device) #sould i not add it .... meaning this is the last observation after the player dies\n",
    "                        action_masks[step_count] = torch.Tensor(observation['action_mask']).to(self.device)\n",
    "                        \n",
    "                        #curr_agent = agent#int(agent[-1])\n",
    "                        current_agent[step_count] = curr_agent = agent\n",
    "                        current_phase[step_count] = phase = env.phase_selection\n",
    "                        phase_mapping = self.map_agent_phase_hot(phase,num_classes = self.total_phases).float()\n",
    "                        \n",
    "                        curr_agent_mapping = self.map_agent_phase_hot(int(curr_agent)-1,\n",
    "                                                                      num_classes = self.total_agents \n",
    "                                                                     ).float()\n",
    "                        \n",
    "                        current_troops_count[step_count] = torch.tensor([env.board.agents[i].bucket for i in env.possible_agents],requires_grad =False).to(self.device)\n",
    "                    \n",
    "\n",
    "                        \n",
    "\n",
    "                        for i in self.hero_agents_list:\n",
    "                            self.hero_agents_list[i].model.current_model_in(observation,curr_agent,\n",
    "                                                                            phase_mapping,curr_agent_mapping,\n",
    "                                                                            env_board_agents=env.board.agents)\n",
    "                            self.hero_agents_list[i].model.update_CL_sample_store(curr_agent_=i,\n",
    "                                                                                  inp = {'step':step_count,\n",
    "                                                                                         'act_2_1':[] ,\n",
    "                                                                                         'act_2_2':[] ,\n",
    "                                                                                         'curr_reward_list':[]\n",
    "                                          },before_action=1)\n",
    "                            \n",
    "                        \n",
    "                        action_taken = False\n",
    "                        log_prob_a2 = None  \n",
    "                        if True:\n",
    "                            \n",
    "                            mask = observation[\"action_mask\"]\n",
    "                            if (self.global_step < self.args.learning_starts) or (\n",
    "                                np.random.rand() > min(\n",
    "                                                ((curren_epi)/((self.num_iterations*self.num_episodes)/10))\n",
    "                                                , 0.95)\n",
    "                                                #) or (agent != self.the_hero_agent) \n",
    "                                                ) or ( agent not in self.hero_agents_list):\n",
    "        \n",
    "                                \n",
    "                                action = env.action_space(agent).sample()\n",
    "                                \n",
    "                                #part_0 =np.random.choice(np.where(env.board.calculated_action_mask(agent))[0])\n",
    "                                part_0 =np.random.choice(np.where(observation['action_mask'])[0])\n",
    "                                action = torch.tensor([\n",
    "                                                        [\n",
    "                                                         [part_0],\n",
    "                                                         [np.around(action[1],2)]\n",
    "                                                        ]\n",
    "                                                        ],requires_grad =False).to(self.device)\n",
    "                                \n",
    "                                action = action[:,:,0]\n",
    "                                action_1 = action[:,0]\n",
    "                                action_2 = action[:,1]\n",
    "                                log_prob_a2 = torch.tensor([  1/(sum(mask)+0.0001) ],requires_grad =False).to(self.device)\n",
    "                                for i in self.hero_agents_list:\n",
    "                                    \n",
    "                                    self.hero_agents_list[i].model.update_CL_sample_store(curr_agent_=curr_agent,\n",
    "                                                                                          inp = {'step':step_count,\n",
    "                                                                                                 'act_2_1':action_1 ,\n",
    "                                                                                                 'act_2_2':[] ,\n",
    "                                                                                                 'curr_reward_list':[]\n",
    "                                                  },before_action=2)\n",
    "                                    \n",
    "                                    self.hero_agents_list[i].model.update_CL_sample_store(curr_agent_=curr_agent,\n",
    "                                                                                          inp = {'step':step_count,\n",
    "                                                                                                 'act_2_1':[] ,\n",
    "                                                                                                 'act_2_2':action_2 ,\n",
    "                                                                                                 'curr_reward_list':[]\n",
    "                                                  },before_action=3)\n",
    "\n",
    "                                \n",
    "\n",
    "\n",
    "                            \n",
    "\n",
    "                                                              \n",
    "                            else:\n",
    "                                action_taken = True\n",
    "                                #action_masks[step]\n",
    "\n",
    "                                #need to update this\n",
    "                                action_1,_,action_taken,_ = self.hero_agents_list[curr_agent].action_predict(save_R=False,action_masks=action_masks[step_count])\n",
    "                                #action_1 = action_1[None,:]\n",
    "                                #so the action_1 should be updated here... and prediction of action_2 would be re-done\n",
    "\n",
    "\n",
    "                                \n",
    "                                \n",
    "                                if action_taken:\n",
    "                                    current_agent_acting[step_count] = curr_agent\n",
    "\n",
    "                                for i in self.hero_agents_list:\n",
    "                                    \n",
    "                                    self.hero_agents_list[i].model.update_CL_sample_store(curr_agent_=curr_agent,\n",
    "                                                                                          inp = {'step':step_count,\n",
    "                                                                                                 'act_2_1':action_1 ,\n",
    "                                                                                                 #'act_2_1':action_1[0] ,\n",
    "                                                                                                 'act_2_2':[] ,\n",
    "                                                                                                 'curr_reward_list':[]\n",
    "                                                  },before_action=2)\n",
    "\n",
    "                                _,action_2,_,log_prob_a2 = self.hero_agents_list[curr_agent].action_predict(save_R=False,action_masks=action_masks[step_count],return_log_prob_a2 = True)\n",
    "                               # action_2 = action_2[None,:]\n",
    "\n",
    "\n",
    "                                for i in self.hero_agents_list:\n",
    "                                    \n",
    "                                    self.hero_agents_list[i].model.update_CL_sample_store(curr_agent_=curr_agent,\n",
    "                                                                                          inp = {'step':step_count,\n",
    "                                                                                                 'act_2_1':[] ,\n",
    "                                                                                                 #'act_2_2':action_2[0] ,\n",
    "                                                                                                 'act_2_2':action_2 ,\n",
    "                                                                                                 'curr_reward_list':[]\n",
    "                                                  },before_action=3)\n",
    "                                    \n",
    "\n",
    "\n",
    "\n",
    "                            \n",
    "                                #print(action,action.requires_grad)\n",
    "                                #if len(action.shape)<2:\n",
    "                                    #print(episode,\"---2--\",action, action.shape)\n",
    "                                    #a()\n",
    "                                    #break\n",
    "                                \n",
    "                            \n",
    "                                #action = self.actor(torch.Tensor(model_in).to(self.device))\n",
    "\n",
    "                            #this is only to predict and save the return to go for all the transformer agents, will optimize later\n",
    "                            _ =  [ self.hero_agents_list[i].action_predict(save_R=True, action_masks=action_masks[step_count] # it only matters for the correct agent ... we are only saving the Q\n",
    "\n",
    "                                                                           \n",
    "                                                                          \n",
    "                                                                          ) for i in self.hero_agents_list]\n",
    "\n",
    "                            \n",
    "                            actions_1[step_count] = action_1\n",
    "                            actions_2[step_count] = action_2\n",
    "                            if log_prob_a2 != None:\n",
    "                                log_probs_actions_2[step_count] = log_prob_a2\n",
    "                            curr_agent_ = int(curr_agent)\n",
    "                            \n",
    "                            \n",
    "        \n",
    "                            if not observation['action_mask'][action_1.long()]: \n",
    "                                fault_condition =True\n",
    "                                \n",
    "                                #self.faulting_player = agent\n",
    "\n",
    "                                \n",
    "\n",
    "\n",
    "                                if  curr_agent_ in self.hero_agents_list:\n",
    "                                    self.hero_agents_list[curr_agent_].bad_move_count+=1\n",
    "                                    self.hero_agents_list[curr_agent_].bad_move_phase_count[int(current_phase[step_count][0])]+=1  # when is the where_is_it_performing_bad_really\n",
    "                                    #print('here',agent, action, observation['action_mask'])\n",
    "                            \n",
    "        \n",
    "                            if  curr_agent_ in self.hero_agents_list:\n",
    "                                self.hero_agents_list[curr_agent_].move_count[int(current_phase[step_count][0])]+=1  \n",
    "                            #if self.the_hero_agent == curr_agent:\n",
    "                                #move_count[int(current_phase[step][0])]+=1        \n",
    "        \n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            \n",
    "                        #print('here',agent, action)\n",
    "                        if action_1 != None :\n",
    "                            act_2_1 = action_1[0]\n",
    "                            act_2_2 = action_2[0]\n",
    "                            \n",
    "                            #act_2 = action.detach().cpu().numpy()[0]#list([action.detach().cpu().numpy()[0][0], max(action.detach().cpu().numpy()[0][1],0.1) ])\n",
    "                            #act_2 = torch.tensor([act_2[0], max(act_2[1],0.001) ]).to(device = self.device)\n",
    "                            #print(max(act_2_2.clone().detach().cpu().item(),0.001))\n",
    "                            env.step([act_2_1.clone().detach().cpu().item(), max(act_2_2.clone().detach().cpu().item(),0.001) ])  \n",
    "                            try:\n",
    "                                _ =1\n",
    "                                #env.step([act_2_1.clone().detach().cpu().item(), max(act_2_2.clone().detach().cpu().item(),0.001) ])    \n",
    "                            except Exception as e:\n",
    "                                print(\"action_taken\",action_taken)\n",
    "                                print(e)\n",
    "                                \n",
    "                                print([act_2_1.clone().detach().cpu(), max(act_2_2.clone().detach().cpu(),0.001) ])\n",
    "                                \n",
    "                            \n",
    "                        #env.step(act_2 if action != None else None)        \n",
    "        \n",
    "        \n",
    "                        if True:\n",
    "        \n",
    "                            \n",
    "                            curr_reward_list =  env.curr_rewards\n",
    "                            \n",
    "                            if (step_count == (self.episode_time_lim-1))   or (self.global_step == (self.num_steps-1)): # draw reward\n",
    "                                is_draw=1\n",
    "                                curr_reward_list = {i:-100 for i in env.possible_agents }\n",
    "\n",
    "                            \n",
    "\n",
    "                            #if self.hero == curr_agent_:\n",
    "                            #    DT_input['action'][-1]  =act_2\n",
    "                            #DT_input['return_to_go'][-1] -=  curr_reward_list[self.hero]\n",
    "                            #returntogo[step] = DT_input['return_to_go'][-1]\n",
    "\n",
    "                            for i in self.hero_agents_list:\n",
    "                                self.hero_agents_list[i].model.update_CL_sample_store(curr_agent_=curr_agent,\n",
    "                                              inp = {'step':step_count,\n",
    "                                                     'act_2_1':None ,\n",
    "                                                     'act_2_2':None ,\n",
    "                                                     'curr_reward_list':curr_reward_list[i]\n",
    "                                              },before_action=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            \n",
    "                            rewards_2[step_count] = torch.tensor([curr_reward_list[i] for i in env.possible_agents]).to(self.device,dtype=torch.float32)\n",
    "                            if step >1:\n",
    "                                dones_2[step_count] = torch.tensor([ int(env.terminations[i]) - dones_2[step_count-1,i-1]  for i in env.possible_agents]).to(self.device)\n",
    "                            else:\n",
    "                                dones_2[step_count] = torch.tensor([env.terminations[i] for i in env.possible_agents]).to(self.device)\n",
    "\n",
    "\n",
    "                        #list_curr_reward_list = np.array(list(curr_reward_list.values()))\n",
    "                        \n",
    "                        #if sum(curr_reward_list.values()) == -300:\n",
    "                            #print('here')\n",
    "                            #is_draw=1\n",
    "        \n",
    "                        \n",
    "                        for age_i in env.possible_agents:\n",
    "                            \n",
    "                            total_rewards[age_i]+=curr_reward_list[age_i] #env.curr_rewards[age_i] if (step_count != episode_time_lim) else -100\n",
    "                                    \n",
    "                        \n",
    "                        step +=1\n",
    "                        self.global_step+=1\n",
    "        \n",
    "                    else:\n",
    "                        self.training_performance_return.append(total_rewards[1])\n",
    "                        print('done:',env.terminations,#env.terminations.values(),\n",
    "                              \",total_reward:\",total_rewards, ',iteration:',iteration,\",episode:\", episode )\n",
    "                        print(env.board.territories)\n",
    "                        break    \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "                    step_count+=1\n",
    "                    \n",
    "                    if (self.global_step == self.num_steps) :# or (fault_condition and (fa ulting_player != agent) and (len(env.agents)==0)):\n",
    "                        \n",
    "                        print('global_break_1')\n",
    "                        self.training_performance_return.append(total_rewards[1])\n",
    "                        print('done:',env.terminations,#env.terminations.values(),\n",
    "                              \",total_reward:\",total_rewards, ',iteration:',iteration,\",episode:\", episode )\n",
    "                        print(env.board.territories)\n",
    "\n",
    "                        #have to get out of the outer loop\n",
    "                        break\n",
    "                    elif (step_count == self.episode_time_lim):\n",
    "                        print('episode_break_1')\n",
    "                        self.training_performance_return.append(total_rewards[1])\n",
    "                        print('done:',env.terminations,#env.terminations.values(),\n",
    "                              \",total_reward:\",total_rewards, ',iteration:',iteration,\",episode:\", episode )\n",
    "                        print(env.board.territories)\n",
    "                        break\n",
    "\n",
    "                #self.obs=obs\n",
    "                \n",
    "                #print_here\n",
    "                print(episode,step_count,iteration)\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                #print(rewards[step-2])\n",
    "                if self.global_step == self.num_steps:\n",
    "                    print('global_break_2')\n",
    "                    break \n",
    "\n",
    "                for i in self.hero_agents_list:\n",
    "                    self.hero_agents_list[i].position =self.total_agents\n",
    "                    \n",
    "                    \n",
    "                #[ position = 3 for i in ] \n",
    "                for k_,(i_,j_) in enumerate(sorted([(j_,i_) for i_,j_ in total_rewards.items()],reverse=True) \n",
    "                      ):\n",
    "                    if int(j_) in self.hero_agents_list:\n",
    "                        self.hero_agents_list[int(j_)].position = k_+1\n",
    "                        print(j_,self.hero_agents_list[int(j_)].position)\n",
    "                        \n",
    "                        \n",
    "                    #if j_==self.the_hero_agent:\n",
    "                    #    position = k_+1\n",
    "\n",
    "                cur_epi_list = (episodes == curren_epi)\n",
    "                        \n",
    "                if self.args.TB_log:\n",
    "                    self.write_exploring(is_draw,#position,\n",
    "                            curren_epi,step,\n",
    "                            step_count,\n",
    "                            total_rewards,#bad_move_count\n",
    "                            #,bad_move_phase_count,\n",
    "                            #move_count,\n",
    "                            observation,\n",
    "                            env,\n",
    "                            cur_epi_list,\n",
    "                            current_agent_acting)\n",
    "\n",
    "                #paths = []\n",
    "\n",
    "                for i in self.hero_agents_list:\n",
    "                    self.hero_agents_list[i].model.update_train_data(\n",
    "                         step_count,\n",
    "                         obs,\n",
    "                            self.ob_space_shape,\n",
    "                            rewards_2[:,i-1],\n",
    "                            dones_2[:,i-1],\n",
    "                            actions_1[cur_epi_list],actions_2,log_probs_actions_2,\n",
    "                            action_masks,\n",
    "                            current_agent,\n",
    "                            current_agent_acting,\n",
    "                            current_phase,\n",
    "                            current_troops_count[:,i-1],\n",
    "                            map_agent_phase_vector = self.map_agent_phase_vector,\n",
    "                            \n",
    "                         )\n",
    "\n",
    "\n",
    "\n",
    "        #this  ---------------\n",
    "        #avg_episode_length = torch.mean(torch.tensor(\n",
    "        #                    [(episodes[:step] == i_epi).sum() for i_epi in episodes[:step].unique()]).float())#np.mean([(episodes[:step] == i_epi).sum() for i_epi in episodes[:step].unique()])\n",
    "        #if self.args.TB_log:\n",
    "        #    self.writer.add_scalar(\"charts/avg_episodic_length\", avg_episode_length, self.global_step)\n",
    "\n",
    "\n",
    "\n",
    "        #return paths\n",
    "        #return rb    \n",
    "        \n",
    "\n",
    "    def write_exploring(self,is_draw,#position,\n",
    "                        curren_epi,step,\n",
    "                        step_count,\n",
    "                        total_rewards,#bad_move_count\n",
    "                        #,bad_move_phase_count,\n",
    "                        #move_count,\n",
    "                        observation,\n",
    "                        env,\n",
    "                        cur_epi_list,\n",
    "                        current_agent_acting):\n",
    "\n",
    "        if is_draw:\n",
    "            self.draw_count +=1\n",
    "\n",
    "            for i in self.hero_agents_list:\n",
    "\n",
    "                self.writer.add_scalar(f\"draw_charts_agent_{i}/position_draw\",self.hero_agents_list[i].position\n",
    "                                                                                            ,self.draw_count) #draw_count is the number of draw episodes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                self.hero_agents_list[i].draw_territory_count = int(observation['observation'][:,i].sum()) #this is the total number of states\n",
    "                \n",
    "                self.hero_agents_list[i].count_draw_dict[\n",
    "                                                        self.hero_agents_list[i].position\n",
    "                                                        ] +=1\n",
    "                self.writer.add_scalar(f\"draw_charts_agent_{i}/draw_territory_count\",\n",
    "                                                                               self.hero_agents_list[i].draw_territory_count,\n",
    "                                                                               self.draw_count)#self.global_step)\n",
    "                \n",
    "                for j in self.hero_agents_list[i].count_draw_dict:\n",
    "                    self.writer.add_scalar(f\"draw_charts_agent_{i}/{j}_position_prop_draw\",int(\n",
    "                                                                                            self.hero_agents_list[i].position==j\n",
    "                                                                                            ),self.draw_count)\n",
    "\n",
    "                    if j not in [1,2]:\n",
    "                        self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_all_prop\",int(\n",
    "                                                                                            self.hero_agents_list[i].position==j\n",
    "                                                                                            ),(curren_epi+1))\n",
    "\n",
    "                        self.writer.add_scalar(f\"draw_charts_agent_{i}/{j}_place_in_draw\",\n",
    "                                                                   self.hero_agents_list[i].count_draw_dict[j],\n",
    "                                                                   self.draw_count)\n",
    "\n",
    "                        self.writer.add_scalar(f\"draw_charts_agent_{i}/{j}_place_in_draw_ratio\",\n",
    "                                                                   self.hero_agents_list[i].count_draw_dict[j]/self.draw_count,\n",
    "                                                                   self.draw_count)\n",
    "                        \n",
    "                    \n",
    "            self.writer.add_scalar(\"draw_charts/draw_count\",self.draw_count,(curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(\"draw_charts/draw\",1,(curren_epi+1))\n",
    "            self.writer.add_scalar(\"draw_charts/draw_to_total_count\",self.draw_count/(curren_epi +1+0.000001),(curren_epi +1))#self.global_step)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            non_draw_count =(curren_epi-self.draw_count+1+0.000001)\n",
    "            for i in self.hero_agents_list:\n",
    "\n",
    "                self.writer.add_scalar(f\"win_charts_agent_{i}/position_win\",self.hero_agents_list[i].position\n",
    "                                                                                            ,(curren_epi+1))\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                self.hero_agents_list[i].count_dict[self.hero_agents_list[i].position\n",
    "                                               ] +=1\n",
    "\n",
    "                \n",
    "                for j in self.hero_agents_list[i].count_dict:\n",
    "                    self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_prop\",int(\n",
    "                                                            self.hero_agents_list[i].position==j\n",
    "                                                            ),(curren_epi+1))\n",
    "\n",
    "                    self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position\",self.hero_agents_list[i].count_dict[j],\n",
    "                                           (curren_epi+1))\n",
    "\n",
    "                    self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_to_total_terminated\",self.hero_agents_list[i].count_dict[j]/non_draw_count,(curren_epi+1))\n",
    "\n",
    "                    if j not in [1,2]:\n",
    "                        self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_all_prop\",int(\n",
    "                                                                                            self.hero_agents_list[i].position==j\n",
    "                                                                                            ),(curren_epi+1))\n",
    "\n",
    "\n",
    "            self.writer.add_scalar(\"draw_charts/draw\",0,(curren_epi+1))\n",
    "            \n",
    "        for i in self.hero_agents_list:\n",
    "\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/model_move_count_per_episode\",   sum(current_agent_acting[:step_count] ==i)  ,  (curren_epi +1))\n",
    "\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/model_2_total_move_count_per_episode\",   sum(current_agent_acting[:step_count] ==i)/sum( self.hero_agents_list[i].move_count.values()),  (curren_epi +1))\n",
    "\n",
    "            self.writer.add_scalar(f\"win_charts_agent_{i}/position_all\",self.hero_agents_list[i].position\n",
    "                                                                                            ,(curren_epi+1))\n",
    "\n",
    "            \n",
    "            for j in self.hero_agents_list[i].count_dict:\n",
    "                self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_to_total\",(\n",
    "                                                            self.hero_agents_list[i].count_dict[j]+\n",
    "                                                            self.hero_agents_list[i].count_draw_dict[j]\n",
    "                                                        \n",
    "                                                            )/(curren_epi +1+0.00001 ),(curren_epi+1))#global_step)\n",
    "\n",
    "            \n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_count_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_count,            (curren_epi +1))#self.global_step)\n",
    "                \n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_count_position_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[0],            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_count_attack_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[1],            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_count_fortify_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[2],            (curren_epi +1))#self.global_step)\n",
    "            \n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/total_moves\",sum(\n",
    "                                                    self.hero_agents_list[i].move_count.values()),            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_to_step_count_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_count/(sum(\n",
    "                                                   self.hero_agents_list[i].move_count.values())+1),            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_to_step_position_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[0]/( \n",
    "                                                   self.hero_agents_list[i].move_count[0]+1),            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_to_step_attack_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[1]/( \n",
    "                                                   self.hero_agents_list[i].move_count[1]+1),            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_to_step_fortify_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[2]/( \n",
    "                                                   self.hero_agents_list[i].move_count[2]+1),            (curren_epi +1))#self.global_step)\n",
    "\n",
    "        \n",
    "        self.writer.add_scalar(\"charts/epsilon\",(curren_epi/((self.num_iterations*self.num_episodes)/10)),(curren_epi +1))#self.global_step)\n",
    "        self.writer.add_scalar(\"charts/avg_per_epi_total_reward\", np.mean(list(total_rewards.values())), (curren_epi +1))#self.global_step)\n",
    "\n",
    "        \n",
    "\n",
    "        #values_total = {i:0 for i in self.env.possible_agents}\n",
    "        \n",
    "\n",
    "        \n",
    "        self.writer.add_scalar(\"charts/episodic_length\", cur_epi_list[:step].sum(), (curren_epi +1))#self.global_step)\n",
    "        \n",
    "        for i in env.possible_agents:\n",
    "            #cur_index = torch.where((current_agent[:,0] == i) &( cur_epi_list ))[0]\n",
    "\n",
    "            #values_total[i] = values[cur_index].mean()\n",
    "            #writer.add_scalar(\"charts/mean_value_per_epi_agent_\"+str(i), values_total[i], global_step)\n",
    "            \n",
    "            self.writer.add_scalar(\"charts/total_reward_per_epi_agent_\"+str(i), total_rewards[i], (curren_epi +1))#self.global_step)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c5ef2c-809a-471a-a3ab-ac044049a212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b80d7f2-d7f7-428d-9b5f-f70642dcc8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac8815f-f99c-4b15-82e7-90cc48c66fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e841a5ba-0d1a-404d-bf4d-36db6450b88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18cfc23-dba9-4a51-97e7-f901760c21ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36b611c-fc8a-4ef2-ba05-c754bb9da346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad476a7-6507-41a1-8427-506ba727cad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec6ad37-742d-4510-893e-27cfec644bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c365e-30f1-48e5-8d25-c6f04be1b6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fdf692-0c3e-48c4-a7d9-7902ae187a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27560f9-f1fb-4a5c-9374-652525467ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e4626-d33f-4660-9ee4-c98f94b67f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfc5abf-53b6-4d9a-85b5-6b5429bd27fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e32426-5897-4d30-8b73-d71b64227320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ca33b-dca6-43d6-af8a-5898e6e05435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    import stable_baselines3 as sb3\n",
    "\n",
    "    if sb3.__version__ < \"2.0\":\n",
    "        raise ValueError(\n",
    "            \"\"\"Ongoing migration: run the following command to install the new dependencies:\n",
    "poetry run pip install \"stable_baselines3==2.0.0a1\"\n",
    "\"\"\"\n",
    "        )\n",
    "    args = Args()#tyro.cli(Args)\n",
    "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    #if args.track:\n",
    "    #    import wandb\n",
    "\n",
    "        #wandb.init(\n",
    "        #    project=args.wandb_project_name,\n",
    "        #    entity=args.wandb_entity,\n",
    "        #    sync_tensorboard=True,\n",
    "        #    config=vars(args),\n",
    "        #    name=run_name,\n",
    "        #    monitor_gym=True,\n",
    "        #    save_code=True,\n",
    "        #)\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    # TRY NOT TO MODIFY: seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])\n",
    "    assert isinstance(envs.single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n",
    "\n",
    "    actor = Actor(envs).to(device)\n",
    "    qf1 = QNetwork(envs).to(device)\n",
    "    qf1_target = QNetwork(envs).to(device)\n",
    "    target_actor = Actor(envs).to(device)\n",
    "    target_actor.load_state_dict(actor.state_dict())\n",
    "    qf1_target.load_state_dict(qf1.state_dict())\n",
    "    q_optimizer = optim.Adam(list(qf1.parameters()), lr=args.learning_rate)\n",
    "    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.learning_rate)\n",
    "\n",
    "    envs.single_observation_space.dtype = np.float32\n",
    "    rb = ReplayBuffer(\n",
    "        args.buffer_size,\n",
    "        envs.single_observation_space,\n",
    "        envs.single_action_space,\n",
    "        device,\n",
    "        handle_timeout_termination=False,\n",
    "    )\n",
    "    start_time = time.time()\n",
    "\n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    obs, _ = envs.reset(seed=args.seed)\n",
    "    for global_step in range(args.total_timesteps):\n",
    "        # ALGO LOGIC: put action logic here\n",
    "        if global_step < args.learning_starts:\n",
    "            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                actions = actor(torch.Tensor(obs).to(device))\n",
    "                actions += torch.normal(0, actor.action_scale * args.exploration_noise)\n",
    "                actions = actions.cpu().numpy().clip(envs.single_action_space.low, envs.single_action_space.high)\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        if \"final_info\" in infos:\n",
    "            for info in infos[\"final_info\"]:\n",
    "                print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "                break\n",
    "\n",
    "        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "        real_next_obs = next_obs.copy()\n",
    "        for idx, trunc in enumerate(truncations):\n",
    "            if trunc:\n",
    "                real_next_obs[idx] = infos[\"final_observation\"][idx]\n",
    "        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "\n",
    "        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "        obs = next_obs\n",
    "\n",
    "        # ALGO LOGIC: training.\n",
    "        if global_step > args.learning_starts:\n",
    "            data = rb.sample(args.batch_size)\n",
    "            with torch.no_grad():\n",
    "                next_state_actions = target_actor(data.next_observations)\n",
    "                qf1_next_target = qf1_target(data.next_observations, next_state_actions)\n",
    "                next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)\n",
    "\n",
    "            qf1_a_values = qf1(data.observations, data.actions).view(-1)\n",
    "            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "\n",
    "            # optimize the model\n",
    "            q_optimizer.zero_grad()\n",
    "            qf1_loss.backward()\n",
    "            q_optimizer.step()\n",
    "\n",
    "            if global_step % args.policy_frequency == 0:\n",
    "                actor_loss = -qf1(data.observations, actor(data.observations)).mean()\n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "\n",
    "                # update the target network\n",
    "                for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
    "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "            if global_step % 100 == 0:\n",
    "                writer.add_scalar(\"losses/qf1_values\", qf1_a_values.mean().item(), global_step)\n",
    "                writer.add_scalar(\"losses/qf1_loss\", qf1_loss.item(), global_step)\n",
    "                writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), global_step)\n",
    "                print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "                writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "    if args.save_model:\n",
    "        model_path = f\"runs/{run_name}/{args.exp_name}.cleanrl_model\"\n",
    "        torch.save((actor.state_dict(), qf1.state_dict()), model_path)\n",
    "        print(f\"model saved to {model_path}\")\n",
    "        from cleanrl_utils.evals.ddpg_eval import evaluate\n",
    "\n",
    "        episodic_returns = evaluate(\n",
    "            model_path,\n",
    "            make_env,\n",
    "            args.env_id,\n",
    "            eval_episodes=10,\n",
    "            run_name=f\"{run_name}-eval\",\n",
    "            Model=(Actor, QNetwork),\n",
    "            device=device,\n",
    "            exploration_noise=args.exploration_noise,\n",
    "        )\n",
    "        for idx, episodic_return in enumerate(episodic_returns):\n",
    "            writer.add_scalar(\"eval/episodic_return\", episodic_return, idx)\n",
    "\n",
    "        if args.upload_model:\n",
    "            from cleanrl_utils.huggingface import push_to_hub\n",
    "\n",
    "            repo_name = f\"{args.env_id}-{args.exp_name}-seed{args.seed}\"\n",
    "            repo_id = f\"{args.hf_entity}/{repo_name}\" if args.hf_entity else repo_name\n",
    "            push_to_hub(args, episodic_returns, repo_id, \"DDPG\", f\"runs/{run_name}\", f\"videos/{run_name}-eval\")\n",
    "\n",
    "    envs.close()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b463dca-79aa-4afa-add7-6b3e2f0c8529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4107e755-84b7-4197-99c4-78a7a88d29d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaebee5-3968-4cef-b1e5-fbfb575ab99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4ef06-ea3f-4c7c-8314-ff9fc26307e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL_project)",
   "language": "python",
   "name": "dl_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
